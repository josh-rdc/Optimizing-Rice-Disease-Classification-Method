{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64653a26-b164-4036-93b5-be0f7358a8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auxiliaries (Importing Datafile, Checking Time)\n",
    "import os \n",
    "import time\n",
    "\n",
    "# Calculations\n",
    "import numpy as np\n",
    "\n",
    "# For Importing Data Files\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# For Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.filters import threshold_multiotsu\n",
    "from skimage import color\n",
    "\n",
    "# For Image Pre-Processing\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979b03d-2172-4465-999b-1e65d282be22",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create dataframe\n",
    "\n",
    "3 columns\n",
    "1. Disease Classification\n",
    "2. Image Name\n",
    "3. Image RGB Values (RGB Converted from BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce4c625-de79-4861-b12a-56db5eb4203e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def natural_sort_key(s):\n",
    "    \"\"\"Key function for natural sorting.\"\"\"\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "def create_dataframe_from_folders(root_folder):\n",
    "    # Initialize empty lists to store data for each column\n",
    "    folder_names = []\n",
    "    photo_names = []\n",
    "    photos = []\n",
    "\n",
    "    # Traverse through the root folder and its subfolders\n",
    "    for folder_name in sorted(os.listdir(root_folder)):\n",
    "        folder_path = os.path.join(root_folder, folder_name)\n",
    "\n",
    "        # Check if the entry in the root folder is a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            for photo_name in sorted(os.listdir(folder_path), key=natural_sort_key):\n",
    "                # Photos are in common image formats (e.g., jpg, png)\n",
    "                if photo_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    photo_path = os.path.join(folder_path, photo_name)\n",
    "\n",
    "                    # Read image data using cv2\n",
    "                    image_data = cv2.imread(photo_path)\n",
    "\n",
    "                    # Convert BGR to RGB\n",
    "                    image_data_rgb = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # Append data to the lists\n",
    "                    folder_names.append(folder_name)\n",
    "                    photo_names.append(photo_name)\n",
    "                    photos.append(image_data_rgb)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'class': folder_names,\n",
    "        'img_name': photo_names,\n",
    "        'rgb': photos\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af7bde-ac77-45fa-8c61-7a53363109b4",
   "metadata": {},
   "source": [
    "Get data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f698915-bb74-4ec7-8ff1-606c6471d686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   class                img_name  \\\n",
      "0  bacterial_leaf_blight  BLB_multi_leaf (1).jpg   \n",
      "1  bacterial_leaf_blight  BLB_multi_leaf (2).jpg   \n",
      "2  bacterial_leaf_blight  BLB_multi_leaf (3).jpg   \n",
      "3  bacterial_leaf_blight  BLB_multi_leaf (4).jpg   \n",
      "4  bacterial_leaf_blight  BLB_multi_leaf (5).jpg   \n",
      "\n",
      "                                                 rgb  \n",
      "0  [[[115, 152, 47], [78, 113, 9], [116, 147, 43]...  \n",
      "1  [[[198, 218, 97], [190, 210, 89], [189, 208, 9...  \n",
      "2  [[[137, 176, 51], [166, 203, 64], [165, 200, 3...  \n",
      "3  [[[192, 212, 143], [201, 222, 157], [187, 206,...  \n",
      "4  [[[132, 176, 27], [135, 180, 27], [161, 206, 4...  \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "extra_resized_folder_path = r\"C:\\Users\\Josh\\000 Files\\003 Mengg AI\\01a 201 (AI)\\03 Mini-Project\\resized_raw_images (14 Classes)\"\n",
    "extra_resized = create_dataframe_from_folders(extra_resized_folder_path)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(extra_resized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5426b9d-5fe5-470e-95cd-b9bd72e1875a",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Class Counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be65000-1849-48e3-b646-93efd2758afd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "bakanae                  100\n",
      "brown_spot               100\n",
      "grassy_stunt_virus       100\n",
      "healthy_rice_plant       100\n",
      "ragged_stunt_virus       100\n",
      "stem_rot                 100\n",
      "tungro_virus             100\n",
      "bacterial_leaf_streak     99\n",
      "rice_false_smut           99\n",
      "narrow_brown_spot         98\n",
      "rice_blast                98\n",
      "sheath_blight             98\n",
      "bacterial_leaf_blight     97\n",
      "sheath_rot                91\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_counts = extra_resized['class'].value_counts()\n",
    "\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac8225-3fe7-4259-a715-381055cfb61f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d99ab4-0a06-495a-8af2-2d2ca69d19a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Texture Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef73441b-106f-4a76-af4a-30d0192a58c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mahotas\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "def compute_glcm_features(df):\n",
    "    df['gray'] = df['rgb'].apply(lambda x: cv2.cvtColor(np.uint8(x), cv2.COLOR_RGB2GRAY))\n",
    "    def calculate_glcm(gray_image):\n",
    "        # Compute GLCM using mahotas\n",
    "        glcm = mahotas.features.haralick(gray_image.astype(np.uint8), ignore_zeros=True)\n",
    "        return glcm.mean(axis=0)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        gray_image = row['gray']\n",
    "        glcm_features = calculate_glcm(gray_image)\n",
    "        features.append(glcm_features)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Define meaningful column names for GLCM features\n",
    "    glcm_columns = [\n",
    "        'Angular Second Moment',\n",
    "        'Contrast',\n",
    "        'Correlation',\n",
    "        'Sum of Squares: Variance',\n",
    "        'Inverse Difference Moment',\n",
    "        'Sum Average',\n",
    "        'Sum Variance',\n",
    "        'Sum Entropy',\n",
    "        'Entropy',\n",
    "        'Difference Variance',\n",
    "        'Difference Entropy',\n",
    "        'Informational Measure of Correlation 1',\n",
    "        'Informational Measure of Correlation 2'\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame with the new GLCM features and meaningful column names\n",
    "    glcm_df = pd.DataFrame(features, columns=[f'GLCM_{col}' for col in glcm_columns])\n",
    "\n",
    "    # Concatenate the new DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([df, glcm_df], axis=1)\n",
    "\n",
    "    print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2b059a-fa17-4742-a4e9-61286361606a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 47.48 seconds\n"
     ]
    }
   ],
   "source": [
    "glcm = compute_glcm_features(extra_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564a00a9-a6ed-4d71-b79b-82ded4744d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove the first n columns\n",
    "f_glcm = glcm.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3555e8-efe3-4a70-8258-9bf9d11a2d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLCM_Angular Second Moment</th>\n",
       "      <th>GLCM_Contrast</th>\n",
       "      <th>GLCM_Correlation</th>\n",
       "      <th>GLCM_Sum of Squares: Variance</th>\n",
       "      <th>GLCM_Inverse Difference Moment</th>\n",
       "      <th>GLCM_Sum Average</th>\n",
       "      <th>GLCM_Sum Variance</th>\n",
       "      <th>GLCM_Sum Entropy</th>\n",
       "      <th>GLCM_Entropy</th>\n",
       "      <th>GLCM_Difference Variance</th>\n",
       "      <th>GLCM_Difference Entropy</th>\n",
       "      <th>GLCM_Informational Measure of Correlation 1</th>\n",
       "      <th>GLCM_Informational Measure of Correlation 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000093</td>\n",
       "      <td>1876.058313</td>\n",
       "      <td>0.693690</td>\n",
       "      <td>3062.090566</td>\n",
       "      <td>0.086892</td>\n",
       "      <td>281.006018</td>\n",
       "      <td>10372.303952</td>\n",
       "      <td>8.629042</td>\n",
       "      <td>14.243405</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>6.206143</td>\n",
       "      <td>-0.154215</td>\n",
       "      <td>0.950487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>1334.884861</td>\n",
       "      <td>0.797998</td>\n",
       "      <td>3303.508641</td>\n",
       "      <td>0.077155</td>\n",
       "      <td>276.277140</td>\n",
       "      <td>11879.149702</td>\n",
       "      <td>8.601060</td>\n",
       "      <td>14.082083</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>6.006272</td>\n",
       "      <td>-0.160813</td>\n",
       "      <td>0.952879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000140</td>\n",
       "      <td>1340.143870</td>\n",
       "      <td>0.744815</td>\n",
       "      <td>2625.859569</td>\n",
       "      <td>0.091972</td>\n",
       "      <td>293.873154</td>\n",
       "      <td>9163.294405</td>\n",
       "      <td>8.446153</td>\n",
       "      <td>13.764957</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>5.900897</td>\n",
       "      <td>-0.165284</td>\n",
       "      <td>0.954139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>1293.754229</td>\n",
       "      <td>0.776222</td>\n",
       "      <td>2890.458305</td>\n",
       "      <td>0.071097</td>\n",
       "      <td>289.332162</td>\n",
       "      <td>10268.078990</td>\n",
       "      <td>8.648174</td>\n",
       "      <td>14.226573</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>6.040155</td>\n",
       "      <td>-0.157107</td>\n",
       "      <td>0.952233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>1076.703851</td>\n",
       "      <td>0.693791</td>\n",
       "      <td>1757.932426</td>\n",
       "      <td>0.082002</td>\n",
       "      <td>284.620945</td>\n",
       "      <td>5955.025853</td>\n",
       "      <td>8.284213</td>\n",
       "      <td>13.795743</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>5.881877</td>\n",
       "      <td>-0.134893</td>\n",
       "      <td>0.923023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GLCM_Angular Second Moment  GLCM_Contrast  GLCM_Correlation  \\\n",
       "0                    0.000093    1876.058313          0.693690   \n",
       "1                    0.000090    1334.884861          0.797998   \n",
       "2                    0.000140    1340.143870          0.744815   \n",
       "3                    0.000071    1293.754229          0.776222   \n",
       "4                    0.000101    1076.703851          0.693791   \n",
       "\n",
       "   GLCM_Sum of Squares: Variance  GLCM_Inverse Difference Moment  \\\n",
       "0                    3062.090566                        0.086892   \n",
       "1                    3303.508641                        0.077155   \n",
       "2                    2625.859569                        0.091972   \n",
       "3                    2890.458305                        0.071097   \n",
       "4                    1757.932426                        0.082002   \n",
       "\n",
       "   GLCM_Sum Average  GLCM_Sum Variance  GLCM_Sum Entropy  GLCM_Entropy  \\\n",
       "0        281.006018       10372.303952          8.629042     14.243405   \n",
       "1        276.277140       11879.149702          8.601060     14.082083   \n",
       "2        293.873154        9163.294405          8.446153     13.764957   \n",
       "3        289.332162       10268.078990          8.648174     14.226573   \n",
       "4        284.620945        5955.025853          8.284213     13.795743   \n",
       "\n",
       "   GLCM_Difference Variance  GLCM_Difference Entropy  \\\n",
       "0                  0.000072                 6.206143   \n",
       "1                  0.000079                 6.006272   \n",
       "2                  0.000095                 5.900897   \n",
       "3                  0.000073                 6.040155   \n",
       "4                  0.000086                 5.881877   \n",
       "\n",
       "   GLCM_Informational Measure of Correlation 1  \\\n",
       "0                                    -0.154215   \n",
       "1                                    -0.160813   \n",
       "2                                    -0.165284   \n",
       "3                                    -0.157107   \n",
       "4                                    -0.134893   \n",
       "\n",
       "   GLCM_Informational Measure of Correlation 2  \n",
       "0                                     0.950487  \n",
       "1                                     0.952879  \n",
       "2                                     0.954139  \n",
       "3                                     0.952233  \n",
       "4                                     0.923023  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_glcm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21ad4b-7eb0-4110-bcb4-6e592e28d837",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Histogram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d94ca35f-9437-4b01-8009-1a0eed2ae870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import color\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "354c8451-629c-447d-beef-076db3311418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def values(dataframe):\n",
    "    # Ensure the 'rgb' column exists in the dataframe\n",
    "    dataframe['hsv'] = dataframe['rgb'].apply(lambda x: cv2.cvtColor(np.uint8(x), cv2.COLOR_RGB2HSV))\n",
    "    #dataframe['hsi'] = dataframe['rgb'].apply(lambda x: color.rgb2hsi(np.uint8(x)))\n",
    "    dataframe['lab'] = dataframe['rgb'].apply(lambda x: cv2.cvtColor(np.uint8(x), cv2.COLOR_RGB2LAB))\n",
    "\n",
    "    dataframe['red'] = dataframe['rgb'].apply(lambda rgb: rgb[:,:,0])\n",
    "    dataframe['green'] = dataframe['rgb'].apply(lambda rgb: rgb[:,:,1])\n",
    "    dataframe['blue'] = dataframe['rgb'].apply(lambda rgb: rgb[:,:,2])\n",
    "    \n",
    "    dataframe['hue_v'] = dataframe['hsv'].apply(lambda hsv: hsv[:,:,0])\n",
    "    dataframe['saturation_v'] = dataframe['hsv'].apply(lambda hsv: hsv[:,:,1])\n",
    "    dataframe['value_v'] = dataframe['hsv'].apply(lambda hsv: hsv[:,:,2])\n",
    "    \n",
    "    #dataframe['hue_i'] = dataframe['hsv'].apply(lambda hsi: hsi[:,:,0])\n",
    "    #dataframe['saturation_i'] = dataframe['hsv'].apply(lambda hsi: hsi[:,:,1])\n",
    "    #dataframe['intensity_i'] = dataframe['hsv'].apply(lambda hsi: hsi[:,:,2])\n",
    "    \n",
    "    dataframe['lightness'] = dataframe['lab'].apply(lambda lab: lab[:, :, 0])\n",
    "    dataframe['a'] = dataframe['lab'].apply(lambda lab: lab[:, :, 1])\n",
    "    dataframe['b'] = dataframe['lab'].apply(lambda lab: lab[:, :, 2])\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d5082c1-48a7-4a77-ae63-d276523f8173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_bins_to_dataframe(dataframe, column_name, num_bins):\n",
    "    # Extract the specified column\n",
    "    original_column = dataframe[column_name]\n",
    "\n",
    "    # Define bin edges based on the min and max values in the matrices\n",
    "    min_value = np.min([np.min(matrix) for matrix in original_column])\n",
    "    max_value = np.max([np.max(matrix) for matrix in original_column])\n",
    "\n",
    "    bin_edges = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "    # Create column names for the bins\n",
    "    bin_column_names = [f'{column_name}_{i}' for i in range(num_bins)]\n",
    "\n",
    "    # Iterate through each matrix in the original column\n",
    "    for i, matrix in enumerate(original_column):\n",
    "        # Digitize each element in the matrix into the corresponding bin\n",
    "        bin_indices = np.digitize(matrix.flatten(), bin_edges, right=True)\n",
    "\n",
    "        # Count the occurrences of each bin index\n",
    "        bin_counts = np.bincount(bin_indices, minlength=num_bins + 1)[1:]\n",
    "\n",
    "        # Add new columns to the DataFrame for each bin\n",
    "        for j, bin_column_name in enumerate(bin_column_names):\n",
    "            dataframe.loc[i, bin_column_name] = bin_counts[j]\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eeb1fba-0704-4f4b-b34a-2e2ef1cbd5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channels = values(extra_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1ab83c9-fc0e-4cff-9869-3fa8c12e4469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_bins_dataframe(dataframe, col, num_bins):\n",
    "    bins_dataframe = pd.DataFrame()\n",
    "\n",
    "    #print(col)\n",
    "    min_value = np.min([np.min(matrix) for matrix in dataframe[col]])\n",
    "    max_value = np.max([np.max(matrix) for matrix in dataframe[col]])\n",
    "\n",
    "    bin_edges = np.linspace(min_value, max_value, num_bins + 1)\n",
    "    bin_column_names = [f'{col}_{i}' for i in range(1, num_bins + 1)]\n",
    "\n",
    "    for matrix in dataframe[col]:\n",
    "        bin_indices = np.digitize(matrix.flatten(), bin_edges, right=True)\n",
    "        bin_counts = np.bincount(bin_indices, minlength=num_bins + 1)[1:]\n",
    "\n",
    "        bins_dataframe = pd.concat([bins_dataframe, pd.DataFrame(bin_counts).transpose()], axis=0, ignore_index=True)\n",
    "\n",
    "    bins_dataframe.columns = bin_column_names\n",
    "        \n",
    "    return bins_dataframe\n",
    "\n",
    "def hist_features(df, cols, bins):\n",
    "    complete_bins = pd.DataFrame()\n",
    "\n",
    "    for col in cols:\n",
    "        col_bins = create_bins_dataframe(df, col, bins)\n",
    "        complete_bins = pd.concat([complete_bins, col_bins], axis=1)\n",
    "\n",
    "    return complete_bins\n",
    "\n",
    "cols = ['red', 'green', 'blue', 'hue_v', 'saturation_v', 'value_v', 'lightness', 'a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ef1f4f2-7b29-4783-80bb-942502c3f276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_histogram = hist_features(channels, cols, 82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "200b7499-7dd8-4545-a930-7e28dd715d64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_1</th>\n",
       "      <th>red_2</th>\n",
       "      <th>red_3</th>\n",
       "      <th>red_4</th>\n",
       "      <th>red_5</th>\n",
       "      <th>red_6</th>\n",
       "      <th>red_7</th>\n",
       "      <th>red_8</th>\n",
       "      <th>red_9</th>\n",
       "      <th>red_10</th>\n",
       "      <th>...</th>\n",
       "      <th>b_73</th>\n",
       "      <th>b_74</th>\n",
       "      <th>b_75</th>\n",
       "      <th>b_76</th>\n",
       "      <th>b_77</th>\n",
       "      <th>b_78</th>\n",
       "      <th>b_79</th>\n",
       "      <th>b_80</th>\n",
       "      <th>b_81</th>\n",
       "      <th>b_82</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "      <td>93</td>\n",
       "      <td>143</td>\n",
       "      <td>223</td>\n",
       "      <td>231</td>\n",
       "      <td>305</td>\n",
       "      <td>314</td>\n",
       "      <td>330</td>\n",
       "      <td>432</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153</td>\n",
       "      <td>210</td>\n",
       "      <td>261</td>\n",
       "      <td>308</td>\n",
       "      <td>349</td>\n",
       "      <td>391</td>\n",
       "      <td>392</td>\n",
       "      <td>358</td>\n",
       "      <td>383</td>\n",
       "      <td>554</td>\n",
       "      <td>...</td>\n",
       "      <td>541</td>\n",
       "      <td>556</td>\n",
       "      <td>187</td>\n",
       "      <td>98</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>62</td>\n",
       "      <td>145</td>\n",
       "      <td>205</td>\n",
       "      <td>290</td>\n",
       "      <td>337</td>\n",
       "      <td>325</td>\n",
       "      <td>338</td>\n",
       "      <td>454</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>53</td>\n",
       "      <td>89</td>\n",
       "      <td>106</td>\n",
       "      <td>194</td>\n",
       "      <td>242</td>\n",
       "      <td>267</td>\n",
       "      <td>436</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>1417</td>\n",
       "      <td>1133</td>\n",
       "      <td>195</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 738 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   red_1  red_2  red_3  red_4  red_5  red_6  red_7  red_8  red_9  red_10  ...  \\\n",
       "0     19     47     93    143    223    231    305    314    330     432  ...   \n",
       "1    153    210    261    308    349    391    392    358    383     554  ...   \n",
       "2     14     35     62    145    205    290    337    325    338     454  ...   \n",
       "3     14     14     40     53     89    106    194    242    267     436  ...   \n",
       "4      0      1      3      4      1      6      6      9      8      31  ...   \n",
       "\n",
       "   b_73  b_74  b_75  b_76  b_77  b_78  b_79  b_80  b_81  b_82  \n",
       "0    73    27     0     0     0     0     0     0     0     0  \n",
       "1   541   556   187    98    47    33     0     0     0     0  \n",
       "2   157    51     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0     0  \n",
       "4  1417  1133   195    42     8     1     0     0     0     0  \n",
       "\n",
       "[5 rows x 738 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_histogram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58cb8f-65ac-44fc-99e6-58e42abfabfa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Color Moments Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8b0823e-3e5b-4f1c-95a6-8b8b4cc97e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import cv2\n",
    "\n",
    "def color_moments(dataframe, cols):\n",
    "    for col in cols:\n",
    "        print(col)\n",
    "        mean_values = []\n",
    "        variance_values = []\n",
    "        kurtosis_values = []\n",
    "        skewness_values = []\n",
    "\n",
    "        for img in dataframe[col]:\n",
    "            # 'img' is a 3D numpy array representing an RGB image\n",
    "            # Calculate statistics for each image\n",
    "            mean_channel = np.mean(img, axis=(0, 1))\n",
    "            variance_channel = np.var(img, axis=(0, 1))\n",
    "            kurtosis_channel = kurtosis(img, axis=(0, 1))\n",
    "            skewness_channel = skew(img, axis=(0, 1))\n",
    "\n",
    "            # Append values to respective lists\n",
    "            mean_values.append(mean_channel)\n",
    "            variance_values.append(variance_channel)\n",
    "            kurtosis_values.append(kurtosis_channel)\n",
    "            skewness_values.append(skewness_channel)\n",
    "\n",
    "        # Add the new columns to the DataFrame with channel names\n",
    "        for i in range(img.shape[2]):  # 'img' has shape (height, width, channels)\n",
    "            dataframe[f'{col}_channel_{i}_mean'] = [x[i] for x in mean_values]\n",
    "            dataframe[f'{col}_channel_{i}_variance'] = [x[i] for x in variance_values]\n",
    "            dataframe[f'{col}_channel_{i}_kurtosis'] = [x[i] for x in kurtosis_values]\n",
    "            dataframe[f'{col}_channel_{i}_skewness'] = [x[i] for x in skewness_values]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2cc9e6e-ddcb-41fe-aa4b-72e3f44afc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb\n",
      "hsv\n",
      "lab\n"
     ]
    }
   ],
   "source": [
    "color_df = color_moments(extra_resized, ['rgb','hsv','lab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c245d80-93b2-4cc8-82a3-77f40ab9b53b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_color = color_df.drop(columns=['class','img_name','rgb','gray','hsv','lab','red','green','blue', 'hue_v', 'saturation_v', 'value_v', 'lightness', 'a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b52a436e-88b8-43bc-b1d7-b17b428f0ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rgb_channel_0_mean</th>\n",
       "      <th>rgb_channel_0_variance</th>\n",
       "      <th>rgb_channel_0_kurtosis</th>\n",
       "      <th>rgb_channel_0_skewness</th>\n",
       "      <th>rgb_channel_1_mean</th>\n",
       "      <th>rgb_channel_1_variance</th>\n",
       "      <th>rgb_channel_1_kurtosis</th>\n",
       "      <th>rgb_channel_1_skewness</th>\n",
       "      <th>rgb_channel_2_mean</th>\n",
       "      <th>rgb_channel_2_variance</th>\n",
       "      <th>...</th>\n",
       "      <th>lab_channel_0_kurtosis</th>\n",
       "      <th>lab_channel_0_skewness</th>\n",
       "      <th>lab_channel_1_mean</th>\n",
       "      <th>lab_channel_1_variance</th>\n",
       "      <th>lab_channel_1_kurtosis</th>\n",
       "      <th>lab_channel_1_skewness</th>\n",
       "      <th>lab_channel_2_mean</th>\n",
       "      <th>lab_channel_2_variance</th>\n",
       "      <th>lab_channel_2_kurtosis</th>\n",
       "      <th>lab_channel_2_skewness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.110013</td>\n",
       "      <td>3078.218207</td>\n",
       "      <td>-0.689453</td>\n",
       "      <td>-0.192670</td>\n",
       "      <td>157.315011</td>\n",
       "      <td>3449.301597</td>\n",
       "      <td>-0.484632</td>\n",
       "      <td>-0.518085</td>\n",
       "      <td>67.831274</td>\n",
       "      <td>2087.782836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.296129</td>\n",
       "      <td>-0.606758</td>\n",
       "      <td>106.950853</td>\n",
       "      <td>61.335754</td>\n",
       "      <td>0.410354</td>\n",
       "      <td>0.917856</td>\n",
       "      <td>169.947226</td>\n",
       "      <td>203.966363</td>\n",
       "      <td>0.049806</td>\n",
       "      <td>-0.576146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132.208227</td>\n",
       "      <td>3961.755789</td>\n",
       "      <td>-1.130258</td>\n",
       "      <td>-0.302681</td>\n",
       "      <td>158.333586</td>\n",
       "      <td>3558.081163</td>\n",
       "      <td>-0.908913</td>\n",
       "      <td>-0.458059</td>\n",
       "      <td>50.368921</td>\n",
       "      <td>1453.704717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.781886</td>\n",
       "      <td>-0.534726</td>\n",
       "      <td>103.516083</td>\n",
       "      <td>21.999662</td>\n",
       "      <td>2.755215</td>\n",
       "      <td>1.327478</td>\n",
       "      <td>177.711097</td>\n",
       "      <td>198.029258</td>\n",
       "      <td>0.350495</td>\n",
       "      <td>-0.735196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143.534598</td>\n",
       "      <td>2684.064771</td>\n",
       "      <td>-0.122034</td>\n",
       "      <td>-0.726785</td>\n",
       "      <td>163.988122</td>\n",
       "      <td>3032.684210</td>\n",
       "      <td>0.098623</td>\n",
       "      <td>-0.932985</td>\n",
       "      <td>67.333745</td>\n",
       "      <td>1393.034182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395657</td>\n",
       "      <td>-1.041771</td>\n",
       "      <td>106.869300</td>\n",
       "      <td>39.706570</td>\n",
       "      <td>1.180393</td>\n",
       "      <td>1.184484</td>\n",
       "      <td>173.509228</td>\n",
       "      <td>201.620730</td>\n",
       "      <td>0.534828</td>\n",
       "      <td>-0.842716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134.806521</td>\n",
       "      <td>3150.853829</td>\n",
       "      <td>-0.752603</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>155.817223</td>\n",
       "      <td>2935.205771</td>\n",
       "      <td>-0.687641</td>\n",
       "      <td>-0.282174</td>\n",
       "      <td>112.783462</td>\n",
       "      <td>2356.163989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.573437</td>\n",
       "      <td>-0.355013</td>\n",
       "      <td>112.668666</td>\n",
       "      <td>26.981995</td>\n",
       "      <td>2.443752</td>\n",
       "      <td>0.798156</td>\n",
       "      <td>148.112783</td>\n",
       "      <td>56.290194</td>\n",
       "      <td>1.918221</td>\n",
       "      <td>-0.305414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133.956593</td>\n",
       "      <td>1928.268962</td>\n",
       "      <td>-0.601913</td>\n",
       "      <td>0.189324</td>\n",
       "      <td>165.174087</td>\n",
       "      <td>1995.183999</td>\n",
       "      <td>-0.653920</td>\n",
       "      <td>-0.250057</td>\n",
       "      <td>46.036372</td>\n",
       "      <td>1525.296728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.566688</td>\n",
       "      <td>-0.252408</td>\n",
       "      <td>101.051419</td>\n",
       "      <td>77.759832</td>\n",
       "      <td>0.647577</td>\n",
       "      <td>1.165816</td>\n",
       "      <td>180.711256</td>\n",
       "      <td>190.856598</td>\n",
       "      <td>0.315457</td>\n",
       "      <td>-0.751957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rgb_channel_0_mean  rgb_channel_0_variance  rgb_channel_0_kurtosis  \\\n",
       "0          135.110013             3078.218207               -0.689453   \n",
       "1          132.208227             3961.755789               -1.130258   \n",
       "2          143.534598             2684.064771               -0.122034   \n",
       "3          134.806521             3150.853829               -0.752603   \n",
       "4          133.956593             1928.268962               -0.601913   \n",
       "\n",
       "   rgb_channel_0_skewness  rgb_channel_1_mean  rgb_channel_1_variance  \\\n",
       "0               -0.192670          157.315011             3449.301597   \n",
       "1               -0.302681          158.333586             3558.081163   \n",
       "2               -0.726785          163.988122             3032.684210   \n",
       "3                0.011988          155.817223             2935.205771   \n",
       "4                0.189324          165.174087             1995.183999   \n",
       "\n",
       "   rgb_channel_1_kurtosis  rgb_channel_1_skewness  rgb_channel_2_mean  \\\n",
       "0               -0.484632               -0.518085           67.831274   \n",
       "1               -0.908913               -0.458059           50.368921   \n",
       "2                0.098623               -0.932985           67.333745   \n",
       "3               -0.687641               -0.282174          112.783462   \n",
       "4               -0.653920               -0.250057           46.036372   \n",
       "\n",
       "   rgb_channel_2_variance  ...  lab_channel_0_kurtosis  \\\n",
       "0             2087.782836  ...               -0.296129   \n",
       "1             1453.704717  ...               -0.781886   \n",
       "2             1393.034182  ...                0.395657   \n",
       "3             2356.163989  ...               -0.573437   \n",
       "4             1525.296728  ...               -0.566688   \n",
       "\n",
       "   lab_channel_0_skewness  lab_channel_1_mean  lab_channel_1_variance  \\\n",
       "0               -0.606758          106.950853               61.335754   \n",
       "1               -0.534726          103.516083               21.999662   \n",
       "2               -1.041771          106.869300               39.706570   \n",
       "3               -0.355013          112.668666               26.981995   \n",
       "4               -0.252408          101.051419               77.759832   \n",
       "\n",
       "   lab_channel_1_kurtosis  lab_channel_1_skewness  lab_channel_2_mean  \\\n",
       "0                0.410354                0.917856          169.947226   \n",
       "1                2.755215                1.327478          177.711097   \n",
       "2                1.180393                1.184484          173.509228   \n",
       "3                2.443752                0.798156          148.112783   \n",
       "4                0.647577                1.165816          180.711256   \n",
       "\n",
       "   lab_channel_2_variance  lab_channel_2_kurtosis  lab_channel_2_skewness  \n",
       "0              203.966363                0.049806               -0.576146  \n",
       "1              198.029258                0.350495               -0.735196  \n",
       "2              201.620730                0.534828               -0.842716  \n",
       "3               56.290194                1.918221               -0.305414  \n",
       "4              190.856598                0.315457               -0.751957  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_color.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374ff69-1bee-49c9-a127-38bf7b2154fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Zernike Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb2e3d8b-9bcb-4314-8271-fbda88d99c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mahotas.features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def zernike(df, zernike_order):\n",
    "    features_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        gray_img = row['gray']\n",
    "\n",
    "        # Check if the image is grayscale\n",
    "        if len(gray_img.shape) == 2:\n",
    "            # If grayscale, compute Zernike moments directly\n",
    "            moments = mahotas.features.zernike_moments(gray_img, radius=zernike_order)\n",
    "        else:\n",
    "            raise ValueError(\"Input image must be grayscale.\")\n",
    "\n",
    "        features_list.append(moments)\n",
    "\n",
    "    # Determine the number of features per image\n",
    "    num_features_per_image = len(features_list[0])\n",
    "\n",
    "    # Add features to the DataFrame\n",
    "    feature_columns = [f'feature_{i}' for i in range(num_features_per_image)]\n",
    "    df_features = pd.DataFrame(features_list, columns=feature_columns)\n",
    "    df_result = pd.concat([df, df_features], axis=1)\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2839e315-3874-4df5-9d76-5f159e11d6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.085352</td>\n",
       "      <td>0.096424</td>\n",
       "      <td>0.007132</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>0.035195</td>\n",
       "      <td>0.047294</td>\n",
       "      <td>0.034389</td>\n",
       "      <td>0.012664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021457</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>0.024865</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.017575</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.035992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.014304</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>0.037181</td>\n",
       "      <td>0.013888</td>\n",
       "      <td>0.019413</td>\n",
       "      <td>0.032812</td>\n",
       "      <td>0.021023</td>\n",
       "      <td>0.036471</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019798</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.033492</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.008411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.079290</td>\n",
       "      <td>0.030770</td>\n",
       "      <td>0.009481</td>\n",
       "      <td>0.047826</td>\n",
       "      <td>0.057547</td>\n",
       "      <td>0.014013</td>\n",
       "      <td>0.021884</td>\n",
       "      <td>0.035470</td>\n",
       "      <td>0.021808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028761</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>0.039179</td>\n",
       "      <td>0.039791</td>\n",
       "      <td>0.023068</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.024128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.037962</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.070988</td>\n",
       "      <td>0.044010</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>0.015647</td>\n",
       "      <td>0.008096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.012084</td>\n",
       "      <td>0.020084</td>\n",
       "      <td>0.021049</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.017935</td>\n",
       "      <td>0.029116</td>\n",
       "      <td>0.023423</td>\n",
       "      <td>0.023256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.041976</td>\n",
       "      <td>0.050413</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>0.035719</td>\n",
       "      <td>0.045422</td>\n",
       "      <td>0.056392</td>\n",
       "      <td>0.064779</td>\n",
       "      <td>0.029646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025135</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.041041</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.004188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0    0.31831   0.017224   0.085352   0.096424   0.007132   0.019454   \n",
       "1    0.31831   0.014304   0.028368   0.037181   0.013888   0.019413   \n",
       "2    0.31831   0.079290   0.030770   0.009481   0.047826   0.057547   \n",
       "3    0.31831   0.037962   0.004782   0.037121   0.070988   0.044010   \n",
       "4    0.31831   0.008223   0.041976   0.050413   0.029785   0.035719   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  ...  feature_15  feature_16  \\\n",
       "0   0.035195   0.047294   0.034389   0.012664  ...    0.021457    0.009277   \n",
       "1   0.032812   0.021023   0.036471   0.003854  ...    0.019798    0.021939   \n",
       "2   0.014013   0.021884   0.035470   0.021808  ...    0.028761    0.049967   \n",
       "3   0.004336   0.021578   0.015647   0.008096  ...    0.017136    0.012084   \n",
       "4   0.045422   0.056392   0.064779   0.029646  ...    0.025135    0.003060   \n",
       "\n",
       "   feature_17  feature_18  feature_19  feature_20  feature_21  feature_22  \\\n",
       "0    0.024865    0.004817    0.017575    0.003713    0.003742    0.016868   \n",
       "1    0.024250    0.033492    0.027907    0.000967    0.009835    0.017213   \n",
       "2    0.039179    0.039791    0.023068    0.031944    0.021562    0.009072   \n",
       "3    0.020084    0.021049    0.013913    0.004487    0.017935    0.029116   \n",
       "4    0.024900    0.048771    0.041041    0.005773    0.034733    0.012531   \n",
       "\n",
       "   feature_23  feature_24  \n",
       "0    0.031700    0.035992  \n",
       "1    0.006593    0.008411  \n",
       "2    0.019779    0.024128  \n",
       "3    0.023423    0.023256  \n",
       "4    0.014290    0.004188  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zernike_df = zernike(extra_resized, 10)\n",
    "f_zernike = zernike_df.iloc[:, 51:]\n",
    "f_zernike.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f9f14-f5c1-49e8-bc81-0ff472678dd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Legendre Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04c31faf-4303-4c2c-9116-db4207ca5f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extra_resized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c11c6966-8fce-4fe6-b283-0ae5a249459b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l_input = extra_resized.drop(columns=['class','hsv','lab','red','green','blue', 'hue_v', 'saturation_v', 'value_v', 'lightness', 'a', 'b'])\n",
    "l_input = l_input.iloc[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "797a48fb-2cbb-4377-a2c7-c851bf96b9de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>rgb</th>\n",
       "      <th>gray</th>\n",
       "      <th>rgb_channel_0_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLB_multi_leaf (1).jpg</td>\n",
       "      <td>[[[115, 152, 47], [78, 113, 9], [116, 147, 43]...</td>\n",
       "      <td>[[129, 91, 126, 196, 208, 157, 175, 128, 114, ...</td>\n",
       "      <td>135.110013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLB_multi_leaf (2).jpg</td>\n",
       "      <td>[[[198, 218, 97], [190, 210, 89], [189, 208, 9...</td>\n",
       "      <td>[[198, 190, 189, 183, 189, 189, 187, 187, 189,...</td>\n",
       "      <td>132.208227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLB_multi_leaf (3).jpg</td>\n",
       "      <td>[[[137, 176, 51], [166, 203, 64], [165, 200, 3...</td>\n",
       "      <td>[[150, 176, 171, 177, 193, 187, 142, 136, 135,...</td>\n",
       "      <td>143.534598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLB_multi_leaf (4).jpg</td>\n",
       "      <td>[[[192, 212, 143], [201, 222, 157], [187, 206,...</td>\n",
       "      <td>[[198, 208, 194, 198, 196, 159, 143, 145, 165,...</td>\n",
       "      <td>134.806521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BLB_multi_leaf (5).jpg</td>\n",
       "      <td>[[[132, 176, 27], [135, 180, 27], [161, 206, 4...</td>\n",
       "      <td>[[146, 149, 174, 126, 153, 153, 185, 201, 210,...</td>\n",
       "      <td>133.956593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 img_name                                                rgb  \\\n",
       "0  BLB_multi_leaf (1).jpg  [[[115, 152, 47], [78, 113, 9], [116, 147, 43]...   \n",
       "1  BLB_multi_leaf (2).jpg  [[[198, 218, 97], [190, 210, 89], [189, 208, 9...   \n",
       "2  BLB_multi_leaf (3).jpg  [[[137, 176, 51], [166, 203, 64], [165, 200, 3...   \n",
       "3  BLB_multi_leaf (4).jpg  [[[192, 212, 143], [201, 222, 157], [187, 206,...   \n",
       "4  BLB_multi_leaf (5).jpg  [[[132, 176, 27], [135, 180, 27], [161, 206, 4...   \n",
       "\n",
       "                                                gray  rgb_channel_0_mean  \n",
       "0  [[129, 91, 126, 196, 208, 157, 175, 128, 114, ...          135.110013  \n",
       "1  [[198, 190, 189, 183, 189, 189, 187, 187, 189,...          132.208227  \n",
       "2  [[150, 176, 171, 177, 193, 187, 142, 136, 135,...          143.534598  \n",
       "3  [[198, 208, 194, 198, 196, 159, 143, 145, 165,...          134.806521  \n",
       "4  [[146, 149, 174, 126, 153, 153, 185, 201, 210,...          133.956593  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eac178d0-3ed3-4c52-a778-5ae124e3d99c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from scipy.special import legendre\n",
    "\n",
    "def compute_legendre_features(rgb_image, degree):\n",
    "    # Get V channel from RGB image\n",
    "    v_channel = rgb_image[:, :, 2]\n",
    "\n",
    "    # Define x and y coordinates\n",
    "    x_size, y_size = v_channel.shape\n",
    "    x = np.linspace(-1, 1, x_size)\n",
    "    y = np.linspace(-1, 1, y_size)\n",
    "\n",
    "    # Compute Legendre features for the V channel\n",
    "    legendre_features = np.zeros((degree + 1, degree + 1))\n",
    "\n",
    "    for j in range(degree + 1):\n",
    "        for k in range(degree + 1):\n",
    "            legendre_features[j, k] = np.sum(v_channel * legendre(j)(x) * legendre(k)(y))\n",
    "\n",
    "    return legendre_features.flatten()\n",
    "\n",
    "def add_legendre(df, degree):\n",
    "    # Create a new DataFrame for Legendre features\n",
    "    legendre_columns = [f'v_legendre_{i}_{j}' for i in range(degree + 1) for j in range(degree + 1)]\n",
    "    legendre_df = pd.DataFrame(df['rgb'].apply(lambda rgb: compute_legendre_features(rgb, degree)).values.tolist(), columns=legendre_columns)\n",
    "\n",
    "    # Concatenate the new DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([df, legendre_df], axis=1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43846dc1-14e3-448d-adc4-a22e38c851ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "legendre_df = add_legendre(l_input, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b74b7ce3-9c49-42a7-95d2-ea2df961247b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_legendre_0_0</th>\n",
       "      <th>v_legendre_0_1</th>\n",
       "      <th>v_legendre_0_2</th>\n",
       "      <th>v_legendre_0_3</th>\n",
       "      <th>v_legendre_0_4</th>\n",
       "      <th>v_legendre_0_5</th>\n",
       "      <th>v_legendre_0_6</th>\n",
       "      <th>v_legendre_0_7</th>\n",
       "      <th>v_legendre_0_8</th>\n",
       "      <th>v_legendre_0_9</th>\n",
       "      <th>...</th>\n",
       "      <th>v_legendre_10_1</th>\n",
       "      <th>v_legendre_10_2</th>\n",
       "      <th>v_legendre_10_3</th>\n",
       "      <th>v_legendre_10_4</th>\n",
       "      <th>v_legendre_10_5</th>\n",
       "      <th>v_legendre_10_6</th>\n",
       "      <th>v_legendre_10_7</th>\n",
       "      <th>v_legendre_10_8</th>\n",
       "      <th>v_legendre_10_9</th>\n",
       "      <th>v_legendre_10_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3403502.0</td>\n",
       "      <td>97362.493274</td>\n",
       "      <td>-48201.759416</td>\n",
       "      <td>-43493.535531</td>\n",
       "      <td>-7187.545250</td>\n",
       "      <td>-13629.967522</td>\n",
       "      <td>26794.954058</td>\n",
       "      <td>-16840.577069</td>\n",
       "      <td>-19979.273476</td>\n",
       "      <td>3169.619967</td>\n",
       "      <td>...</td>\n",
       "      <td>11979.555139</td>\n",
       "      <td>6950.403894</td>\n",
       "      <td>-1337.870389</td>\n",
       "      <td>10980.581080</td>\n",
       "      <td>-2731.321916</td>\n",
       "      <td>13832.668995</td>\n",
       "      <td>-4770.100418</td>\n",
       "      <td>6167.284262</td>\n",
       "      <td>4327.878377</td>\n",
       "      <td>171054.141659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2527311.0</td>\n",
       "      <td>103013.053812</td>\n",
       "      <td>-36361.603672</td>\n",
       "      <td>5917.770393</td>\n",
       "      <td>47863.129349</td>\n",
       "      <td>1575.078268</td>\n",
       "      <td>12099.674282</td>\n",
       "      <td>-3201.383018</td>\n",
       "      <td>18315.225460</td>\n",
       "      <td>1422.844543</td>\n",
       "      <td>...</td>\n",
       "      <td>-821.672825</td>\n",
       "      <td>16762.645859</td>\n",
       "      <td>2097.910709</td>\n",
       "      <td>12284.262029</td>\n",
       "      <td>195.439018</td>\n",
       "      <td>20480.883830</td>\n",
       "      <td>3132.492664</td>\n",
       "      <td>11516.628122</td>\n",
       "      <td>8400.271678</td>\n",
       "      <td>134299.780209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3378538.0</td>\n",
       "      <td>57117.757848</td>\n",
       "      <td>-80632.660661</td>\n",
       "      <td>-510.363287</td>\n",
       "      <td>11451.263244</td>\n",
       "      <td>-12619.841113</td>\n",
       "      <td>-28828.907259</td>\n",
       "      <td>-769.368737</td>\n",
       "      <td>27917.794633</td>\n",
       "      <td>-6883.608118</td>\n",
       "      <td>...</td>\n",
       "      <td>3963.350675</td>\n",
       "      <td>18954.699088</td>\n",
       "      <td>-2074.126958</td>\n",
       "      <td>8085.322444</td>\n",
       "      <td>1051.648198</td>\n",
       "      <td>8771.579378</td>\n",
       "      <td>-2117.981725</td>\n",
       "      <td>3647.296353</td>\n",
       "      <td>5448.970980</td>\n",
       "      <td>164009.682234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5659023.0</td>\n",
       "      <td>-217617.215247</td>\n",
       "      <td>39749.222928</td>\n",
       "      <td>-6034.575324</td>\n",
       "      <td>13640.673159</td>\n",
       "      <td>21378.007074</td>\n",
       "      <td>44068.112251</td>\n",
       "      <td>-24151.624353</td>\n",
       "      <td>14049.614456</td>\n",
       "      <td>-33151.322155</td>\n",
       "      <td>...</td>\n",
       "      <td>-4619.286498</td>\n",
       "      <td>15514.090473</td>\n",
       "      <td>-6651.294307</td>\n",
       "      <td>31327.480610</td>\n",
       "      <td>553.852950</td>\n",
       "      <td>28293.237294</td>\n",
       "      <td>-4090.823341</td>\n",
       "      <td>27115.154433</td>\n",
       "      <td>-18929.538381</td>\n",
       "      <td>296838.144091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2309921.0</td>\n",
       "      <td>-148810.686099</td>\n",
       "      <td>-37569.159223</td>\n",
       "      <td>52055.054845</td>\n",
       "      <td>44925.442955</td>\n",
       "      <td>-23770.114279</td>\n",
       "      <td>17043.186742</td>\n",
       "      <td>11900.041479</td>\n",
       "      <td>1297.065850</td>\n",
       "      <td>-35581.018742</td>\n",
       "      <td>...</td>\n",
       "      <td>-9263.364484</td>\n",
       "      <td>11693.901946</td>\n",
       "      <td>-7594.119586</td>\n",
       "      <td>7040.301359</td>\n",
       "      <td>-2945.920523</td>\n",
       "      <td>20936.096634</td>\n",
       "      <td>1642.000010</td>\n",
       "      <td>5613.007761</td>\n",
       "      <td>-10798.603049</td>\n",
       "      <td>122542.843959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   v_legendre_0_0  v_legendre_0_1  v_legendre_0_2  v_legendre_0_3  \\\n",
       "0       3403502.0    97362.493274   -48201.759416   -43493.535531   \n",
       "1       2527311.0   103013.053812   -36361.603672     5917.770393   \n",
       "2       3378538.0    57117.757848   -80632.660661     -510.363287   \n",
       "3       5659023.0  -217617.215247    39749.222928    -6034.575324   \n",
       "4       2309921.0  -148810.686099   -37569.159223    52055.054845   \n",
       "\n",
       "   v_legendre_0_4  v_legendre_0_5  v_legendre_0_6  v_legendre_0_7  \\\n",
       "0    -7187.545250   -13629.967522    26794.954058   -16840.577069   \n",
       "1    47863.129349     1575.078268    12099.674282    -3201.383018   \n",
       "2    11451.263244   -12619.841113   -28828.907259     -769.368737   \n",
       "3    13640.673159    21378.007074    44068.112251   -24151.624353   \n",
       "4    44925.442955   -23770.114279    17043.186742    11900.041479   \n",
       "\n",
       "   v_legendre_0_8  v_legendre_0_9  ...  v_legendre_10_1  v_legendre_10_2  \\\n",
       "0   -19979.273476     3169.619967  ...     11979.555139      6950.403894   \n",
       "1    18315.225460     1422.844543  ...      -821.672825     16762.645859   \n",
       "2    27917.794633    -6883.608118  ...      3963.350675     18954.699088   \n",
       "3    14049.614456   -33151.322155  ...     -4619.286498     15514.090473   \n",
       "4     1297.065850   -35581.018742  ...     -9263.364484     11693.901946   \n",
       "\n",
       "   v_legendre_10_3  v_legendre_10_4  v_legendre_10_5  v_legendre_10_6  \\\n",
       "0     -1337.870389     10980.581080     -2731.321916     13832.668995   \n",
       "1      2097.910709     12284.262029       195.439018     20480.883830   \n",
       "2     -2074.126958      8085.322444      1051.648198      8771.579378   \n",
       "3     -6651.294307     31327.480610       553.852950     28293.237294   \n",
       "4     -7594.119586      7040.301359     -2945.920523     20936.096634   \n",
       "\n",
       "   v_legendre_10_7  v_legendre_10_8  v_legendre_10_9  v_legendre_10_10  \n",
       "0     -4770.100418      6167.284262      4327.878377     171054.141659  \n",
       "1      3132.492664     11516.628122      8400.271678     134299.780209  \n",
       "2     -2117.981725      3647.296353      5448.970980     164009.682234  \n",
       "3     -4090.823341     27115.154433    -18929.538381     296838.144091  \n",
       "4      1642.000010      5613.007761    -10798.603049     122542.843959  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_legendre = legendre_df.iloc[:, 4:]\n",
    "f_legendre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bb6a4-6f30-4f3c-8b50-0bad0e8b6c75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Complete Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db5931a6-4d94-44df-8c1a-82076732f0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame with just the selected column\n",
    "classes = pd.DataFrame(extra_resized['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5002378-98b0-4b70-ba29-a07ba57764f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#complete_features = pd.concat([classes,f_color,f_histogram, f_glcm], axis=1)\n",
    "complete_features = pd.concat([classes,f_histogram, f_glcm, f_color, f_legendre, f_zernike], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3453770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_features.to_csv('complete_features.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "060e531f-0a6e-455b-8dbf-07ca777d8c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>red_1</th>\n",
       "      <th>red_2</th>\n",
       "      <th>red_3</th>\n",
       "      <th>red_4</th>\n",
       "      <th>red_5</th>\n",
       "      <th>red_6</th>\n",
       "      <th>red_7</th>\n",
       "      <th>red_8</th>\n",
       "      <th>red_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "      <td>93</td>\n",
       "      <td>143</td>\n",
       "      <td>223</td>\n",
       "      <td>231</td>\n",
       "      <td>305</td>\n",
       "      <td>314</td>\n",
       "      <td>330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021457</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>0.024865</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.017575</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.035992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>153</td>\n",
       "      <td>210</td>\n",
       "      <td>261</td>\n",
       "      <td>308</td>\n",
       "      <td>349</td>\n",
       "      <td>391</td>\n",
       "      <td>392</td>\n",
       "      <td>358</td>\n",
       "      <td>383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019798</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.033492</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.008411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>62</td>\n",
       "      <td>145</td>\n",
       "      <td>205</td>\n",
       "      <td>290</td>\n",
       "      <td>337</td>\n",
       "      <td>325</td>\n",
       "      <td>338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028761</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>0.039179</td>\n",
       "      <td>0.039791</td>\n",
       "      <td>0.023068</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.024128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>53</td>\n",
       "      <td>89</td>\n",
       "      <td>106</td>\n",
       "      <td>194</td>\n",
       "      <td>242</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.012084</td>\n",
       "      <td>0.020084</td>\n",
       "      <td>0.021049</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.017935</td>\n",
       "      <td>0.029116</td>\n",
       "      <td>0.023423</td>\n",
       "      <td>0.023256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025135</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.041041</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.004188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 934 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   class  red_1  red_2  red_3  red_4  red_5  red_6  red_7  \\\n",
       "0  bacterial_leaf_blight     19     47     93    143    223    231    305   \n",
       "1  bacterial_leaf_blight    153    210    261    308    349    391    392   \n",
       "2  bacterial_leaf_blight     14     35     62    145    205    290    337   \n",
       "3  bacterial_leaf_blight     14     14     40     53     89    106    194   \n",
       "4  bacterial_leaf_blight      0      1      3      4      1      6      6   \n",
       "\n",
       "   red_8  red_9  ...  feature_15  feature_16  feature_17  feature_18  \\\n",
       "0    314    330  ...    0.021457    0.009277    0.024865    0.004817   \n",
       "1    358    383  ...    0.019798    0.021939    0.024250    0.033492   \n",
       "2    325    338  ...    0.028761    0.049967    0.039179    0.039791   \n",
       "3    242    267  ...    0.017136    0.012084    0.020084    0.021049   \n",
       "4      9      8  ...    0.025135    0.003060    0.024900    0.048771   \n",
       "\n",
       "   feature_19  feature_20  feature_21  feature_22  feature_23  feature_24  \n",
       "0    0.017575    0.003713    0.003742    0.016868    0.031700    0.035992  \n",
       "1    0.027907    0.000967    0.009835    0.017213    0.006593    0.008411  \n",
       "2    0.023068    0.031944    0.021562    0.009072    0.019779    0.024128  \n",
       "3    0.013913    0.004487    0.017935    0.029116    0.023423    0.023256  \n",
       "4    0.041041    0.005773    0.034733    0.012531    0.014290    0.004188  \n",
       "\n",
       "[5 rows x 934 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5fb4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "glcm_features = complete_features.iloc[:, [0] + list(range(739, 751))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93653fd4-c7c1-4763-ab89-2102d2f29c4c",
   "metadata": {},
   "source": [
    "# Data Normalization and One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80096471-c274-4870-bea1-05b14edc651f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_dataframe(df):\n",
    "    # Create a MinMaxScaler instance\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Extract numerical columns (only numerical columns need normalization)\n",
    "    numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "    # Apply normalization to each numerical column\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b235b92a-228e-4021-975a-b860cafec988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josh\\AppData\\Local\\Temp\\ipykernel_19472\\3205247050.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>GLCM_Angular Second Moment</th>\n",
       "      <th>GLCM_Contrast</th>\n",
       "      <th>GLCM_Correlation</th>\n",
       "      <th>GLCM_Sum of Squares: Variance</th>\n",
       "      <th>GLCM_Inverse Difference Moment</th>\n",
       "      <th>GLCM_Sum Average</th>\n",
       "      <th>GLCM_Sum Variance</th>\n",
       "      <th>GLCM_Sum Entropy</th>\n",
       "      <th>GLCM_Entropy</th>\n",
       "      <th>GLCM_Difference Variance</th>\n",
       "      <th>GLCM_Difference Entropy</th>\n",
       "      <th>GLCM_Informational Measure of Correlation 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.633642</td>\n",
       "      <td>0.488753</td>\n",
       "      <td>0.325152</td>\n",
       "      <td>0.043351</td>\n",
       "      <td>0.507497</td>\n",
       "      <td>0.279646</td>\n",
       "      <td>0.963804</td>\n",
       "      <td>0.966145</td>\n",
       "      <td>0.008405</td>\n",
       "      <td>0.936177</td>\n",
       "      <td>0.883108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.450374</td>\n",
       "      <td>0.664801</td>\n",
       "      <td>0.352494</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>0.496963</td>\n",
       "      <td>0.322489</td>\n",
       "      <td>0.960193</td>\n",
       "      <td>0.953952</td>\n",
       "      <td>0.010659</td>\n",
       "      <td>0.900968</td>\n",
       "      <td>0.871647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.452155</td>\n",
       "      <td>0.575041</td>\n",
       "      <td>0.275746</td>\n",
       "      <td>0.049164</td>\n",
       "      <td>0.536162</td>\n",
       "      <td>0.245272</td>\n",
       "      <td>0.940202</td>\n",
       "      <td>0.929983</td>\n",
       "      <td>0.015624</td>\n",
       "      <td>0.882405</td>\n",
       "      <td>0.863882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.436445</td>\n",
       "      <td>0.628048</td>\n",
       "      <td>0.305714</td>\n",
       "      <td>0.025278</td>\n",
       "      <td>0.526046</td>\n",
       "      <td>0.276683</td>\n",
       "      <td>0.966273</td>\n",
       "      <td>0.964873</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>0.906937</td>\n",
       "      <td>0.878084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.362941</td>\n",
       "      <td>0.488924</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>0.037756</td>\n",
       "      <td>0.515550</td>\n",
       "      <td>0.154054</td>\n",
       "      <td>0.919304</td>\n",
       "      <td>0.932310</td>\n",
       "      <td>0.012845</td>\n",
       "      <td>0.879055</td>\n",
       "      <td>0.916672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   class  GLCM_Angular Second Moment  GLCM_Contrast  \\\n",
       "0  bacterial_leaf_blight                    0.000053       0.633642   \n",
       "1  bacterial_leaf_blight                    0.000049       0.450374   \n",
       "2  bacterial_leaf_blight                    0.000111       0.452155   \n",
       "3  bacterial_leaf_blight                    0.000026       0.436445   \n",
       "4  bacterial_leaf_blight                    0.000064       0.362941   \n",
       "\n",
       "   GLCM_Correlation  GLCM_Sum of Squares: Variance  \\\n",
       "0          0.488753                       0.325152   \n",
       "1          0.664801                       0.352494   \n",
       "2          0.575041                       0.275746   \n",
       "3          0.628048                       0.305714   \n",
       "4          0.488924                       0.177449   \n",
       "\n",
       "   GLCM_Inverse Difference Moment  GLCM_Sum Average  GLCM_Sum Variance  \\\n",
       "0                        0.043351          0.507497           0.279646   \n",
       "1                        0.032210          0.496963           0.322489   \n",
       "2                        0.049164          0.536162           0.245272   \n",
       "3                        0.025278          0.526046           0.276683   \n",
       "4                        0.037756          0.515550           0.154054   \n",
       "\n",
       "   GLCM_Sum Entropy  GLCM_Entropy  GLCM_Difference Variance  \\\n",
       "0          0.963804      0.966145                  0.008405   \n",
       "1          0.960193      0.953952                  0.010659   \n",
       "2          0.940202      0.929983                  0.015624   \n",
       "3          0.966273      0.964873                  0.008683   \n",
       "4          0.919304      0.932310                  0.012845   \n",
       "\n",
       "   GLCM_Difference Entropy  GLCM_Informational Measure of Correlation 1  \n",
       "0                 0.936177                                     0.883108  \n",
       "1                 0.900968                                     0.871647  \n",
       "2                 0.882405                                     0.863882  \n",
       "3                 0.906937                                     0.878084  \n",
       "4                 0.879055                                     0.916672  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_features = normalize_dataframe(glcm_features)\n",
    "normalized_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b339da2-35a0-455d-8db7-266f056dd5ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josh\\AppData\\Local\\Temp\\ipykernel_19472\\1409264267.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  normalized_features['class'] = label_encoder.fit_transform(normalized_features['class'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "normalized_features['class'] = label_encoder.fit_transform(normalized_features['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "097defd8-e917-45e2-a877-dfe178bfe066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>GLCM_Angular Second Moment</th>\n",
       "      <th>GLCM_Contrast</th>\n",
       "      <th>GLCM_Correlation</th>\n",
       "      <th>GLCM_Sum of Squares: Variance</th>\n",
       "      <th>GLCM_Inverse Difference Moment</th>\n",
       "      <th>GLCM_Sum Average</th>\n",
       "      <th>GLCM_Sum Variance</th>\n",
       "      <th>GLCM_Sum Entropy</th>\n",
       "      <th>GLCM_Entropy</th>\n",
       "      <th>GLCM_Difference Variance</th>\n",
       "      <th>GLCM_Difference Entropy</th>\n",
       "      <th>GLCM_Informational Measure of Correlation 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.633642</td>\n",
       "      <td>0.488753</td>\n",
       "      <td>0.325152</td>\n",
       "      <td>0.043351</td>\n",
       "      <td>0.507497</td>\n",
       "      <td>0.279646</td>\n",
       "      <td>0.963804</td>\n",
       "      <td>0.966145</td>\n",
       "      <td>0.008405</td>\n",
       "      <td>0.936177</td>\n",
       "      <td>0.883108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.450374</td>\n",
       "      <td>0.664801</td>\n",
       "      <td>0.352494</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>0.496963</td>\n",
       "      <td>0.322489</td>\n",
       "      <td>0.960193</td>\n",
       "      <td>0.953952</td>\n",
       "      <td>0.010659</td>\n",
       "      <td>0.900968</td>\n",
       "      <td>0.871647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.452155</td>\n",
       "      <td>0.575041</td>\n",
       "      <td>0.275746</td>\n",
       "      <td>0.049164</td>\n",
       "      <td>0.536162</td>\n",
       "      <td>0.245272</td>\n",
       "      <td>0.940202</td>\n",
       "      <td>0.929983</td>\n",
       "      <td>0.015624</td>\n",
       "      <td>0.882405</td>\n",
       "      <td>0.863882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.436445</td>\n",
       "      <td>0.628048</td>\n",
       "      <td>0.305714</td>\n",
       "      <td>0.025278</td>\n",
       "      <td>0.526046</td>\n",
       "      <td>0.276683</td>\n",
       "      <td>0.966273</td>\n",
       "      <td>0.964873</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>0.906937</td>\n",
       "      <td>0.878084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.362941</td>\n",
       "      <td>0.488924</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>0.037756</td>\n",
       "      <td>0.515550</td>\n",
       "      <td>0.154054</td>\n",
       "      <td>0.919304</td>\n",
       "      <td>0.932310</td>\n",
       "      <td>0.012845</td>\n",
       "      <td>0.879055</td>\n",
       "      <td>0.916672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  GLCM_Angular Second Moment  GLCM_Contrast  GLCM_Correlation  \\\n",
       "0      0                    0.000053       0.633642          0.488753   \n",
       "1      0                    0.000049       0.450374          0.664801   \n",
       "2      0                    0.000111       0.452155          0.575041   \n",
       "3      0                    0.000026       0.436445          0.628048   \n",
       "4      0                    0.000064       0.362941          0.488924   \n",
       "\n",
       "   GLCM_Sum of Squares: Variance  GLCM_Inverse Difference Moment  \\\n",
       "0                       0.325152                        0.043351   \n",
       "1                       0.352494                        0.032210   \n",
       "2                       0.275746                        0.049164   \n",
       "3                       0.305714                        0.025278   \n",
       "4                       0.177449                        0.037756   \n",
       "\n",
       "   GLCM_Sum Average  GLCM_Sum Variance  GLCM_Sum Entropy  GLCM_Entropy  \\\n",
       "0          0.507497           0.279646          0.963804      0.966145   \n",
       "1          0.496963           0.322489          0.960193      0.953952   \n",
       "2          0.536162           0.245272          0.940202      0.929983   \n",
       "3          0.526046           0.276683          0.966273      0.964873   \n",
       "4          0.515550           0.154054          0.919304      0.932310   \n",
       "\n",
       "   GLCM_Difference Variance  GLCM_Difference Entropy  \\\n",
       "0                  0.008405                 0.936177   \n",
       "1                  0.010659                 0.900968   \n",
       "2                  0.015624                 0.882405   \n",
       "3                  0.008683                 0.906937   \n",
       "4                  0.012845                 0.879055   \n",
       "\n",
       "   GLCM_Informational Measure of Correlation 1  \n",
       "0                                     0.883108  \n",
       "1                                     0.871647  \n",
       "2                                     0.863882  \n",
       "3                                     0.878084  \n",
       "4                                     0.916672  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77638c94-04af-43fb-a1db-0548a9c497cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acc8851d-dc36-4634-8d2f-8a3016c14ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def custom_train_test_split(dataframe, target_column, test_size=0.2, random_state=None):\n",
    "    # Separate features and labels\n",
    "    X = dataframe.drop(target_column, axis=1)  # Features\n",
    "    y = dataframe[target_column]  # Labels\n",
    "\n",
    "    # Perform the train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53594f6d-e920-4b66-91d8-7ba1ad60e4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = custom_train_test_split(normalized_features, 'class', test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "165bd148-e43b-401d-94a1-24f08ca4ef46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features (all columns except the first)\n",
    "x_d = normalized_features.iloc[:, 1:].values\n",
    "\n",
    "# Extract target variable (first column)\n",
    "y_d = normalized_features.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b4890-0ac4-4c6d-b3c1-9acf0c770916",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "705fec05-d73e-470c-9a2b-d71b16d96f36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "33/33 [==============================] - 2s 15ms/step - loss: 2.6076 - accuracy: 0.1422 - val_loss: 2.4838 - val_accuracy: 0.1964\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4049 - accuracy: 0.2719 - val_loss: 2.3204 - val_accuracy: 0.3393\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2004 - accuracy: 0.3244 - val_loss: 2.1717 - val_accuracy: 0.2857\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0328 - accuracy: 0.3540 - val_loss: 2.0535 - val_accuracy: 0.2679\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9052 - accuracy: 0.3855 - val_loss: 1.9592 - val_accuracy: 0.2857\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8027 - accuracy: 0.4198 - val_loss: 1.8706 - val_accuracy: 0.3036\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7211 - accuracy: 0.4628 - val_loss: 1.8125 - val_accuracy: 0.3571\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6568 - accuracy: 0.4771 - val_loss: 1.7590 - val_accuracy: 0.3571\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6055 - accuracy: 0.4914 - val_loss: 1.7213 - val_accuracy: 0.3750\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5594 - accuracy: 0.5010 - val_loss: 1.6907 - val_accuracy: 0.3929\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5222 - accuracy: 0.5057 - val_loss: 1.6804 - val_accuracy: 0.3750\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4892 - accuracy: 0.5191 - val_loss: 1.6397 - val_accuracy: 0.3929\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4637 - accuracy: 0.5363 - val_loss: 1.6218 - val_accuracy: 0.4464\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4412 - accuracy: 0.5458 - val_loss: 1.6095 - val_accuracy: 0.4643\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4113 - accuracy: 0.5468 - val_loss: 1.5884 - val_accuracy: 0.4464\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3904 - accuracy: 0.5544 - val_loss: 1.5819 - val_accuracy: 0.4643\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3719 - accuracy: 0.5639 - val_loss: 1.5693 - val_accuracy: 0.4643\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3502 - accuracy: 0.5639 - val_loss: 1.5524 - val_accuracy: 0.5179\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3374 - accuracy: 0.5811 - val_loss: 1.5597 - val_accuracy: 0.4643\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3229 - accuracy: 0.5906 - val_loss: 1.5399 - val_accuracy: 0.5536\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3098 - accuracy: 0.6002 - val_loss: 1.5362 - val_accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2951 - accuracy: 0.5992 - val_loss: 1.5171 - val_accuracy: 0.5714\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2817 - accuracy: 0.6002 - val_loss: 1.5311 - val_accuracy: 0.5179\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2710 - accuracy: 0.6088 - val_loss: 1.5122 - val_accuracy: 0.5536\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2575 - accuracy: 0.6135 - val_loss: 1.5018 - val_accuracy: 0.5357\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2447 - accuracy: 0.6050 - val_loss: 1.4938 - val_accuracy: 0.5714\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2397 - accuracy: 0.6078 - val_loss: 1.4950 - val_accuracy: 0.5357\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2332 - accuracy: 0.6116 - val_loss: 1.4889 - val_accuracy: 0.5536\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2200 - accuracy: 0.6040 - val_loss: 1.4702 - val_accuracy: 0.5536\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2135 - accuracy: 0.6135 - val_loss: 1.4733 - val_accuracy: 0.5714\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2025 - accuracy: 0.6221 - val_loss: 1.4693 - val_accuracy: 0.5536\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1958 - accuracy: 0.6135 - val_loss: 1.4554 - val_accuracy: 0.5357\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1817 - accuracy: 0.6212 - val_loss: 1.4527 - val_accuracy: 0.5536\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1754 - accuracy: 0.6260 - val_loss: 1.4471 - val_accuracy: 0.5536\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1666 - accuracy: 0.6212 - val_loss: 1.4381 - val_accuracy: 0.5714\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1595 - accuracy: 0.6307 - val_loss: 1.4381 - val_accuracy: 0.5714\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1505 - accuracy: 0.6250 - val_loss: 1.4246 - val_accuracy: 0.5893\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1410 - accuracy: 0.6374 - val_loss: 1.4308 - val_accuracy: 0.5714\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1396 - accuracy: 0.6317 - val_loss: 1.4183 - val_accuracy: 0.5714\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1257 - accuracy: 0.6336 - val_loss: 1.4097 - val_accuracy: 0.6071\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1181 - accuracy: 0.6336 - val_loss: 1.3988 - val_accuracy: 0.6071\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1107 - accuracy: 0.6412 - val_loss: 1.4125 - val_accuracy: 0.6071\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1015 - accuracy: 0.6441 - val_loss: 1.4032 - val_accuracy: 0.5536\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0962 - accuracy: 0.6355 - val_loss: 1.4017 - val_accuracy: 0.5893\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0918 - accuracy: 0.6412 - val_loss: 1.3959 - val_accuracy: 0.6071\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0828 - accuracy: 0.6489 - val_loss: 1.4012 - val_accuracy: 0.6071\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0742 - accuracy: 0.6489 - val_loss: 1.3891 - val_accuracy: 0.6071\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0654 - accuracy: 0.6527 - val_loss: 1.3841 - val_accuracy: 0.5893\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0651 - accuracy: 0.6536 - val_loss: 1.3948 - val_accuracy: 0.6071\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0548 - accuracy: 0.6508 - val_loss: 1.3837 - val_accuracy: 0.5893\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0488 - accuracy: 0.6555 - val_loss: 1.3707 - val_accuracy: 0.6250\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0404 - accuracy: 0.6670 - val_loss: 1.3689 - val_accuracy: 0.6071\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0363 - accuracy: 0.6641 - val_loss: 1.3603 - val_accuracy: 0.6071\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0307 - accuracy: 0.6508 - val_loss: 1.3752 - val_accuracy: 0.6071\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0236 - accuracy: 0.6565 - val_loss: 1.3602 - val_accuracy: 0.6071\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0169 - accuracy: 0.6670 - val_loss: 1.3501 - val_accuracy: 0.6071\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0169 - accuracy: 0.6584 - val_loss: 1.3629 - val_accuracy: 0.6071\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0122 - accuracy: 0.6708 - val_loss: 1.3367 - val_accuracy: 0.6071\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0010 - accuracy: 0.6641 - val_loss: 1.3439 - val_accuracy: 0.6250\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9943 - accuracy: 0.6660 - val_loss: 1.3474 - val_accuracy: 0.6250\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9878 - accuracy: 0.6746 - val_loss: 1.3346 - val_accuracy: 0.6250\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9883 - accuracy: 0.6679 - val_loss: 1.3380 - val_accuracy: 0.6250\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9757 - accuracy: 0.6794 - val_loss: 1.3402 - val_accuracy: 0.6250\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9716 - accuracy: 0.6794 - val_loss: 1.3301 - val_accuracy: 0.6071\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9715 - accuracy: 0.6689 - val_loss: 1.3390 - val_accuracy: 0.6250\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9647 - accuracy: 0.6708 - val_loss: 1.3179 - val_accuracy: 0.6250\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9627 - accuracy: 0.6679 - val_loss: 1.3268 - val_accuracy: 0.6250\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9577 - accuracy: 0.6794 - val_loss: 1.3368 - val_accuracy: 0.6071\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9479 - accuracy: 0.6765 - val_loss: 1.3221 - val_accuracy: 0.6250\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9462 - accuracy: 0.6832 - val_loss: 1.3538 - val_accuracy: 0.5893\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9392 - accuracy: 0.6737 - val_loss: 1.3169 - val_accuracy: 0.6250\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9382 - accuracy: 0.6899 - val_loss: 1.3142 - val_accuracy: 0.6071\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9340 - accuracy: 0.6842 - val_loss: 1.3095 - val_accuracy: 0.6250\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9256 - accuracy: 0.6889 - val_loss: 1.3230 - val_accuracy: 0.6071\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9217 - accuracy: 0.6870 - val_loss: 1.3205 - val_accuracy: 0.5893\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9213 - accuracy: 0.6823 - val_loss: 1.3080 - val_accuracy: 0.6071\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9139 - accuracy: 0.6927 - val_loss: 1.3015 - val_accuracy: 0.6250\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9133 - accuracy: 0.6889 - val_loss: 1.2984 - val_accuracy: 0.6071\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9062 - accuracy: 0.6908 - val_loss: 1.3033 - val_accuracy: 0.6071\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9008 - accuracy: 0.6880 - val_loss: 1.3016 - val_accuracy: 0.6250\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9014 - accuracy: 0.7004 - val_loss: 1.2948 - val_accuracy: 0.6071\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8960 - accuracy: 0.6880 - val_loss: 1.2809 - val_accuracy: 0.6071\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8903 - accuracy: 0.6985 - val_loss: 1.2934 - val_accuracy: 0.6071\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8850 - accuracy: 0.6937 - val_loss: 1.3020 - val_accuracy: 0.6071\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8809 - accuracy: 0.6994 - val_loss: 1.2783 - val_accuracy: 0.5893\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8818 - accuracy: 0.7004 - val_loss: 1.3005 - val_accuracy: 0.6250\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8797 - accuracy: 0.6975 - val_loss: 1.2896 - val_accuracy: 0.6071\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8756 - accuracy: 0.6947 - val_loss: 1.2735 - val_accuracy: 0.5893\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8676 - accuracy: 0.7004 - val_loss: 1.2894 - val_accuracy: 0.6250\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8665 - accuracy: 0.7004 - val_loss: 1.2729 - val_accuracy: 0.6071\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8597 - accuracy: 0.6994 - val_loss: 1.2861 - val_accuracy: 0.6071\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8588 - accuracy: 0.7137 - val_loss: 1.2693 - val_accuracy: 0.6071\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8561 - accuracy: 0.7023 - val_loss: 1.2874 - val_accuracy: 0.5893\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8536 - accuracy: 0.6937 - val_loss: 1.2637 - val_accuracy: 0.5893\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8486 - accuracy: 0.6985 - val_loss: 1.2811 - val_accuracy: 0.6071\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8461 - accuracy: 0.7071 - val_loss: 1.2718 - val_accuracy: 0.5893\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8456 - accuracy: 0.7032 - val_loss: 1.2909 - val_accuracy: 0.5893\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8429 - accuracy: 0.7071 - val_loss: 1.2617 - val_accuracy: 0.5893\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8406 - accuracy: 0.7118 - val_loss: 1.2620 - val_accuracy: 0.5893\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8443 - accuracy: 0.7042 - val_loss: 1.2720 - val_accuracy: 0.6250\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.1204 - accuracy: 0.6812\n",
      "\n",
      "Test accuracy: 0.6811594367027283\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Data in x_d and y_d\n",
    "X = x_d\n",
    "y = y_d\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the ANN model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(14, activation='softmax')  # 14 output classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.05)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c715fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions = model.predict(X_test_scaled)\n",
    "# predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "# np.savetxt('test_predictions.csv', np.hstack((predicted_labels.reshape(-1, 1), y_test.reshape(-1,1))), delimiter=',', fmt='%d')\n",
    "# # np.savetxt('actual labels.csv', y_test.reshape(-1,1), delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157b35a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Feature Selection PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9f8a956",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var: 0.99\n",
      "(1380, 6)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.4526 - accuracy: 0.1947 - val_loss: 2.3581 - val_accuracy: 0.3036\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2487 - accuracy: 0.2891 - val_loss: 2.2096 - val_accuracy: 0.3214\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1004 - accuracy: 0.3798 - val_loss: 2.0813 - val_accuracy: 0.3036\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9618 - accuracy: 0.4036 - val_loss: 1.9777 - val_accuracy: 0.3571\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8463 - accuracy: 0.4275 - val_loss: 1.8895 - val_accuracy: 0.3393\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7458 - accuracy: 0.4571 - val_loss: 1.8158 - val_accuracy: 0.3393\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6678 - accuracy: 0.4742 - val_loss: 1.7747 - val_accuracy: 0.3571\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6017 - accuracy: 0.4933 - val_loss: 1.7325 - val_accuracy: 0.3929\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5511 - accuracy: 0.5219 - val_loss: 1.7014 - val_accuracy: 0.3929\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5143 - accuracy: 0.5277 - val_loss: 1.6577 - val_accuracy: 0.4286\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4782 - accuracy: 0.5286 - val_loss: 1.6637 - val_accuracy: 0.4107\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4481 - accuracy: 0.5592 - val_loss: 1.6280 - val_accuracy: 0.4286\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4205 - accuracy: 0.5544 - val_loss: 1.6105 - val_accuracy: 0.4286\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4026 - accuracy: 0.5611 - val_loss: 1.5947 - val_accuracy: 0.4107\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3773 - accuracy: 0.5782 - val_loss: 1.5830 - val_accuracy: 0.4464\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3618 - accuracy: 0.5677 - val_loss: 1.5617 - val_accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3407 - accuracy: 0.5849 - val_loss: 1.5706 - val_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3263 - accuracy: 0.5887 - val_loss: 1.5476 - val_accuracy: 0.5179\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3077 - accuracy: 0.5897 - val_loss: 1.5340 - val_accuracy: 0.5179\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2907 - accuracy: 0.5897 - val_loss: 1.5366 - val_accuracy: 0.5179\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2768 - accuracy: 0.5964 - val_loss: 1.5110 - val_accuracy: 0.5536\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2611 - accuracy: 0.6107 - val_loss: 1.4992 - val_accuracy: 0.5536\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2487 - accuracy: 0.6069 - val_loss: 1.4996 - val_accuracy: 0.5536\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2387 - accuracy: 0.6097 - val_loss: 1.4937 - val_accuracy: 0.5357\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2251 - accuracy: 0.6231 - val_loss: 1.4744 - val_accuracy: 0.5536\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2123 - accuracy: 0.6097 - val_loss: 1.4888 - val_accuracy: 0.5536\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2017 - accuracy: 0.6202 - val_loss: 1.4639 - val_accuracy: 0.5357\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1986 - accuracy: 0.6183 - val_loss: 1.4481 - val_accuracy: 0.5357\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1828 - accuracy: 0.6269 - val_loss: 1.4659 - val_accuracy: 0.5179\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1737 - accuracy: 0.6212 - val_loss: 1.4514 - val_accuracy: 0.5714\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1604 - accuracy: 0.6202 - val_loss: 1.4488 - val_accuracy: 0.5714\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1538 - accuracy: 0.6250 - val_loss: 1.4416 - val_accuracy: 0.5714\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1411 - accuracy: 0.6345 - val_loss: 1.4266 - val_accuracy: 0.5536\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1366 - accuracy: 0.6279 - val_loss: 1.4398 - val_accuracy: 0.5714\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1286 - accuracy: 0.6345 - val_loss: 1.4282 - val_accuracy: 0.5714\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1176 - accuracy: 0.6317 - val_loss: 1.4097 - val_accuracy: 0.5714\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1092 - accuracy: 0.6460 - val_loss: 1.4133 - val_accuracy: 0.5536\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1062 - accuracy: 0.6393 - val_loss: 1.4182 - val_accuracy: 0.5893\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0938 - accuracy: 0.6431 - val_loss: 1.4059 - val_accuracy: 0.5714\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0892 - accuracy: 0.6412 - val_loss: 1.3964 - val_accuracy: 0.5714\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0811 - accuracy: 0.6527 - val_loss: 1.3875 - val_accuracy: 0.5714\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0734 - accuracy: 0.6498 - val_loss: 1.3915 - val_accuracy: 0.5893\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0693 - accuracy: 0.6498 - val_loss: 1.3815 - val_accuracy: 0.5714\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0585 - accuracy: 0.6603 - val_loss: 1.3880 - val_accuracy: 0.5714\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0513 - accuracy: 0.6489 - val_loss: 1.3889 - val_accuracy: 0.5179\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0459 - accuracy: 0.6613 - val_loss: 1.3638 - val_accuracy: 0.5714\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0421 - accuracy: 0.6594 - val_loss: 1.3788 - val_accuracy: 0.5714\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0363 - accuracy: 0.6660 - val_loss: 1.3612 - val_accuracy: 0.5714\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0328 - accuracy: 0.6670 - val_loss: 1.3681 - val_accuracy: 0.5714\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0283 - accuracy: 0.6613 - val_loss: 1.3535 - val_accuracy: 0.5536\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0204 - accuracy: 0.6603 - val_loss: 1.3624 - val_accuracy: 0.5893\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0127 - accuracy: 0.6632 - val_loss: 1.3495 - val_accuracy: 0.5714\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0100 - accuracy: 0.6718 - val_loss: 1.3564 - val_accuracy: 0.5714\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0027 - accuracy: 0.6660 - val_loss: 1.3395 - val_accuracy: 0.5714\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0051 - accuracy: 0.6689 - val_loss: 1.3656 - val_accuracy: 0.5714\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9986 - accuracy: 0.6641 - val_loss: 1.3494 - val_accuracy: 0.5893\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9890 - accuracy: 0.6765 - val_loss: 1.3356 - val_accuracy: 0.5714\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9846 - accuracy: 0.6803 - val_loss: 1.3435 - val_accuracy: 0.5893\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9808 - accuracy: 0.6794 - val_loss: 1.3357 - val_accuracy: 0.6071\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9798 - accuracy: 0.6823 - val_loss: 1.3217 - val_accuracy: 0.5893\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9709 - accuracy: 0.6823 - val_loss: 1.3245 - val_accuracy: 0.6250\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9714 - accuracy: 0.6813 - val_loss: 1.3373 - val_accuracy: 0.6250\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9660 - accuracy: 0.6832 - val_loss: 1.3212 - val_accuracy: 0.5714\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9611 - accuracy: 0.6880 - val_loss: 1.3238 - val_accuracy: 0.6429\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9565 - accuracy: 0.6899 - val_loss: 1.3216 - val_accuracy: 0.6429\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9509 - accuracy: 0.6937 - val_loss: 1.3115 - val_accuracy: 0.6429\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9498 - accuracy: 0.6880 - val_loss: 1.3256 - val_accuracy: 0.6071\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9428 - accuracy: 0.6994 - val_loss: 1.3107 - val_accuracy: 0.6429\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9445 - accuracy: 0.6908 - val_loss: 1.3174 - val_accuracy: 0.6429\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9379 - accuracy: 0.6966 - val_loss: 1.3247 - val_accuracy: 0.6429\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9358 - accuracy: 0.6937 - val_loss: 1.3193 - val_accuracy: 0.6071\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9270 - accuracy: 0.7080 - val_loss: 1.3168 - val_accuracy: 0.6250\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9308 - accuracy: 0.7004 - val_loss: 1.3049 - val_accuracy: 0.6071\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9239 - accuracy: 0.7061 - val_loss: 1.3117 - val_accuracy: 0.6429\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9185 - accuracy: 0.7061 - val_loss: 1.3049 - val_accuracy: 0.6250\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9128 - accuracy: 0.7080 - val_loss: 1.3018 - val_accuracy: 0.6250\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9131 - accuracy: 0.7061 - val_loss: 1.3076 - val_accuracy: 0.6071\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9081 - accuracy: 0.7099 - val_loss: 1.3071 - val_accuracy: 0.6071\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9058 - accuracy: 0.7052 - val_loss: 1.2848 - val_accuracy: 0.6250\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9010 - accuracy: 0.7090 - val_loss: 1.3053 - val_accuracy: 0.6429\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9002 - accuracy: 0.7080 - val_loss: 1.2814 - val_accuracy: 0.6071\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8973 - accuracy: 0.7099 - val_loss: 1.2905 - val_accuracy: 0.6071\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8931 - accuracy: 0.7109 - val_loss: 1.3012 - val_accuracy: 0.6250\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8871 - accuracy: 0.7118 - val_loss: 1.2703 - val_accuracy: 0.6071\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8878 - accuracy: 0.7118 - val_loss: 1.2828 - val_accuracy: 0.6250\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8839 - accuracy: 0.7118 - val_loss: 1.2835 - val_accuracy: 0.6071\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8808 - accuracy: 0.7118 - val_loss: 1.2804 - val_accuracy: 0.6250\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8801 - accuracy: 0.7156 - val_loss: 1.2787 - val_accuracy: 0.6429\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8800 - accuracy: 0.7128 - val_loss: 1.2876 - val_accuracy: 0.6250\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8708 - accuracy: 0.7166 - val_loss: 1.3044 - val_accuracy: 0.6250\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8722 - accuracy: 0.7176 - val_loss: 1.2735 - val_accuracy: 0.6250\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8692 - accuracy: 0.7166 - val_loss: 1.2916 - val_accuracy: 0.6250\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8651 - accuracy: 0.7147 - val_loss: 1.2899 - val_accuracy: 0.6250\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8653 - accuracy: 0.7166 - val_loss: 1.2645 - val_accuracy: 0.6250\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8625 - accuracy: 0.7195 - val_loss: 1.2957 - val_accuracy: 0.6250\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8685 - accuracy: 0.7176 - val_loss: 1.2676 - val_accuracy: 0.6071\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8552 - accuracy: 0.7223 - val_loss: 1.2606 - val_accuracy: 0.6250\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8524 - accuracy: 0.7223 - val_loss: 1.2707 - val_accuracy: 0.6071\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8491 - accuracy: 0.7214 - val_loss: 1.2801 - val_accuracy: 0.6250\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8518 - accuracy: 0.7233 - val_loss: 1.2734 - val_accuracy: 0.6250\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.1302 - accuracy: 0.6884\n",
      "\n",
      "Test accuracy: 0.6884058117866516\n",
      "var: 0.991\n",
      "(1380, 6)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 12ms/step - loss: 2.4637 - accuracy: 0.1460 - val_loss: 2.3628 - val_accuracy: 0.2143\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2476 - accuracy: 0.2996 - val_loss: 2.2181 - val_accuracy: 0.3214\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1018 - accuracy: 0.3445 - val_loss: 2.1108 - val_accuracy: 0.3036\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9796 - accuracy: 0.3721 - val_loss: 2.0149 - val_accuracy: 0.2857\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8722 - accuracy: 0.4141 - val_loss: 1.9306 - val_accuracy: 0.3036\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7824 - accuracy: 0.4265 - val_loss: 1.8605 - val_accuracy: 0.3393\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7030 - accuracy: 0.4504 - val_loss: 1.7863 - val_accuracy: 0.3393\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6364 - accuracy: 0.4599 - val_loss: 1.7376 - val_accuracy: 0.3393\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5867 - accuracy: 0.4914 - val_loss: 1.6853 - val_accuracy: 0.3929\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5410 - accuracy: 0.4866 - val_loss: 1.6615 - val_accuracy: 0.3750\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5064 - accuracy: 0.5105 - val_loss: 1.6297 - val_accuracy: 0.4286\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4749 - accuracy: 0.5210 - val_loss: 1.6127 - val_accuracy: 0.4107\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4432 - accuracy: 0.5296 - val_loss: 1.5806 - val_accuracy: 0.4286\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4204 - accuracy: 0.5344 - val_loss: 1.5629 - val_accuracy: 0.4643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3966 - accuracy: 0.5706 - val_loss: 1.5508 - val_accuracy: 0.4643\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3731 - accuracy: 0.5658 - val_loss: 1.5307 - val_accuracy: 0.4821\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3516 - accuracy: 0.5868 - val_loss: 1.5126 - val_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3328 - accuracy: 0.5926 - val_loss: 1.5025 - val_accuracy: 0.4821\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3178 - accuracy: 0.5840 - val_loss: 1.4853 - val_accuracy: 0.4821\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2999 - accuracy: 0.5992 - val_loss: 1.4614 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2825 - accuracy: 0.6002 - val_loss: 1.4454 - val_accuracy: 0.5179\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2684 - accuracy: 0.6040 - val_loss: 1.4390 - val_accuracy: 0.5179\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2563 - accuracy: 0.6040 - val_loss: 1.4305 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2437 - accuracy: 0.6145 - val_loss: 1.4196 - val_accuracy: 0.5179\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2298 - accuracy: 0.6107 - val_loss: 1.4121 - val_accuracy: 0.5179\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2180 - accuracy: 0.6193 - val_loss: 1.4004 - val_accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2068 - accuracy: 0.6174 - val_loss: 1.3823 - val_accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1959 - accuracy: 0.6183 - val_loss: 1.3783 - val_accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1866 - accuracy: 0.6279 - val_loss: 1.3828 - val_accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1764 - accuracy: 0.6221 - val_loss: 1.3631 - val_accuracy: 0.5179\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1650 - accuracy: 0.6174 - val_loss: 1.3581 - val_accuracy: 0.5179\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1596 - accuracy: 0.6307 - val_loss: 1.3593 - val_accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1484 - accuracy: 0.6307 - val_loss: 1.3399 - val_accuracy: 0.5179\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1392 - accuracy: 0.6317 - val_loss: 1.3468 - val_accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1300 - accuracy: 0.6355 - val_loss: 1.3280 - val_accuracy: 0.5179\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1245 - accuracy: 0.6336 - val_loss: 1.3362 - val_accuracy: 0.5000\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1176 - accuracy: 0.6307 - val_loss: 1.3272 - val_accuracy: 0.5000\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1082 - accuracy: 0.6460 - val_loss: 1.3180 - val_accuracy: 0.5000\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1013 - accuracy: 0.6460 - val_loss: 1.3069 - val_accuracy: 0.5000\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0967 - accuracy: 0.6498 - val_loss: 1.3120 - val_accuracy: 0.5000\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0876 - accuracy: 0.6431 - val_loss: 1.3122 - val_accuracy: 0.4821\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0850 - accuracy: 0.6508 - val_loss: 1.2933 - val_accuracy: 0.5536\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0759 - accuracy: 0.6565 - val_loss: 1.2874 - val_accuracy: 0.5179\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0719 - accuracy: 0.6546 - val_loss: 1.2907 - val_accuracy: 0.5000\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0612 - accuracy: 0.6603 - val_loss: 1.2818 - val_accuracy: 0.5179\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0559 - accuracy: 0.6689 - val_loss: 1.2717 - val_accuracy: 0.5179\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0507 - accuracy: 0.6622 - val_loss: 1.2660 - val_accuracy: 0.5714\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0464 - accuracy: 0.6603 - val_loss: 1.2641 - val_accuracy: 0.5179\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0405 - accuracy: 0.6679 - val_loss: 1.2541 - val_accuracy: 0.5536\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0346 - accuracy: 0.6613 - val_loss: 1.2536 - val_accuracy: 0.5357\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0282 - accuracy: 0.6727 - val_loss: 1.2467 - val_accuracy: 0.5179\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0227 - accuracy: 0.6641 - val_loss: 1.2484 - val_accuracy: 0.5000\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0172 - accuracy: 0.6698 - val_loss: 1.2380 - val_accuracy: 0.5000\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0123 - accuracy: 0.6727 - val_loss: 1.2323 - val_accuracy: 0.5357\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0066 - accuracy: 0.6775 - val_loss: 1.2373 - val_accuracy: 0.5179\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0020 - accuracy: 0.6756 - val_loss: 1.2275 - val_accuracy: 0.5357\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0006 - accuracy: 0.6756 - val_loss: 1.2368 - val_accuracy: 0.5179\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9921 - accuracy: 0.6746 - val_loss: 1.2135 - val_accuracy: 0.5000\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9919 - accuracy: 0.6765 - val_loss: 1.2137 - val_accuracy: 0.5179\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9837 - accuracy: 0.6794 - val_loss: 1.2129 - val_accuracy: 0.5357\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9822 - accuracy: 0.6842 - val_loss: 1.2089 - val_accuracy: 0.5357\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9745 - accuracy: 0.6842 - val_loss: 1.2111 - val_accuracy: 0.5179\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9749 - accuracy: 0.6851 - val_loss: 1.1874 - val_accuracy: 0.5357\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9726 - accuracy: 0.6861 - val_loss: 1.2148 - val_accuracy: 0.5357\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9691 - accuracy: 0.6851 - val_loss: 1.2053 - val_accuracy: 0.5357\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9657 - accuracy: 0.6880 - val_loss: 1.1960 - val_accuracy: 0.5179\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9607 - accuracy: 0.6851 - val_loss: 1.1980 - val_accuracy: 0.5179\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9519 - accuracy: 0.6842 - val_loss: 1.1891 - val_accuracy: 0.5357\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9486 - accuracy: 0.6966 - val_loss: 1.1844 - val_accuracy: 0.5536\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9483 - accuracy: 0.6975 - val_loss: 1.1850 - val_accuracy: 0.5536\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9399 - accuracy: 0.6994 - val_loss: 1.1895 - val_accuracy: 0.5536\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9387 - accuracy: 0.6899 - val_loss: 1.1895 - val_accuracy: 0.5536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9358 - accuracy: 0.6937 - val_loss: 1.1707 - val_accuracy: 0.5536\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9319 - accuracy: 0.7013 - val_loss: 1.1566 - val_accuracy: 0.5714\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9268 - accuracy: 0.7023 - val_loss: 1.1609 - val_accuracy: 0.5536\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9231 - accuracy: 0.7042 - val_loss: 1.1724 - val_accuracy: 0.5536\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9206 - accuracy: 0.6985 - val_loss: 1.1605 - val_accuracy: 0.5536\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9227 - accuracy: 0.6947 - val_loss: 1.1576 - val_accuracy: 0.5357\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9161 - accuracy: 0.6994 - val_loss: 1.1605 - val_accuracy: 0.5536\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9115 - accuracy: 0.7071 - val_loss: 1.1441 - val_accuracy: 0.5536\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.9069 - accuracy: 0.7004 - val_loss: 1.1486 - val_accuracy: 0.5357\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9002 - accuracy: 0.7061 - val_loss: 1.1569 - val_accuracy: 0.5714\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9009 - accuracy: 0.7109 - val_loss: 1.1345 - val_accuracy: 0.5357\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8957 - accuracy: 0.7013 - val_loss: 1.1479 - val_accuracy: 0.5536\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8911 - accuracy: 0.7071 - val_loss: 1.1329 - val_accuracy: 0.5536\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8879 - accuracy: 0.7128 - val_loss: 1.1323 - val_accuracy: 0.5714\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8886 - accuracy: 0.7071 - val_loss: 1.1331 - val_accuracy: 0.5714\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8858 - accuracy: 0.7128 - val_loss: 1.1293 - val_accuracy: 0.5357\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8826 - accuracy: 0.7052 - val_loss: 1.1413 - val_accuracy: 0.5536\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8760 - accuracy: 0.7109 - val_loss: 1.1263 - val_accuracy: 0.5893\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8743 - accuracy: 0.7147 - val_loss: 1.1035 - val_accuracy: 0.5714\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8718 - accuracy: 0.7137 - val_loss: 1.1347 - val_accuracy: 0.5714\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8663 - accuracy: 0.7090 - val_loss: 1.1083 - val_accuracy: 0.5893\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8658 - accuracy: 0.7147 - val_loss: 1.1161 - val_accuracy: 0.5893\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8612 - accuracy: 0.7147 - val_loss: 1.1255 - val_accuracy: 0.5714\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8565 - accuracy: 0.7099 - val_loss: 1.1177 - val_accuracy: 0.5893\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8546 - accuracy: 0.7099 - val_loss: 1.1301 - val_accuracy: 0.5714\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8574 - accuracy: 0.7156 - val_loss: 1.1113 - val_accuracy: 0.5893\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8485 - accuracy: 0.7156 - val_loss: 1.1295 - val_accuracy: 0.5893\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8482 - accuracy: 0.7166 - val_loss: 1.1203 - val_accuracy: 0.5893\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 1.0718 - accuracy: 0.7138\n",
      "\n",
      "Test accuracy: 0.7137681245803833\n",
      "var: 0.992\n",
      "(1380, 7)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 12ms/step - loss: 2.4989 - accuracy: 0.1937 - val_loss: 2.3856 - val_accuracy: 0.2857\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2462 - accuracy: 0.3311 - val_loss: 2.2085 - val_accuracy: 0.3571\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0843 - accuracy: 0.3769 - val_loss: 2.0894 - val_accuracy: 0.3393\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9548 - accuracy: 0.4046 - val_loss: 1.9901 - val_accuracy: 0.3571\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8494 - accuracy: 0.4370 - val_loss: 1.9147 - val_accuracy: 0.3571\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7683 - accuracy: 0.4380 - val_loss: 1.8545 - val_accuracy: 0.3750\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7030 - accuracy: 0.4523 - val_loss: 1.8070 - val_accuracy: 0.3929\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6529 - accuracy: 0.4647 - val_loss: 1.7754 - val_accuracy: 0.4107\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6073 - accuracy: 0.4781 - val_loss: 1.7327 - val_accuracy: 0.4464\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5689 - accuracy: 0.4885 - val_loss: 1.7217 - val_accuracy: 0.4107\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5375 - accuracy: 0.4990 - val_loss: 1.6940 - val_accuracy: 0.4107\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5059 - accuracy: 0.5134 - val_loss: 1.6706 - val_accuracy: 0.3929\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4819 - accuracy: 0.5191 - val_loss: 1.6671 - val_accuracy: 0.4107\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4549 - accuracy: 0.5334 - val_loss: 1.6410 - val_accuracy: 0.4821\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4318 - accuracy: 0.5344 - val_loss: 1.6146 - val_accuracy: 0.5179\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4100 - accuracy: 0.5525 - val_loss: 1.6069 - val_accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3844 - accuracy: 0.5677 - val_loss: 1.6023 - val_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3659 - accuracy: 0.5792 - val_loss: 1.5671 - val_accuracy: 0.5179\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3458 - accuracy: 0.5849 - val_loss: 1.5557 - val_accuracy: 0.5179\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3282 - accuracy: 0.6040 - val_loss: 1.5548 - val_accuracy: 0.5357\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3106 - accuracy: 0.6021 - val_loss: 1.5294 - val_accuracy: 0.5357\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2883 - accuracy: 0.6107 - val_loss: 1.5003 - val_accuracy: 0.5357\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2744 - accuracy: 0.6155 - val_loss: 1.4986 - val_accuracy: 0.5536\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2567 - accuracy: 0.6193 - val_loss: 1.4555 - val_accuracy: 0.5179\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2418 - accuracy: 0.6240 - val_loss: 1.4490 - val_accuracy: 0.5357\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2289 - accuracy: 0.6202 - val_loss: 1.4480 - val_accuracy: 0.5179\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2097 - accuracy: 0.6260 - val_loss: 1.4301 - val_accuracy: 0.5179\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1969 - accuracy: 0.6393 - val_loss: 1.4209 - val_accuracy: 0.5357\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1845 - accuracy: 0.6384 - val_loss: 1.4101 - val_accuracy: 0.5179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1706 - accuracy: 0.6374 - val_loss: 1.4006 - val_accuracy: 0.5179\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1595 - accuracy: 0.6374 - val_loss: 1.3761 - val_accuracy: 0.5357\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1473 - accuracy: 0.6460 - val_loss: 1.3881 - val_accuracy: 0.5357\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1367 - accuracy: 0.6479 - val_loss: 1.3567 - val_accuracy: 0.5714\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1281 - accuracy: 0.6574 - val_loss: 1.3525 - val_accuracy: 0.5536\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1165 - accuracy: 0.6546 - val_loss: 1.3401 - val_accuracy: 0.5714\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1057 - accuracy: 0.6632 - val_loss: 1.3377 - val_accuracy: 0.6071\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1000 - accuracy: 0.6594 - val_loss: 1.3546 - val_accuracy: 0.5714\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0885 - accuracy: 0.6555 - val_loss: 1.3192 - val_accuracy: 0.5536\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0781 - accuracy: 0.6594 - val_loss: 1.3164 - val_accuracy: 0.5893\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0710 - accuracy: 0.6632 - val_loss: 1.3172 - val_accuracy: 0.5893\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0628 - accuracy: 0.6708 - val_loss: 1.3070 - val_accuracy: 0.5893\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0562 - accuracy: 0.6660 - val_loss: 1.3037 - val_accuracy: 0.5893\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0498 - accuracy: 0.6689 - val_loss: 1.2923 - val_accuracy: 0.6071\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0400 - accuracy: 0.6708 - val_loss: 1.2820 - val_accuracy: 0.5714\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0315 - accuracy: 0.6765 - val_loss: 1.2765 - val_accuracy: 0.5893\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0225 - accuracy: 0.6851 - val_loss: 1.2720 - val_accuracy: 0.5893\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0195 - accuracy: 0.6813 - val_loss: 1.2739 - val_accuracy: 0.6250\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0101 - accuracy: 0.6851 - val_loss: 1.2661 - val_accuracy: 0.6071\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0059 - accuracy: 0.6861 - val_loss: 1.2430 - val_accuracy: 0.5893\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9959 - accuracy: 0.6889 - val_loss: 1.2628 - val_accuracy: 0.6071\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9929 - accuracy: 0.6937 - val_loss: 1.2497 - val_accuracy: 0.5893\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9873 - accuracy: 0.6918 - val_loss: 1.2546 - val_accuracy: 0.5893\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9818 - accuracy: 0.6937 - val_loss: 1.2353 - val_accuracy: 0.5893\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9765 - accuracy: 0.6918 - val_loss: 1.2518 - val_accuracy: 0.6071\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9709 - accuracy: 0.6918 - val_loss: 1.2364 - val_accuracy: 0.6071\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9649 - accuracy: 0.6947 - val_loss: 1.2325 - val_accuracy: 0.6607\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9582 - accuracy: 0.6956 - val_loss: 1.2320 - val_accuracy: 0.6071\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9571 - accuracy: 0.7013 - val_loss: 1.2466 - val_accuracy: 0.6071\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9553 - accuracy: 0.6889 - val_loss: 1.2259 - val_accuracy: 0.6429\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9460 - accuracy: 0.6956 - val_loss: 1.2233 - val_accuracy: 0.6250\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9390 - accuracy: 0.6966 - val_loss: 1.2266 - val_accuracy: 0.6250\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9404 - accuracy: 0.6966 - val_loss: 1.2045 - val_accuracy: 0.6429\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9407 - accuracy: 0.6966 - val_loss: 1.2338 - val_accuracy: 0.6071\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9267 - accuracy: 0.7004 - val_loss: 1.2257 - val_accuracy: 0.6071\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9221 - accuracy: 0.6994 - val_loss: 1.2244 - val_accuracy: 0.6250\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9237 - accuracy: 0.7023 - val_loss: 1.2171 - val_accuracy: 0.6429\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9180 - accuracy: 0.7118 - val_loss: 1.2104 - val_accuracy: 0.6250\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9112 - accuracy: 0.7128 - val_loss: 1.1998 - val_accuracy: 0.6250\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9093 - accuracy: 0.7052 - val_loss: 1.2177 - val_accuracy: 0.6071\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9065 - accuracy: 0.7071 - val_loss: 1.1909 - val_accuracy: 0.6250\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8979 - accuracy: 0.7118 - val_loss: 1.2275 - val_accuracy: 0.6071\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8972 - accuracy: 0.7099 - val_loss: 1.1911 - val_accuracy: 0.6071\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8916 - accuracy: 0.7023 - val_loss: 1.2099 - val_accuracy: 0.6429\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8905 - accuracy: 0.7090 - val_loss: 1.2082 - val_accuracy: 0.6429\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8849 - accuracy: 0.7061 - val_loss: 1.2009 - val_accuracy: 0.6250\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8768 - accuracy: 0.7147 - val_loss: 1.2042 - val_accuracy: 0.6429\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8760 - accuracy: 0.7147 - val_loss: 1.2052 - val_accuracy: 0.6429\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8742 - accuracy: 0.7090 - val_loss: 1.1948 - val_accuracy: 0.6429\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8685 - accuracy: 0.7061 - val_loss: 1.2091 - val_accuracy: 0.6250\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8654 - accuracy: 0.7109 - val_loss: 1.1854 - val_accuracy: 0.6429\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8617 - accuracy: 0.7147 - val_loss: 1.1891 - val_accuracy: 0.6429\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8604 - accuracy: 0.7023 - val_loss: 1.2048 - val_accuracy: 0.6250\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8581 - accuracy: 0.7147 - val_loss: 1.1906 - val_accuracy: 0.6250\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8582 - accuracy: 0.7195 - val_loss: 1.1890 - val_accuracy: 0.6429\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8486 - accuracy: 0.7166 - val_loss: 1.1936 - val_accuracy: 0.6250\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8454 - accuracy: 0.7185 - val_loss: 1.2124 - val_accuracy: 0.6250\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8413 - accuracy: 0.7185 - val_loss: 1.1843 - val_accuracy: 0.6429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8446 - accuracy: 0.7176 - val_loss: 1.1747 - val_accuracy: 0.6250\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8396 - accuracy: 0.7166 - val_loss: 1.1846 - val_accuracy: 0.6250\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8353 - accuracy: 0.7223 - val_loss: 1.1756 - val_accuracy: 0.6607\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8300 - accuracy: 0.7233 - val_loss: 1.1813 - val_accuracy: 0.6429\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8279 - accuracy: 0.7223 - val_loss: 1.1869 - val_accuracy: 0.6429\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8266 - accuracy: 0.7233 - val_loss: 1.1826 - val_accuracy: 0.6250\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8234 - accuracy: 0.7195 - val_loss: 1.1618 - val_accuracy: 0.6250\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8178 - accuracy: 0.7214 - val_loss: 1.2003 - val_accuracy: 0.6250\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8152 - accuracy: 0.7233 - val_loss: 1.1753 - val_accuracy: 0.6429\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8108 - accuracy: 0.7242 - val_loss: 1.1921 - val_accuracy: 0.6250\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8114 - accuracy: 0.7242 - val_loss: 1.1749 - val_accuracy: 0.6607\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8071 - accuracy: 0.7271 - val_loss: 1.1846 - val_accuracy: 0.6250\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8029 - accuracy: 0.7290 - val_loss: 1.1722 - val_accuracy: 0.6429\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0608 - accuracy: 0.6957\n",
      "\n",
      "Test accuracy: 0.695652186870575\n",
      "var: 0.993\n",
      "(1380, 7)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 11ms/step - loss: 2.6915 - accuracy: 0.1193 - val_loss: 2.5361 - val_accuracy: 0.1429\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4092 - accuracy: 0.2462 - val_loss: 2.3512 - val_accuracy: 0.2500\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2282 - accuracy: 0.3282 - val_loss: 2.1999 - val_accuracy: 0.2857\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0809 - accuracy: 0.3798 - val_loss: 2.0707 - val_accuracy: 0.3036\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9621 - accuracy: 0.4027 - val_loss: 1.9716 - val_accuracy: 0.3571\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8644 - accuracy: 0.4218 - val_loss: 1.8904 - val_accuracy: 0.3571\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7794 - accuracy: 0.4342 - val_loss: 1.8228 - val_accuracy: 0.3750\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7110 - accuracy: 0.4542 - val_loss: 1.7777 - val_accuracy: 0.3750\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6532 - accuracy: 0.4666 - val_loss: 1.7371 - val_accuracy: 0.3750\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6049 - accuracy: 0.4781 - val_loss: 1.7138 - val_accuracy: 0.3929\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5633 - accuracy: 0.5095 - val_loss: 1.6737 - val_accuracy: 0.3929\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5272 - accuracy: 0.5134 - val_loss: 1.6562 - val_accuracy: 0.4286\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4944 - accuracy: 0.5401 - val_loss: 1.6457 - val_accuracy: 0.4286\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4650 - accuracy: 0.5487 - val_loss: 1.6337 - val_accuracy: 0.4464\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4413 - accuracy: 0.5725 - val_loss: 1.6027 - val_accuracy: 0.4643\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4163 - accuracy: 0.5706 - val_loss: 1.5860 - val_accuracy: 0.4464\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3950 - accuracy: 0.5744 - val_loss: 1.5742 - val_accuracy: 0.4464\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3733 - accuracy: 0.5840 - val_loss: 1.5743 - val_accuracy: 0.4821\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3560 - accuracy: 0.5878 - val_loss: 1.5580 - val_accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3379 - accuracy: 0.5878 - val_loss: 1.5470 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3236 - accuracy: 0.5897 - val_loss: 1.5254 - val_accuracy: 0.4821\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3063 - accuracy: 0.5935 - val_loss: 1.5152 - val_accuracy: 0.4821\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2916 - accuracy: 0.5945 - val_loss: 1.5161 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2794 - accuracy: 0.6031 - val_loss: 1.4893 - val_accuracy: 0.5179\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2619 - accuracy: 0.6088 - val_loss: 1.5047 - val_accuracy: 0.5179\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2502 - accuracy: 0.6078 - val_loss: 1.4788 - val_accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2374 - accuracy: 0.6078 - val_loss: 1.4737 - val_accuracy: 0.5179\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2284 - accuracy: 0.6164 - val_loss: 1.4635 - val_accuracy: 0.5357\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2192 - accuracy: 0.6193 - val_loss: 1.4483 - val_accuracy: 0.5357\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2039 - accuracy: 0.6202 - val_loss: 1.4454 - val_accuracy: 0.5357\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1947 - accuracy: 0.6193 - val_loss: 1.4419 - val_accuracy: 0.5357\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1839 - accuracy: 0.6345 - val_loss: 1.4345 - val_accuracy: 0.5357\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1706 - accuracy: 0.6365 - val_loss: 1.4204 - val_accuracy: 0.5714\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1637 - accuracy: 0.6307 - val_loss: 1.4157 - val_accuracy: 0.5714\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1527 - accuracy: 0.6450 - val_loss: 1.4020 - val_accuracy: 0.5357\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1469 - accuracy: 0.6479 - val_loss: 1.4091 - val_accuracy: 0.5536\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1395 - accuracy: 0.6365 - val_loss: 1.3884 - val_accuracy: 0.5179\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1290 - accuracy: 0.6469 - val_loss: 1.3918 - val_accuracy: 0.5536\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1179 - accuracy: 0.6469 - val_loss: 1.3719 - val_accuracy: 0.5714\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1115 - accuracy: 0.6393 - val_loss: 1.3791 - val_accuracy: 0.5179\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1047 - accuracy: 0.6489 - val_loss: 1.3795 - val_accuracy: 0.5536\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0964 - accuracy: 0.6431 - val_loss: 1.3647 - val_accuracy: 0.5536\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0928 - accuracy: 0.6527 - val_loss: 1.3641 - val_accuracy: 0.5714\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0837 - accuracy: 0.6527 - val_loss: 1.3714 - val_accuracy: 0.5714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0748 - accuracy: 0.6508 - val_loss: 1.3477 - val_accuracy: 0.5357\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0686 - accuracy: 0.6565 - val_loss: 1.3445 - val_accuracy: 0.5357\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0631 - accuracy: 0.6565 - val_loss: 1.3409 - val_accuracy: 0.5893\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0532 - accuracy: 0.6565 - val_loss: 1.3418 - val_accuracy: 0.5357\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0528 - accuracy: 0.6555 - val_loss: 1.3375 - val_accuracy: 0.5179\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0466 - accuracy: 0.6632 - val_loss: 1.3267 - val_accuracy: 0.5536\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0427 - accuracy: 0.6613 - val_loss: 1.3239 - val_accuracy: 0.5536\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0358 - accuracy: 0.6689 - val_loss: 1.3261 - val_accuracy: 0.5536\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0338 - accuracy: 0.6651 - val_loss: 1.3271 - val_accuracy: 0.5714\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0189 - accuracy: 0.6698 - val_loss: 1.3097 - val_accuracy: 0.5536\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0170 - accuracy: 0.6603 - val_loss: 1.3298 - val_accuracy: 0.5536\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0106 - accuracy: 0.6718 - val_loss: 1.3206 - val_accuracy: 0.6071\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0053 - accuracy: 0.6775 - val_loss: 1.3128 - val_accuracy: 0.5357\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0041 - accuracy: 0.6746 - val_loss: 1.3160 - val_accuracy: 0.5893\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9979 - accuracy: 0.6660 - val_loss: 1.3181 - val_accuracy: 0.5536\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9967 - accuracy: 0.6727 - val_loss: 1.3063 - val_accuracy: 0.5893\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9915 - accuracy: 0.6746 - val_loss: 1.3060 - val_accuracy: 0.5714\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9888 - accuracy: 0.6765 - val_loss: 1.3005 - val_accuracy: 0.5536\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9792 - accuracy: 0.6765 - val_loss: 1.3021 - val_accuracy: 0.5893\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9773 - accuracy: 0.6698 - val_loss: 1.3114 - val_accuracy: 0.5536\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9702 - accuracy: 0.6870 - val_loss: 1.2981 - val_accuracy: 0.5714\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9639 - accuracy: 0.6842 - val_loss: 1.3169 - val_accuracy: 0.5536\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9594 - accuracy: 0.6832 - val_loss: 1.2974 - val_accuracy: 0.5536\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9611 - accuracy: 0.6823 - val_loss: 1.2853 - val_accuracy: 0.5536\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9581 - accuracy: 0.6756 - val_loss: 1.3023 - val_accuracy: 0.5536\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9477 - accuracy: 0.6947 - val_loss: 1.3018 - val_accuracy: 0.5714\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9458 - accuracy: 0.6813 - val_loss: 1.3064 - val_accuracy: 0.5357\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9445 - accuracy: 0.6851 - val_loss: 1.2888 - val_accuracy: 0.5536\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9389 - accuracy: 0.7004 - val_loss: 1.2998 - val_accuracy: 0.5714\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9323 - accuracy: 0.6927 - val_loss: 1.2920 - val_accuracy: 0.5536\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9297 - accuracy: 0.6899 - val_loss: 1.2995 - val_accuracy: 0.5714\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9293 - accuracy: 0.6985 - val_loss: 1.2960 - val_accuracy: 0.5714\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9217 - accuracy: 0.6956 - val_loss: 1.2882 - val_accuracy: 0.5536\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9171 - accuracy: 0.6994 - val_loss: 1.2993 - val_accuracy: 0.5893\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9138 - accuracy: 0.6947 - val_loss: 1.2890 - val_accuracy: 0.5536\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9118 - accuracy: 0.7023 - val_loss: 1.2885 - val_accuracy: 0.5714\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9095 - accuracy: 0.6937 - val_loss: 1.2951 - val_accuracy: 0.5893\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9059 - accuracy: 0.7032 - val_loss: 1.2926 - val_accuracy: 0.5893\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8972 - accuracy: 0.7023 - val_loss: 1.2868 - val_accuracy: 0.6071\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8954 - accuracy: 0.6985 - val_loss: 1.2923 - val_accuracy: 0.5893\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8931 - accuracy: 0.7023 - val_loss: 1.3003 - val_accuracy: 0.5714\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8897 - accuracy: 0.7099 - val_loss: 1.2983 - val_accuracy: 0.5893\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8876 - accuracy: 0.7090 - val_loss: 1.2862 - val_accuracy: 0.5893\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8859 - accuracy: 0.7090 - val_loss: 1.3037 - val_accuracy: 0.5893\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8812 - accuracy: 0.7071 - val_loss: 1.2854 - val_accuracy: 0.5893\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8758 - accuracy: 0.7128 - val_loss: 1.2901 - val_accuracy: 0.5893\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8716 - accuracy: 0.7071 - val_loss: 1.2702 - val_accuracy: 0.6071\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8688 - accuracy: 0.7080 - val_loss: 1.2868 - val_accuracy: 0.5714\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8660 - accuracy: 0.7118 - val_loss: 1.2777 - val_accuracy: 0.5893\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8628 - accuracy: 0.7166 - val_loss: 1.2814 - val_accuracy: 0.5893\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8644 - accuracy: 0.7185 - val_loss: 1.2785 - val_accuracy: 0.5893\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8596 - accuracy: 0.7109 - val_loss: 1.3020 - val_accuracy: 0.5714\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8548 - accuracy: 0.7137 - val_loss: 1.2872 - val_accuracy: 0.5893\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8548 - accuracy: 0.7099 - val_loss: 1.2840 - val_accuracy: 0.5893\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8464 - accuracy: 0.7156 - val_loss: 1.2753 - val_accuracy: 0.5893\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8498 - accuracy: 0.7176 - val_loss: 1.2632 - val_accuracy: 0.5893\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.0475 - accuracy: 0.6993\n",
      "\n",
      "Test accuracy: 0.6992753744125366\n",
      "var: 0.994\n",
      "(1380, 7)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 12ms/step - loss: 2.5540 - accuracy: 0.2233 - val_loss: 2.4699 - val_accuracy: 0.2143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3206 - accuracy: 0.3406 - val_loss: 2.2863 - val_accuracy: 0.2679\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1320 - accuracy: 0.4046 - val_loss: 2.1305 - val_accuracy: 0.3036\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9653 - accuracy: 0.4179 - val_loss: 1.9925 - val_accuracy: 0.3393\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8243 - accuracy: 0.4542 - val_loss: 1.8972 - val_accuracy: 0.3571\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7172 - accuracy: 0.4676 - val_loss: 1.8255 - val_accuracy: 0.4107\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6391 - accuracy: 0.4866 - val_loss: 1.7802 - val_accuracy: 0.4107\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5764 - accuracy: 0.5172 - val_loss: 1.7425 - val_accuracy: 0.4643\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5310 - accuracy: 0.5334 - val_loss: 1.7212 - val_accuracy: 0.4643\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4957 - accuracy: 0.5496 - val_loss: 1.6969 - val_accuracy: 0.4464\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4629 - accuracy: 0.5639 - val_loss: 1.6815 - val_accuracy: 0.4821\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4336 - accuracy: 0.5630 - val_loss: 1.6657 - val_accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4091 - accuracy: 0.5725 - val_loss: 1.6488 - val_accuracy: 0.4643\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3887 - accuracy: 0.5716 - val_loss: 1.6288 - val_accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3679 - accuracy: 0.5763 - val_loss: 1.6265 - val_accuracy: 0.4643\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3520 - accuracy: 0.5849 - val_loss: 1.6132 - val_accuracy: 0.4643\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3315 - accuracy: 0.5802 - val_loss: 1.6059 - val_accuracy: 0.4821\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3201 - accuracy: 0.5887 - val_loss: 1.5823 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3004 - accuracy: 0.5897 - val_loss: 1.5910 - val_accuracy: 0.4821\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2884 - accuracy: 0.5916 - val_loss: 1.5688 - val_accuracy: 0.5179\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2779 - accuracy: 0.5954 - val_loss: 1.5561 - val_accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2619 - accuracy: 0.5992 - val_loss: 1.5515 - val_accuracy: 0.5179\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2464 - accuracy: 0.6011 - val_loss: 1.5529 - val_accuracy: 0.5179\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2356 - accuracy: 0.6107 - val_loss: 1.5227 - val_accuracy: 0.5179\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2257 - accuracy: 0.6069 - val_loss: 1.5194 - val_accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2113 - accuracy: 0.6116 - val_loss: 1.5160 - val_accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2017 - accuracy: 0.6116 - val_loss: 1.5103 - val_accuracy: 0.5179\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1905 - accuracy: 0.6116 - val_loss: 1.5003 - val_accuracy: 0.5179\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1797 - accuracy: 0.6164 - val_loss: 1.4968 - val_accuracy: 0.5179\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1740 - accuracy: 0.6183 - val_loss: 1.4914 - val_accuracy: 0.5179\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1650 - accuracy: 0.6097 - val_loss: 1.4905 - val_accuracy: 0.5000\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1533 - accuracy: 0.6193 - val_loss: 1.4718 - val_accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1443 - accuracy: 0.6288 - val_loss: 1.4562 - val_accuracy: 0.5179\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1389 - accuracy: 0.6260 - val_loss: 1.4575 - val_accuracy: 0.5357\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1290 - accuracy: 0.6345 - val_loss: 1.4410 - val_accuracy: 0.5357\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1229 - accuracy: 0.6326 - val_loss: 1.4457 - val_accuracy: 0.5357\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1122 - accuracy: 0.6374 - val_loss: 1.4397 - val_accuracy: 0.5179\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1069 - accuracy: 0.6355 - val_loss: 1.4258 - val_accuracy: 0.5714\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0987 - accuracy: 0.6345 - val_loss: 1.4298 - val_accuracy: 0.5357\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0881 - accuracy: 0.6422 - val_loss: 1.4200 - val_accuracy: 0.5357\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0850 - accuracy: 0.6374 - val_loss: 1.4199 - val_accuracy: 0.5536\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0781 - accuracy: 0.6431 - val_loss: 1.3997 - val_accuracy: 0.5714\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0711 - accuracy: 0.6412 - val_loss: 1.4051 - val_accuracy: 0.5714\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0657 - accuracy: 0.6508 - val_loss: 1.3803 - val_accuracy: 0.5536\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0593 - accuracy: 0.6450 - val_loss: 1.3942 - val_accuracy: 0.5536\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0560 - accuracy: 0.6479 - val_loss: 1.3647 - val_accuracy: 0.5536\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0430 - accuracy: 0.6489 - val_loss: 1.3728 - val_accuracy: 0.5714\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0373 - accuracy: 0.6546 - val_loss: 1.3740 - val_accuracy: 0.5536\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0338 - accuracy: 0.6622 - val_loss: 1.3657 - val_accuracy: 0.5714\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0289 - accuracy: 0.6594 - val_loss: 1.3597 - val_accuracy: 0.5714\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0218 - accuracy: 0.6536 - val_loss: 1.3671 - val_accuracy: 0.5536\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0163 - accuracy: 0.6594 - val_loss: 1.3517 - val_accuracy: 0.5536\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0102 - accuracy: 0.6679 - val_loss: 1.3498 - val_accuracy: 0.5536\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0029 - accuracy: 0.6698 - val_loss: 1.3595 - val_accuracy: 0.5536\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9987 - accuracy: 0.6632 - val_loss: 1.3304 - val_accuracy: 0.5536\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9949 - accuracy: 0.6784 - val_loss: 1.3276 - val_accuracy: 0.5714\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9908 - accuracy: 0.6679 - val_loss: 1.3317 - val_accuracy: 0.5536\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9852 - accuracy: 0.6689 - val_loss: 1.3365 - val_accuracy: 0.5714\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9765 - accuracy: 0.6794 - val_loss: 1.3139 - val_accuracy: 0.5536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9761 - accuracy: 0.6765 - val_loss: 1.3164 - val_accuracy: 0.5714\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9696 - accuracy: 0.6813 - val_loss: 1.3085 - val_accuracy: 0.5714\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9658 - accuracy: 0.6813 - val_loss: 1.3065 - val_accuracy: 0.5714\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9592 - accuracy: 0.6832 - val_loss: 1.3177 - val_accuracy: 0.5179\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9548 - accuracy: 0.6851 - val_loss: 1.3026 - val_accuracy: 0.5893\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.9523 - accuracy: 0.6823 - val_loss: 1.2969 - val_accuracy: 0.5536\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9464 - accuracy: 0.6927 - val_loss: 1.3024 - val_accuracy: 0.5714\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9414 - accuracy: 0.6889 - val_loss: 1.2980 - val_accuracy: 0.5714\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9414 - accuracy: 0.6937 - val_loss: 1.2905 - val_accuracy: 0.5893\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9351 - accuracy: 0.6889 - val_loss: 1.3062 - val_accuracy: 0.5714\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9316 - accuracy: 0.6899 - val_loss: 1.3007 - val_accuracy: 0.5714\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9284 - accuracy: 0.6832 - val_loss: 1.2787 - val_accuracy: 0.5893\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9233 - accuracy: 0.6947 - val_loss: 1.2851 - val_accuracy: 0.5714\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9177 - accuracy: 0.6908 - val_loss: 1.2882 - val_accuracy: 0.5714\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9112 - accuracy: 0.6861 - val_loss: 1.2809 - val_accuracy: 0.5893\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9120 - accuracy: 0.6975 - val_loss: 1.2718 - val_accuracy: 0.5714\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9109 - accuracy: 0.6956 - val_loss: 1.2853 - val_accuracy: 0.5714\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9052 - accuracy: 0.6985 - val_loss: 1.2747 - val_accuracy: 0.5893\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9004 - accuracy: 0.6966 - val_loss: 1.2825 - val_accuracy: 0.5893\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8975 - accuracy: 0.6966 - val_loss: 1.2638 - val_accuracy: 0.5714\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8944 - accuracy: 0.7023 - val_loss: 1.2727 - val_accuracy: 0.5714\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8904 - accuracy: 0.6994 - val_loss: 1.2721 - val_accuracy: 0.5893\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8864 - accuracy: 0.6994 - val_loss: 1.2570 - val_accuracy: 0.5893\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8868 - accuracy: 0.7080 - val_loss: 1.2465 - val_accuracy: 0.5893\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8771 - accuracy: 0.7042 - val_loss: 1.2681 - val_accuracy: 0.5714\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8781 - accuracy: 0.7004 - val_loss: 1.2655 - val_accuracy: 0.5714\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8771 - accuracy: 0.7118 - val_loss: 1.2439 - val_accuracy: 0.5714\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8727 - accuracy: 0.7071 - val_loss: 1.2479 - val_accuracy: 0.5893\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8701 - accuracy: 0.7080 - val_loss: 1.2401 - val_accuracy: 0.5714\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8647 - accuracy: 0.7032 - val_loss: 1.2579 - val_accuracy: 0.5893\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8602 - accuracy: 0.7185 - val_loss: 1.2616 - val_accuracy: 0.5536\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8582 - accuracy: 0.7090 - val_loss: 1.2510 - val_accuracy: 0.5714\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8550 - accuracy: 0.7128 - val_loss: 1.2349 - val_accuracy: 0.6071\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8501 - accuracy: 0.7147 - val_loss: 1.2362 - val_accuracy: 0.5893\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8492 - accuracy: 0.7147 - val_loss: 1.2384 - val_accuracy: 0.5714\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8498 - accuracy: 0.7099 - val_loss: 1.2431 - val_accuracy: 0.5893\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8438 - accuracy: 0.7109 - val_loss: 1.2223 - val_accuracy: 0.5714\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8439 - accuracy: 0.7052 - val_loss: 1.2408 - val_accuracy: 0.5714\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8373 - accuracy: 0.7195 - val_loss: 1.2357 - val_accuracy: 0.6071\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8369 - accuracy: 0.7204 - val_loss: 1.2226 - val_accuracy: 0.6071\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8348 - accuracy: 0.7156 - val_loss: 1.2135 - val_accuracy: 0.6250\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 1.0655 - accuracy: 0.6848\n",
      "\n",
      "Test accuracy: 0.6847826242446899\n",
      "var: 0.995\n",
      "(1380, 7)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 14ms/step - loss: 2.6243 - accuracy: 0.1326 - val_loss: 2.4938 - val_accuracy: 0.1964\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3326 - accuracy: 0.2739 - val_loss: 2.3240 - val_accuracy: 0.3036\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1582 - accuracy: 0.3302 - val_loss: 2.1769 - val_accuracy: 0.2500\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0004 - accuracy: 0.3454 - val_loss: 2.0350 - val_accuracy: 0.2679\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8569 - accuracy: 0.3865 - val_loss: 1.9326 - val_accuracy: 0.3214\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7515 - accuracy: 0.4332 - val_loss: 1.8511 - val_accuracy: 0.3393\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6680 - accuracy: 0.4685 - val_loss: 1.8038 - val_accuracy: 0.3750\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6089 - accuracy: 0.4924 - val_loss: 1.7525 - val_accuracy: 0.4107\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5587 - accuracy: 0.5076 - val_loss: 1.7293 - val_accuracy: 0.3929\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5190 - accuracy: 0.5191 - val_loss: 1.6919 - val_accuracy: 0.4286\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4844 - accuracy: 0.5286 - val_loss: 1.6645 - val_accuracy: 0.4643\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4551 - accuracy: 0.5382 - val_loss: 1.6365 - val_accuracy: 0.4464\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4323 - accuracy: 0.5401 - val_loss: 1.6281 - val_accuracy: 0.4643\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4102 - accuracy: 0.5458 - val_loss: 1.6020 - val_accuracy: 0.4821\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3904 - accuracy: 0.5525 - val_loss: 1.5803 - val_accuracy: 0.4643\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3767 - accuracy: 0.5706 - val_loss: 1.5734 - val_accuracy: 0.4821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3575 - accuracy: 0.5630 - val_loss: 1.5356 - val_accuracy: 0.5179\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3406 - accuracy: 0.5792 - val_loss: 1.5281 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3230 - accuracy: 0.5868 - val_loss: 1.5161 - val_accuracy: 0.5536\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3075 - accuracy: 0.5859 - val_loss: 1.5098 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2944 - accuracy: 0.5859 - val_loss: 1.4891 - val_accuracy: 0.5536\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2818 - accuracy: 0.5954 - val_loss: 1.4775 - val_accuracy: 0.5357\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2728 - accuracy: 0.5878 - val_loss: 1.4670 - val_accuracy: 0.5357\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2602 - accuracy: 0.6069 - val_loss: 1.4618 - val_accuracy: 0.5714\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2512 - accuracy: 0.6059 - val_loss: 1.4475 - val_accuracy: 0.5893\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2371 - accuracy: 0.6069 - val_loss: 1.4522 - val_accuracy: 0.5536\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2289 - accuracy: 0.6069 - val_loss: 1.4269 - val_accuracy: 0.5893\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2191 - accuracy: 0.6107 - val_loss: 1.4284 - val_accuracy: 0.5714\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2074 - accuracy: 0.6155 - val_loss: 1.4084 - val_accuracy: 0.6071\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2013 - accuracy: 0.6164 - val_loss: 1.4170 - val_accuracy: 0.5714\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1899 - accuracy: 0.6193 - val_loss: 1.3969 - val_accuracy: 0.6071\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1806 - accuracy: 0.6174 - val_loss: 1.3998 - val_accuracy: 0.5893\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1734 - accuracy: 0.6221 - val_loss: 1.3826 - val_accuracy: 0.6071\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1635 - accuracy: 0.6288 - val_loss: 1.3827 - val_accuracy: 0.5714\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1578 - accuracy: 0.6326 - val_loss: 1.3767 - val_accuracy: 0.6071\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1479 - accuracy: 0.6384 - val_loss: 1.3718 - val_accuracy: 0.6250\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1415 - accuracy: 0.6260 - val_loss: 1.3553 - val_accuracy: 0.6250\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1344 - accuracy: 0.6384 - val_loss: 1.3436 - val_accuracy: 0.6071\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1258 - accuracy: 0.6431 - val_loss: 1.3494 - val_accuracy: 0.6250\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1220 - accuracy: 0.6393 - val_loss: 1.3389 - val_accuracy: 0.6250\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1137 - accuracy: 0.6441 - val_loss: 1.3317 - val_accuracy: 0.6071\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1039 - accuracy: 0.6469 - val_loss: 1.3280 - val_accuracy: 0.6250\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1020 - accuracy: 0.6412 - val_loss: 1.3211 - val_accuracy: 0.6250\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0936 - accuracy: 0.6489 - val_loss: 1.3325 - val_accuracy: 0.6250\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0858 - accuracy: 0.6517 - val_loss: 1.3041 - val_accuracy: 0.6250\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0803 - accuracy: 0.6527 - val_loss: 1.3034 - val_accuracy: 0.6429\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0730 - accuracy: 0.6574 - val_loss: 1.2981 - val_accuracy: 0.6250\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0657 - accuracy: 0.6594 - val_loss: 1.2973 - val_accuracy: 0.6429\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0625 - accuracy: 0.6584 - val_loss: 1.2987 - val_accuracy: 0.6429\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0590 - accuracy: 0.6555 - val_loss: 1.2889 - val_accuracy: 0.6250\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0500 - accuracy: 0.6536 - val_loss: 1.2867 - val_accuracy: 0.6429\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0413 - accuracy: 0.6689 - val_loss: 1.2904 - val_accuracy: 0.6250\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0389 - accuracy: 0.6622 - val_loss: 1.2752 - val_accuracy: 0.6429\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0344 - accuracy: 0.6698 - val_loss: 1.2838 - val_accuracy: 0.6250\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0292 - accuracy: 0.6698 - val_loss: 1.2831 - val_accuracy: 0.6429\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0225 - accuracy: 0.6651 - val_loss: 1.2709 - val_accuracy: 0.6250\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0138 - accuracy: 0.6775 - val_loss: 1.2701 - val_accuracy: 0.6429\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0117 - accuracy: 0.6775 - val_loss: 1.2579 - val_accuracy: 0.6250\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0031 - accuracy: 0.6794 - val_loss: 1.2674 - val_accuracy: 0.6250\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0024 - accuracy: 0.6765 - val_loss: 1.2644 - val_accuracy: 0.6250\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9978 - accuracy: 0.6851 - val_loss: 1.2615 - val_accuracy: 0.6250\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9908 - accuracy: 0.6851 - val_loss: 1.2649 - val_accuracy: 0.6071\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9850 - accuracy: 0.6937 - val_loss: 1.2332 - val_accuracy: 0.6250\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9789 - accuracy: 0.6918 - val_loss: 1.2558 - val_accuracy: 0.6250\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9772 - accuracy: 0.6880 - val_loss: 1.2313 - val_accuracy: 0.6429\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9731 - accuracy: 0.6947 - val_loss: 1.2341 - val_accuracy: 0.6429\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9688 - accuracy: 0.6889 - val_loss: 1.2413 - val_accuracy: 0.6429\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9656 - accuracy: 0.6956 - val_loss: 1.2468 - val_accuracy: 0.6429\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9589 - accuracy: 0.6889 - val_loss: 1.2285 - val_accuracy: 0.6250\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9541 - accuracy: 0.7004 - val_loss: 1.2556 - val_accuracy: 0.6250\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9495 - accuracy: 0.6975 - val_loss: 1.2283 - val_accuracy: 0.6429\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9466 - accuracy: 0.7013 - val_loss: 1.2315 - val_accuracy: 0.6250\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9389 - accuracy: 0.6937 - val_loss: 1.2344 - val_accuracy: 0.6250\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9350 - accuracy: 0.7118 - val_loss: 1.2351 - val_accuracy: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9320 - accuracy: 0.7042 - val_loss: 1.2193 - val_accuracy: 0.6250\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9268 - accuracy: 0.7109 - val_loss: 1.2303 - val_accuracy: 0.6250\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9212 - accuracy: 0.7042 - val_loss: 1.2300 - val_accuracy: 0.6250\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9154 - accuracy: 0.7052 - val_loss: 1.2062 - val_accuracy: 0.6429\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9155 - accuracy: 0.7061 - val_loss: 1.2187 - val_accuracy: 0.6250\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9093 - accuracy: 0.7185 - val_loss: 1.2097 - val_accuracy: 0.6429\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9045 - accuracy: 0.7071 - val_loss: 1.1995 - val_accuracy: 0.6429\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9042 - accuracy: 0.7099 - val_loss: 1.2088 - val_accuracy: 0.6429\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8972 - accuracy: 0.7128 - val_loss: 1.2198 - val_accuracy: 0.6429\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8947 - accuracy: 0.7137 - val_loss: 1.1876 - val_accuracy: 0.6429\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8961 - accuracy: 0.7128 - val_loss: 1.1923 - val_accuracy: 0.6429\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8855 - accuracy: 0.7137 - val_loss: 1.1982 - val_accuracy: 0.6429\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8839 - accuracy: 0.7204 - val_loss: 1.1890 - val_accuracy: 0.6607\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8790 - accuracy: 0.7233 - val_loss: 1.2008 - val_accuracy: 0.6607\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8785 - accuracy: 0.7128 - val_loss: 1.1829 - val_accuracy: 0.6607\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8747 - accuracy: 0.7128 - val_loss: 1.1930 - val_accuracy: 0.6607\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8696 - accuracy: 0.7166 - val_loss: 1.1973 - val_accuracy: 0.6607\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8653 - accuracy: 0.7271 - val_loss: 1.1955 - val_accuracy: 0.6429\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8659 - accuracy: 0.7204 - val_loss: 1.1968 - val_accuracy: 0.6607\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8637 - accuracy: 0.7204 - val_loss: 1.1869 - val_accuracy: 0.6607\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8603 - accuracy: 0.7195 - val_loss: 1.1741 - val_accuracy: 0.6429\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8545 - accuracy: 0.7223 - val_loss: 1.1814 - val_accuracy: 0.6429\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8541 - accuracy: 0.7214 - val_loss: 1.1973 - val_accuracy: 0.6429\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8488 - accuracy: 0.7281 - val_loss: 1.1934 - val_accuracy: 0.6429\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8438 - accuracy: 0.7185 - val_loss: 1.1767 - val_accuracy: 0.6607\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8437 - accuracy: 0.7223 - val_loss: 1.1831 - val_accuracy: 0.6429\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.0634 - accuracy: 0.6775\n",
      "\n",
      "Test accuracy: 0.6775362491607666\n",
      "var: 0.996\n",
      "(1380, 7)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 12ms/step - loss: 2.5311 - accuracy: 0.1555 - val_loss: 2.3826 - val_accuracy: 0.2143\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.2489 - accuracy: 0.3206 - val_loss: 2.2608 - val_accuracy: 0.2500\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.1176 - accuracy: 0.3578 - val_loss: 2.1751 - val_accuracy: 0.2857\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.0110 - accuracy: 0.4027 - val_loss: 2.0866 - val_accuracy: 0.2857\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9049 - accuracy: 0.4189 - val_loss: 1.9974 - val_accuracy: 0.2857\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8063 - accuracy: 0.4380 - val_loss: 1.9129 - val_accuracy: 0.3393\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7257 - accuracy: 0.4571 - val_loss: 1.8421 - val_accuracy: 0.3393\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6581 - accuracy: 0.4752 - val_loss: 1.7859 - val_accuracy: 0.3929\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6049 - accuracy: 0.5000 - val_loss: 1.7415 - val_accuracy: 0.4107\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5551 - accuracy: 0.5200 - val_loss: 1.7074 - val_accuracy: 0.4464\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5198 - accuracy: 0.5200 - val_loss: 1.6892 - val_accuracy: 0.4643\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4893 - accuracy: 0.5353 - val_loss: 1.6533 - val_accuracy: 0.4643\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4637 - accuracy: 0.5477 - val_loss: 1.6359 - val_accuracy: 0.4821\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4366 - accuracy: 0.5534 - val_loss: 1.6269 - val_accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4188 - accuracy: 0.5506 - val_loss: 1.5809 - val_accuracy: 0.5536\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3992 - accuracy: 0.5620 - val_loss: 1.5978 - val_accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3806 - accuracy: 0.5735 - val_loss: 1.5712 - val_accuracy: 0.5179\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3651 - accuracy: 0.5658 - val_loss: 1.5716 - val_accuracy: 0.4821\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3473 - accuracy: 0.5773 - val_loss: 1.5661 - val_accuracy: 0.5357\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3316 - accuracy: 0.5802 - val_loss: 1.5504 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3249 - accuracy: 0.5792 - val_loss: 1.5394 - val_accuracy: 0.5357\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3053 - accuracy: 0.5887 - val_loss: 1.5143 - val_accuracy: 0.5179\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2935 - accuracy: 0.5935 - val_loss: 1.5248 - val_accuracy: 0.5179\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2847 - accuracy: 0.5897 - val_loss: 1.5107 - val_accuracy: 0.5179\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2662 - accuracy: 0.6088 - val_loss: 1.4966 - val_accuracy: 0.5179\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2534 - accuracy: 0.6069 - val_loss: 1.4898 - val_accuracy: 0.5357\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2445 - accuracy: 0.6126 - val_loss: 1.4922 - val_accuracy: 0.5179\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2314 - accuracy: 0.6078 - val_loss: 1.4706 - val_accuracy: 0.5357\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2220 - accuracy: 0.6097 - val_loss: 1.4673 - val_accuracy: 0.5179\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2143 - accuracy: 0.6155 - val_loss: 1.4504 - val_accuracy: 0.5179\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2041 - accuracy: 0.6155 - val_loss: 1.4673 - val_accuracy: 0.5536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1911 - accuracy: 0.6183 - val_loss: 1.4536 - val_accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1835 - accuracy: 0.6269 - val_loss: 1.4389 - val_accuracy: 0.5714\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1730 - accuracy: 0.6279 - val_loss: 1.4397 - val_accuracy: 0.5714\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1651 - accuracy: 0.6269 - val_loss: 1.4339 - val_accuracy: 0.6071\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1536 - accuracy: 0.6317 - val_loss: 1.4320 - val_accuracy: 0.5536\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1468 - accuracy: 0.6317 - val_loss: 1.4190 - val_accuracy: 0.5714\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1354 - accuracy: 0.6403 - val_loss: 1.4188 - val_accuracy: 0.5714\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1305 - accuracy: 0.6365 - val_loss: 1.4043 - val_accuracy: 0.5536\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1221 - accuracy: 0.6384 - val_loss: 1.4096 - val_accuracy: 0.5714\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1136 - accuracy: 0.6403 - val_loss: 1.3930 - val_accuracy: 0.5536\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1066 - accuracy: 0.6422 - val_loss: 1.4063 - val_accuracy: 0.6071\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1007 - accuracy: 0.6441 - val_loss: 1.3895 - val_accuracy: 0.5893\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0933 - accuracy: 0.6498 - val_loss: 1.3951 - val_accuracy: 0.5536\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0853 - accuracy: 0.6546 - val_loss: 1.3815 - val_accuracy: 0.6250\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0751 - accuracy: 0.6546 - val_loss: 1.3681 - val_accuracy: 0.5714\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0742 - accuracy: 0.6479 - val_loss: 1.3692 - val_accuracy: 0.5714\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0625 - accuracy: 0.6546 - val_loss: 1.3660 - val_accuracy: 0.5536\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0576 - accuracy: 0.6555 - val_loss: 1.3718 - val_accuracy: 0.5536\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0538 - accuracy: 0.6546 - val_loss: 1.3763 - val_accuracy: 0.5893\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0495 - accuracy: 0.6489 - val_loss: 1.3598 - val_accuracy: 0.5893\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0383 - accuracy: 0.6613 - val_loss: 1.3627 - val_accuracy: 0.5893\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0355 - accuracy: 0.6594 - val_loss: 1.3688 - val_accuracy: 0.5714\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0300 - accuracy: 0.6594 - val_loss: 1.3600 - val_accuracy: 0.5714\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0261 - accuracy: 0.6574 - val_loss: 1.3611 - val_accuracy: 0.6071\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0170 - accuracy: 0.6708 - val_loss: 1.3487 - val_accuracy: 0.5893\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0112 - accuracy: 0.6651 - val_loss: 1.3406 - val_accuracy: 0.6071\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0096 - accuracy: 0.6746 - val_loss: 1.3496 - val_accuracy: 0.5893\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0007 - accuracy: 0.6698 - val_loss: 1.3335 - val_accuracy: 0.6250\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9932 - accuracy: 0.6746 - val_loss: 1.3311 - val_accuracy: 0.6071\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9885 - accuracy: 0.6727 - val_loss: 1.3385 - val_accuracy: 0.5893\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9868 - accuracy: 0.6727 - val_loss: 1.3335 - val_accuracy: 0.5893\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9802 - accuracy: 0.6784 - val_loss: 1.3337 - val_accuracy: 0.5893\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9786 - accuracy: 0.6718 - val_loss: 1.3317 - val_accuracy: 0.5893\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9732 - accuracy: 0.6823 - val_loss: 1.3272 - val_accuracy: 0.6250\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9684 - accuracy: 0.6784 - val_loss: 1.3395 - val_accuracy: 0.5714\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9607 - accuracy: 0.6851 - val_loss: 1.3234 - val_accuracy: 0.5893\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9556 - accuracy: 0.6880 - val_loss: 1.3114 - val_accuracy: 0.6071\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9545 - accuracy: 0.6832 - val_loss: 1.3163 - val_accuracy: 0.6250\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9511 - accuracy: 0.6842 - val_loss: 1.3038 - val_accuracy: 0.6071\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9454 - accuracy: 0.6842 - val_loss: 1.3079 - val_accuracy: 0.6071\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9420 - accuracy: 0.6908 - val_loss: 1.3111 - val_accuracy: 0.5893\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9382 - accuracy: 0.6966 - val_loss: 1.3091 - val_accuracy: 0.6071\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9340 - accuracy: 0.6937 - val_loss: 1.3080 - val_accuracy: 0.5893\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9310 - accuracy: 0.6889 - val_loss: 1.3005 - val_accuracy: 0.6071\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9257 - accuracy: 0.6966 - val_loss: 1.3053 - val_accuracy: 0.5714\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9262 - accuracy: 0.6851 - val_loss: 1.2997 - val_accuracy: 0.6250\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9188 - accuracy: 0.7013 - val_loss: 1.2997 - val_accuracy: 0.6250\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9116 - accuracy: 0.6966 - val_loss: 1.3158 - val_accuracy: 0.6071\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9087 - accuracy: 0.6975 - val_loss: 1.2933 - val_accuracy: 0.6250\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9046 - accuracy: 0.6927 - val_loss: 1.2892 - val_accuracy: 0.6429\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9023 - accuracy: 0.7042 - val_loss: 1.2823 - val_accuracy: 0.6429\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8985 - accuracy: 0.7023 - val_loss: 1.2926 - val_accuracy: 0.6071\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8961 - accuracy: 0.7023 - val_loss: 1.2942 - val_accuracy: 0.6429\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8930 - accuracy: 0.7013 - val_loss: 1.2965 - val_accuracy: 0.6250\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8912 - accuracy: 0.7071 - val_loss: 1.2746 - val_accuracy: 0.6786\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8868 - accuracy: 0.7032 - val_loss: 1.2863 - val_accuracy: 0.6250\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8793 - accuracy: 0.7099 - val_loss: 1.2747 - val_accuracy: 0.6250\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8778 - accuracy: 0.7090 - val_loss: 1.2846 - val_accuracy: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8764 - accuracy: 0.7013 - val_loss: 1.2999 - val_accuracy: 0.6071\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8734 - accuracy: 0.7090 - val_loss: 1.2732 - val_accuracy: 0.6607\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8663 - accuracy: 0.7090 - val_loss: 1.2888 - val_accuracy: 0.6250\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8686 - accuracy: 0.7052 - val_loss: 1.3178 - val_accuracy: 0.6071\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8693 - accuracy: 0.7042 - val_loss: 1.2770 - val_accuracy: 0.6429\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8604 - accuracy: 0.7109 - val_loss: 1.2794 - val_accuracy: 0.6429\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8570 - accuracy: 0.7080 - val_loss: 1.2700 - val_accuracy: 0.6071\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8551 - accuracy: 0.7166 - val_loss: 1.2723 - val_accuracy: 0.6250\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8517 - accuracy: 0.7147 - val_loss: 1.2710 - val_accuracy: 0.6250\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8498 - accuracy: 0.7147 - val_loss: 1.2805 - val_accuracy: 0.6071\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8470 - accuracy: 0.7118 - val_loss: 1.2856 - val_accuracy: 0.6071\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.0892 - accuracy: 0.6920\n",
      "\n",
      "Test accuracy: 0.6920289993286133\n",
      "var: 0.997\n",
      "(1380, 8)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 12ms/step - loss: 2.5298 - accuracy: 0.1326 - val_loss: 2.3855 - val_accuracy: 0.2321\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2787 - accuracy: 0.2968 - val_loss: 2.2290 - val_accuracy: 0.2321\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1383 - accuracy: 0.3397 - val_loss: 2.1064 - val_accuracy: 0.2857\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0236 - accuracy: 0.3550 - val_loss: 1.9985 - val_accuracy: 0.3036\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9115 - accuracy: 0.3826 - val_loss: 1.8961 - val_accuracy: 0.3036\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.8107 - accuracy: 0.4074 - val_loss: 1.8141 - val_accuracy: 0.3571\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7279 - accuracy: 0.4313 - val_loss: 1.7555 - val_accuracy: 0.3393\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6595 - accuracy: 0.4418 - val_loss: 1.6998 - val_accuracy: 0.3750\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6040 - accuracy: 0.4475 - val_loss: 1.6624 - val_accuracy: 0.3750\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5591 - accuracy: 0.4742 - val_loss: 1.6314 - val_accuracy: 0.3929\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5187 - accuracy: 0.4885 - val_loss: 1.5869 - val_accuracy: 0.4464\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4823 - accuracy: 0.5181 - val_loss: 1.5769 - val_accuracy: 0.4643\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4516 - accuracy: 0.5429 - val_loss: 1.5531 - val_accuracy: 0.4464\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4212 - accuracy: 0.5382 - val_loss: 1.5373 - val_accuracy: 0.4643\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3941 - accuracy: 0.5706 - val_loss: 1.5070 - val_accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3762 - accuracy: 0.5782 - val_loss: 1.5071 - val_accuracy: 0.4821\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3514 - accuracy: 0.5811 - val_loss: 1.4991 - val_accuracy: 0.4643\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3322 - accuracy: 0.5868 - val_loss: 1.4917 - val_accuracy: 0.4821\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3234 - accuracy: 0.5782 - val_loss: 1.4847 - val_accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3025 - accuracy: 0.5916 - val_loss: 1.4663 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2866 - accuracy: 0.5964 - val_loss: 1.4686 - val_accuracy: 0.4821\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2737 - accuracy: 0.5983 - val_loss: 1.4578 - val_accuracy: 0.5179\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2605 - accuracy: 0.5992 - val_loss: 1.4456 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2497 - accuracy: 0.5992 - val_loss: 1.4476 - val_accuracy: 0.4821\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2395 - accuracy: 0.6069 - val_loss: 1.4345 - val_accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2273 - accuracy: 0.6031 - val_loss: 1.4314 - val_accuracy: 0.4821\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2150 - accuracy: 0.6145 - val_loss: 1.4175 - val_accuracy: 0.5357\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2053 - accuracy: 0.6164 - val_loss: 1.4301 - val_accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1963 - accuracy: 0.6174 - val_loss: 1.4089 - val_accuracy: 0.5179\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1924 - accuracy: 0.6088 - val_loss: 1.4111 - val_accuracy: 0.5179\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1808 - accuracy: 0.6221 - val_loss: 1.4010 - val_accuracy: 0.5000\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1693 - accuracy: 0.6155 - val_loss: 1.3910 - val_accuracy: 0.5357\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1611 - accuracy: 0.6298 - val_loss: 1.3812 - val_accuracy: 0.5357\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1535 - accuracy: 0.6307 - val_loss: 1.3879 - val_accuracy: 0.5179\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1475 - accuracy: 0.6326 - val_loss: 1.3800 - val_accuracy: 0.5357\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1361 - accuracy: 0.6326 - val_loss: 1.3741 - val_accuracy: 0.5357\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1289 - accuracy: 0.6345 - val_loss: 1.3694 - val_accuracy: 0.5357\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1231 - accuracy: 0.6307 - val_loss: 1.3622 - val_accuracy: 0.5357\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1179 - accuracy: 0.6393 - val_loss: 1.3684 - val_accuracy: 0.5357\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1066 - accuracy: 0.6403 - val_loss: 1.3593 - val_accuracy: 0.5536\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0996 - accuracy: 0.6469 - val_loss: 1.3501 - val_accuracy: 0.5357\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0941 - accuracy: 0.6403 - val_loss: 1.3463 - val_accuracy: 0.5357\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0833 - accuracy: 0.6469 - val_loss: 1.3487 - val_accuracy: 0.5357\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0804 - accuracy: 0.6584 - val_loss: 1.3373 - val_accuracy: 0.5536\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0715 - accuracy: 0.6660 - val_loss: 1.3377 - val_accuracy: 0.5357\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0613 - accuracy: 0.6546 - val_loss: 1.3323 - val_accuracy: 0.5536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.0607 - accuracy: 0.6632 - val_loss: 1.3244 - val_accuracy: 0.5357\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0534 - accuracy: 0.6574 - val_loss: 1.3368 - val_accuracy: 0.5536\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0445 - accuracy: 0.6641 - val_loss: 1.3266 - val_accuracy: 0.5357\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0378 - accuracy: 0.6670 - val_loss: 1.3197 - val_accuracy: 0.5536\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0360 - accuracy: 0.6679 - val_loss: 1.3125 - val_accuracy: 0.5893\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0278 - accuracy: 0.6689 - val_loss: 1.3174 - val_accuracy: 0.5536\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0233 - accuracy: 0.6708 - val_loss: 1.3170 - val_accuracy: 0.5714\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0194 - accuracy: 0.6651 - val_loss: 1.3183 - val_accuracy: 0.5536\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0152 - accuracy: 0.6746 - val_loss: 1.3026 - val_accuracy: 0.5536\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0074 - accuracy: 0.6651 - val_loss: 1.3136 - val_accuracy: 0.5536\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0007 - accuracy: 0.6775 - val_loss: 1.3072 - val_accuracy: 0.5536\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0020 - accuracy: 0.6756 - val_loss: 1.3109 - val_accuracy: 0.5357\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9970 - accuracy: 0.6765 - val_loss: 1.3056 - val_accuracy: 0.5714\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9895 - accuracy: 0.6775 - val_loss: 1.2992 - val_accuracy: 0.5536\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9823 - accuracy: 0.6823 - val_loss: 1.3021 - val_accuracy: 0.5536\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9799 - accuracy: 0.6775 - val_loss: 1.2954 - val_accuracy: 0.5714\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9754 - accuracy: 0.6842 - val_loss: 1.2964 - val_accuracy: 0.5714\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9730 - accuracy: 0.6746 - val_loss: 1.2892 - val_accuracy: 0.5714\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9669 - accuracy: 0.6813 - val_loss: 1.3002 - val_accuracy: 0.5714\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9627 - accuracy: 0.6889 - val_loss: 1.2892 - val_accuracy: 0.5536\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9596 - accuracy: 0.6870 - val_loss: 1.3025 - val_accuracy: 0.5714\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9540 - accuracy: 0.6966 - val_loss: 1.2934 - val_accuracy: 0.5714\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9525 - accuracy: 0.6794 - val_loss: 1.3006 - val_accuracy: 0.5357\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9489 - accuracy: 0.6823 - val_loss: 1.2880 - val_accuracy: 0.5357\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9446 - accuracy: 0.6832 - val_loss: 1.2812 - val_accuracy: 0.6071\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9389 - accuracy: 0.6870 - val_loss: 1.2785 - val_accuracy: 0.5536\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9372 - accuracy: 0.6870 - val_loss: 1.2763 - val_accuracy: 0.5893\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9318 - accuracy: 0.6889 - val_loss: 1.2768 - val_accuracy: 0.5714\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9289 - accuracy: 0.6956 - val_loss: 1.2786 - val_accuracy: 0.5893\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9269 - accuracy: 0.6870 - val_loss: 1.2757 - val_accuracy: 0.5893\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9218 - accuracy: 0.6927 - val_loss: 1.2577 - val_accuracy: 0.5714\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9211 - accuracy: 0.6994 - val_loss: 1.2655 - val_accuracy: 0.5893\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9150 - accuracy: 0.6794 - val_loss: 1.2480 - val_accuracy: 0.5893\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9162 - accuracy: 0.7004 - val_loss: 1.2734 - val_accuracy: 0.6071\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9131 - accuracy: 0.6937 - val_loss: 1.2437 - val_accuracy: 0.6071\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9075 - accuracy: 0.6899 - val_loss: 1.2680 - val_accuracy: 0.6250\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9044 - accuracy: 0.6927 - val_loss: 1.2718 - val_accuracy: 0.5714\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8973 - accuracy: 0.6985 - val_loss: 1.2473 - val_accuracy: 0.5893\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8946 - accuracy: 0.6985 - val_loss: 1.2560 - val_accuracy: 0.6250\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8918 - accuracy: 0.6956 - val_loss: 1.2486 - val_accuracy: 0.6250\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8885 - accuracy: 0.7004 - val_loss: 1.2603 - val_accuracy: 0.6071\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8866 - accuracy: 0.7052 - val_loss: 1.2535 - val_accuracy: 0.6071\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8870 - accuracy: 0.7071 - val_loss: 1.2672 - val_accuracy: 0.5893\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8800 - accuracy: 0.7042 - val_loss: 1.2586 - val_accuracy: 0.6071\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8804 - accuracy: 0.6994 - val_loss: 1.2696 - val_accuracy: 0.5714\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8789 - accuracy: 0.7032 - val_loss: 1.2541 - val_accuracy: 0.5893\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8716 - accuracy: 0.7128 - val_loss: 1.2532 - val_accuracy: 0.6071\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8711 - accuracy: 0.7071 - val_loss: 1.2537 - val_accuracy: 0.6071\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8682 - accuracy: 0.7032 - val_loss: 1.2383 - val_accuracy: 0.6071\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8655 - accuracy: 0.7137 - val_loss: 1.2528 - val_accuracy: 0.5893\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8625 - accuracy: 0.7032 - val_loss: 1.2467 - val_accuracy: 0.6250\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8619 - accuracy: 0.7080 - val_loss: 1.2407 - val_accuracy: 0.6250\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8572 - accuracy: 0.7099 - val_loss: 1.2549 - val_accuracy: 0.5893\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8527 - accuracy: 0.7071 - val_loss: 1.2433 - val_accuracy: 0.6250\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.0704 - accuracy: 0.6812\n",
      "\n",
      "Test accuracy: 0.6811594367027283\n",
      "var: 0.998\n",
      "(1380, 8)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 14ms/step - loss: 2.5811 - accuracy: 0.1183 - val_loss: 2.5034 - val_accuracy: 0.1429\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3208 - accuracy: 0.2328 - val_loss: 2.3491 - val_accuracy: 0.2679\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1660 - accuracy: 0.3282 - val_loss: 2.2181 - val_accuracy: 0.2857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0165 - accuracy: 0.3845 - val_loss: 2.0926 - val_accuracy: 0.3036\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8780 - accuracy: 0.4160 - val_loss: 1.9815 - val_accuracy: 0.3393\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7607 - accuracy: 0.4475 - val_loss: 1.8806 - val_accuracy: 0.3571\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6735 - accuracy: 0.4618 - val_loss: 1.8054 - val_accuracy: 0.3750\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6037 - accuracy: 0.4876 - val_loss: 1.7513 - val_accuracy: 0.4107\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5524 - accuracy: 0.5153 - val_loss: 1.7128 - val_accuracy: 0.4464\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5085 - accuracy: 0.5315 - val_loss: 1.6813 - val_accuracy: 0.4286\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4765 - accuracy: 0.5410 - val_loss: 1.6559 - val_accuracy: 0.4643\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4446 - accuracy: 0.5468 - val_loss: 1.6150 - val_accuracy: 0.4643\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4143 - accuracy: 0.5620 - val_loss: 1.6099 - val_accuracy: 0.4643\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3926 - accuracy: 0.5658 - val_loss: 1.5866 - val_accuracy: 0.4821\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3715 - accuracy: 0.5792 - val_loss: 1.5643 - val_accuracy: 0.4821\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3459 - accuracy: 0.5849 - val_loss: 1.5659 - val_accuracy: 0.5179\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3259 - accuracy: 0.5878 - val_loss: 1.5517 - val_accuracy: 0.4821\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3099 - accuracy: 0.5859 - val_loss: 1.5377 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2929 - accuracy: 0.5983 - val_loss: 1.5347 - val_accuracy: 0.5179\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2783 - accuracy: 0.5964 - val_loss: 1.5120 - val_accuracy: 0.5536\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2651 - accuracy: 0.6088 - val_loss: 1.5059 - val_accuracy: 0.4821\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2497 - accuracy: 0.6107 - val_loss: 1.5004 - val_accuracy: 0.5357\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2363 - accuracy: 0.6164 - val_loss: 1.4901 - val_accuracy: 0.5179\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2231 - accuracy: 0.6107 - val_loss: 1.4804 - val_accuracy: 0.5536\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2119 - accuracy: 0.6164 - val_loss: 1.4570 - val_accuracy: 0.5536\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2004 - accuracy: 0.6260 - val_loss: 1.4504 - val_accuracy: 0.5714\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1916 - accuracy: 0.6212 - val_loss: 1.4570 - val_accuracy: 0.5357\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1799 - accuracy: 0.6422 - val_loss: 1.4634 - val_accuracy: 0.5536\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1678 - accuracy: 0.6298 - val_loss: 1.4313 - val_accuracy: 0.5357\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1609 - accuracy: 0.6365 - val_loss: 1.4322 - val_accuracy: 0.5714\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1448 - accuracy: 0.6460 - val_loss: 1.4357 - val_accuracy: 0.5536\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1361 - accuracy: 0.6479 - val_loss: 1.4207 - val_accuracy: 0.5714\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1280 - accuracy: 0.6431 - val_loss: 1.4219 - val_accuracy: 0.5536\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1181 - accuracy: 0.6527 - val_loss: 1.3927 - val_accuracy: 0.5893\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1075 - accuracy: 0.6450 - val_loss: 1.3986 - val_accuracy: 0.5536\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0974 - accuracy: 0.6517 - val_loss: 1.3746 - val_accuracy: 0.5893\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0929 - accuracy: 0.6460 - val_loss: 1.3892 - val_accuracy: 0.5536\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0830 - accuracy: 0.6594 - val_loss: 1.3711 - val_accuracy: 0.5714\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0763 - accuracy: 0.6536 - val_loss: 1.3915 - val_accuracy: 0.5714\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0719 - accuracy: 0.6613 - val_loss: 1.3571 - val_accuracy: 0.6250\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0625 - accuracy: 0.6546 - val_loss: 1.3678 - val_accuracy: 0.5536\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0532 - accuracy: 0.6698 - val_loss: 1.3697 - val_accuracy: 0.5714\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0473 - accuracy: 0.6565 - val_loss: 1.3493 - val_accuracy: 0.5714\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0411 - accuracy: 0.6632 - val_loss: 1.3618 - val_accuracy: 0.5536\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0315 - accuracy: 0.6698 - val_loss: 1.3364 - val_accuracy: 0.6071\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0264 - accuracy: 0.6679 - val_loss: 1.3555 - val_accuracy: 0.5536\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0231 - accuracy: 0.6632 - val_loss: 1.3509 - val_accuracy: 0.5893\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0127 - accuracy: 0.6756 - val_loss: 1.3489 - val_accuracy: 0.5893\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0078 - accuracy: 0.6765 - val_loss: 1.3396 - val_accuracy: 0.5536\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0042 - accuracy: 0.6632 - val_loss: 1.3180 - val_accuracy: 0.6250\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9969 - accuracy: 0.6765 - val_loss: 1.3216 - val_accuracy: 0.5893\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9922 - accuracy: 0.6718 - val_loss: 1.3501 - val_accuracy: 0.5536\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9884 - accuracy: 0.6765 - val_loss: 1.3124 - val_accuracy: 0.6429\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9815 - accuracy: 0.6727 - val_loss: 1.3561 - val_accuracy: 0.5714\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9747 - accuracy: 0.6851 - val_loss: 1.3202 - val_accuracy: 0.5714\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9698 - accuracy: 0.6813 - val_loss: 1.3178 - val_accuracy: 0.6250\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9679 - accuracy: 0.6861 - val_loss: 1.3219 - val_accuracy: 0.5893\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9653 - accuracy: 0.6737 - val_loss: 1.3195 - val_accuracy: 0.5893\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9583 - accuracy: 0.6880 - val_loss: 1.3094 - val_accuracy: 0.5893\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9507 - accuracy: 0.6889 - val_loss: 1.3203 - val_accuracy: 0.5893\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9489 - accuracy: 0.6861 - val_loss: 1.3235 - val_accuracy: 0.5893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9412 - accuracy: 0.6889 - val_loss: 1.3260 - val_accuracy: 0.5714\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9404 - accuracy: 0.6861 - val_loss: 1.3202 - val_accuracy: 0.5714\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9390 - accuracy: 0.6880 - val_loss: 1.3213 - val_accuracy: 0.5893\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9283 - accuracy: 0.6908 - val_loss: 1.3088 - val_accuracy: 0.5714\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9263 - accuracy: 0.6947 - val_loss: 1.3215 - val_accuracy: 0.5714\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9243 - accuracy: 0.6927 - val_loss: 1.3248 - val_accuracy: 0.5714\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9198 - accuracy: 0.6956 - val_loss: 1.3320 - val_accuracy: 0.5714\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9151 - accuracy: 0.7061 - val_loss: 1.3020 - val_accuracy: 0.5893\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9125 - accuracy: 0.6994 - val_loss: 1.3143 - val_accuracy: 0.6071\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9102 - accuracy: 0.6937 - val_loss: 1.3256 - val_accuracy: 0.5714\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9077 - accuracy: 0.6975 - val_loss: 1.3067 - val_accuracy: 0.6071\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9034 - accuracy: 0.7013 - val_loss: 1.3155 - val_accuracy: 0.6071\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8998 - accuracy: 0.6966 - val_loss: 1.3124 - val_accuracy: 0.5893\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8915 - accuracy: 0.7137 - val_loss: 1.3142 - val_accuracy: 0.5893\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8911 - accuracy: 0.7004 - val_loss: 1.3442 - val_accuracy: 0.5714\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8933 - accuracy: 0.7023 - val_loss: 1.3008 - val_accuracy: 0.5714\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8874 - accuracy: 0.7099 - val_loss: 1.3137 - val_accuracy: 0.5714\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8799 - accuracy: 0.7080 - val_loss: 1.3240 - val_accuracy: 0.5893\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8756 - accuracy: 0.7156 - val_loss: 1.3101 - val_accuracy: 0.6071\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8783 - accuracy: 0.6994 - val_loss: 1.3044 - val_accuracy: 0.5893\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8748 - accuracy: 0.7195 - val_loss: 1.3310 - val_accuracy: 0.5893\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8731 - accuracy: 0.7147 - val_loss: 1.3189 - val_accuracy: 0.5714\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8704 - accuracy: 0.7099 - val_loss: 1.3030 - val_accuracy: 0.5893\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8695 - accuracy: 0.7052 - val_loss: 1.3122 - val_accuracy: 0.5893\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8620 - accuracy: 0.7261 - val_loss: 1.3094 - val_accuracy: 0.5893\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8562 - accuracy: 0.7137 - val_loss: 1.3169 - val_accuracy: 0.5893\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8559 - accuracy: 0.7137 - val_loss: 1.3033 - val_accuracy: 0.5893\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8503 - accuracy: 0.7118 - val_loss: 1.3136 - val_accuracy: 0.5893\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8474 - accuracy: 0.7156 - val_loss: 1.3130 - val_accuracy: 0.5893\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8495 - accuracy: 0.7128 - val_loss: 1.3176 - val_accuracy: 0.5893\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8453 - accuracy: 0.7128 - val_loss: 1.3152 - val_accuracy: 0.5893\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8437 - accuracy: 0.7204 - val_loss: 1.2944 - val_accuracy: 0.5893\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8357 - accuracy: 0.7185 - val_loss: 1.3093 - val_accuracy: 0.5893\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8369 - accuracy: 0.7185 - val_loss: 1.3339 - val_accuracy: 0.5893\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8307 - accuracy: 0.7214 - val_loss: 1.3107 - val_accuracy: 0.5893\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8282 - accuracy: 0.7281 - val_loss: 1.3185 - val_accuracy: 0.5893\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8296 - accuracy: 0.7214 - val_loss: 1.3041 - val_accuracy: 0.6071\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8263 - accuracy: 0.7204 - val_loss: 1.3208 - val_accuracy: 0.5893\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8235 - accuracy: 0.7195 - val_loss: 1.3225 - val_accuracy: 0.5893\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.1564 - accuracy: 0.6630\n",
      "\n",
      "Test accuracy: 0.6630434989929199\n",
      "var: 0.999\n",
      "(1380, 9)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 15ms/step - loss: 2.5227 - accuracy: 0.1135 - val_loss: 2.4651 - val_accuracy: 0.1071\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 2.3144 - accuracy: 0.2156 - val_loss: 2.3275 - val_accuracy: 0.2143\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1620 - accuracy: 0.2891 - val_loss: 2.1778 - val_accuracy: 0.2679\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0132 - accuracy: 0.3588 - val_loss: 2.0482 - val_accuracy: 0.3571\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8851 - accuracy: 0.4055 - val_loss: 1.9230 - val_accuracy: 0.3571\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7800 - accuracy: 0.4313 - val_loss: 1.8338 - val_accuracy: 0.3929\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6934 - accuracy: 0.4542 - val_loss: 1.7476 - val_accuracy: 0.3929\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6327 - accuracy: 0.4695 - val_loss: 1.6927 - val_accuracy: 0.4464\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5796 - accuracy: 0.4733 - val_loss: 1.6499 - val_accuracy: 0.4821\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5384 - accuracy: 0.5038 - val_loss: 1.6203 - val_accuracy: 0.4821\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5011 - accuracy: 0.5010 - val_loss: 1.6001 - val_accuracy: 0.4821\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4717 - accuracy: 0.5248 - val_loss: 1.5709 - val_accuracy: 0.4821\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4472 - accuracy: 0.5344 - val_loss: 1.5607 - val_accuracy: 0.5536\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4190 - accuracy: 0.5534 - val_loss: 1.5420 - val_accuracy: 0.5714\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4013 - accuracy: 0.5525 - val_loss: 1.5354 - val_accuracy: 0.5714\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3809 - accuracy: 0.5611 - val_loss: 1.5250 - val_accuracy: 0.5714\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3587 - accuracy: 0.5773 - val_loss: 1.5153 - val_accuracy: 0.5714\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3378 - accuracy: 0.5840 - val_loss: 1.5087 - val_accuracy: 0.5893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3219 - accuracy: 0.5849 - val_loss: 1.4843 - val_accuracy: 0.5893\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3053 - accuracy: 0.5906 - val_loss: 1.4719 - val_accuracy: 0.5714\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2876 - accuracy: 0.5878 - val_loss: 1.4662 - val_accuracy: 0.6071\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2725 - accuracy: 0.5964 - val_loss: 1.4672 - val_accuracy: 0.5714\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.2562 - accuracy: 0.6059 - val_loss: 1.4598 - val_accuracy: 0.5714\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2432 - accuracy: 0.6088 - val_loss: 1.4373 - val_accuracy: 0.6071\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2310 - accuracy: 0.6126 - val_loss: 1.4430 - val_accuracy: 0.5714\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2204 - accuracy: 0.6135 - val_loss: 1.4243 - val_accuracy: 0.6071\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2073 - accuracy: 0.6126 - val_loss: 1.4228 - val_accuracy: 0.5714\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1951 - accuracy: 0.6135 - val_loss: 1.4296 - val_accuracy: 0.5893\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1820 - accuracy: 0.6221 - val_loss: 1.4219 - val_accuracy: 0.5536\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1744 - accuracy: 0.6240 - val_loss: 1.4067 - val_accuracy: 0.5893\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1613 - accuracy: 0.6279 - val_loss: 1.4038 - val_accuracy: 0.5714\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1513 - accuracy: 0.6260 - val_loss: 1.4178 - val_accuracy: 0.5893\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1404 - accuracy: 0.6240 - val_loss: 1.3932 - val_accuracy: 0.5714\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1339 - accuracy: 0.6298 - val_loss: 1.3982 - val_accuracy: 0.5893\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1250 - accuracy: 0.6298 - val_loss: 1.3805 - val_accuracy: 0.5893\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1152 - accuracy: 0.6355 - val_loss: 1.3938 - val_accuracy: 0.5536\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1076 - accuracy: 0.6365 - val_loss: 1.3559 - val_accuracy: 0.6071\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0993 - accuracy: 0.6384 - val_loss: 1.3868 - val_accuracy: 0.5893\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0932 - accuracy: 0.6403 - val_loss: 1.3800 - val_accuracy: 0.5714\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0892 - accuracy: 0.6412 - val_loss: 1.3654 - val_accuracy: 0.5714\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0796 - accuracy: 0.6355 - val_loss: 1.3646 - val_accuracy: 0.5714\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0683 - accuracy: 0.6489 - val_loss: 1.3656 - val_accuracy: 0.5536\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0649 - accuracy: 0.6422 - val_loss: 1.3608 - val_accuracy: 0.5714\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0595 - accuracy: 0.6498 - val_loss: 1.3635 - val_accuracy: 0.5714\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0500 - accuracy: 0.6517 - val_loss: 1.3498 - val_accuracy: 0.5893\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0468 - accuracy: 0.6498 - val_loss: 1.3420 - val_accuracy: 0.5893\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0393 - accuracy: 0.6489 - val_loss: 1.3536 - val_accuracy: 0.5893\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0344 - accuracy: 0.6527 - val_loss: 1.3574 - val_accuracy: 0.5893\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0292 - accuracy: 0.6594 - val_loss: 1.3359 - val_accuracy: 0.5893\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0256 - accuracy: 0.6632 - val_loss: 1.3494 - val_accuracy: 0.5893\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0212 - accuracy: 0.6622 - val_loss: 1.3515 - val_accuracy: 0.5893\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0138 - accuracy: 0.6584 - val_loss: 1.3368 - val_accuracy: 0.5893\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0085 - accuracy: 0.6651 - val_loss: 1.3399 - val_accuracy: 0.5714\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0019 - accuracy: 0.6679 - val_loss: 1.3420 - val_accuracy: 0.5893\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9948 - accuracy: 0.6708 - val_loss: 1.3241 - val_accuracy: 0.6250\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9958 - accuracy: 0.6756 - val_loss: 1.3272 - val_accuracy: 0.6250\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9886 - accuracy: 0.6746 - val_loss: 1.3245 - val_accuracy: 0.6071\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9841 - accuracy: 0.6784 - val_loss: 1.3207 - val_accuracy: 0.6071\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9795 - accuracy: 0.6718 - val_loss: 1.3240 - val_accuracy: 0.6071\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9749 - accuracy: 0.6756 - val_loss: 1.3363 - val_accuracy: 0.6071\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9722 - accuracy: 0.6794 - val_loss: 1.3243 - val_accuracy: 0.6071\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9683 - accuracy: 0.6784 - val_loss: 1.3164 - val_accuracy: 0.6071\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9640 - accuracy: 0.6813 - val_loss: 1.3298 - val_accuracy: 0.6071\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9573 - accuracy: 0.6784 - val_loss: 1.3379 - val_accuracy: 0.6071\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9567 - accuracy: 0.6851 - val_loss: 1.3190 - val_accuracy: 0.6071\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9523 - accuracy: 0.6870 - val_loss: 1.3216 - val_accuracy: 0.6071\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9495 - accuracy: 0.6823 - val_loss: 1.3133 - val_accuracy: 0.6250\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9460 - accuracy: 0.6832 - val_loss: 1.3158 - val_accuracy: 0.5893\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9363 - accuracy: 0.6823 - val_loss: 1.3083 - val_accuracy: 0.6250\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9347 - accuracy: 0.6880 - val_loss: 1.2987 - val_accuracy: 0.6071\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9338 - accuracy: 0.6899 - val_loss: 1.3391 - val_accuracy: 0.6071\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9268 - accuracy: 0.6889 - val_loss: 1.2840 - val_accuracy: 0.6250\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9227 - accuracy: 0.6880 - val_loss: 1.3049 - val_accuracy: 0.6250\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9207 - accuracy: 0.6908 - val_loss: 1.3089 - val_accuracy: 0.6071\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9153 - accuracy: 0.6927 - val_loss: 1.3019 - val_accuracy: 0.6250\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9134 - accuracy: 0.6899 - val_loss: 1.3016 - val_accuracy: 0.6429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9069 - accuracy: 0.6975 - val_loss: 1.2950 - val_accuracy: 0.6250\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9091 - accuracy: 0.6861 - val_loss: 1.3036 - val_accuracy: 0.6250\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9020 - accuracy: 0.6927 - val_loss: 1.2877 - val_accuracy: 0.6250\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8994 - accuracy: 0.6937 - val_loss: 1.3069 - val_accuracy: 0.6250\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9006 - accuracy: 0.6899 - val_loss: 1.3026 - val_accuracy: 0.6250\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8912 - accuracy: 0.6956 - val_loss: 1.2796 - val_accuracy: 0.6071\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8881 - accuracy: 0.6994 - val_loss: 1.2934 - val_accuracy: 0.6071\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8853 - accuracy: 0.7032 - val_loss: 1.2761 - val_accuracy: 0.6429\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8821 - accuracy: 0.6966 - val_loss: 1.2765 - val_accuracy: 0.6250\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8809 - accuracy: 0.7013 - val_loss: 1.3080 - val_accuracy: 0.6071\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8778 - accuracy: 0.7004 - val_loss: 1.2707 - val_accuracy: 0.6429\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8736 - accuracy: 0.7023 - val_loss: 1.3009 - val_accuracy: 0.6071\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8707 - accuracy: 0.7004 - val_loss: 1.2677 - val_accuracy: 0.6429\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8676 - accuracy: 0.7004 - val_loss: 1.3000 - val_accuracy: 0.5893\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8654 - accuracy: 0.7118 - val_loss: 1.2630 - val_accuracy: 0.6250\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8604 - accuracy: 0.7032 - val_loss: 1.2924 - val_accuracy: 0.6071\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8603 - accuracy: 0.7013 - val_loss: 1.2791 - val_accuracy: 0.6429\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8530 - accuracy: 0.7080 - val_loss: 1.2768 - val_accuracy: 0.6071\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8572 - accuracy: 0.7080 - val_loss: 1.2883 - val_accuracy: 0.6250\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8490 - accuracy: 0.7071 - val_loss: 1.2874 - val_accuracy: 0.6250\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8497 - accuracy: 0.7071 - val_loss: 1.2743 - val_accuracy: 0.6250\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8455 - accuracy: 0.7071 - val_loss: 1.2800 - val_accuracy: 0.6250\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8434 - accuracy: 0.7080 - val_loss: 1.2747 - val_accuracy: 0.6429\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8405 - accuracy: 0.7071 - val_loss: 1.2832 - val_accuracy: 0.6071\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.1188 - accuracy: 0.7029\n",
      "\n",
      "Test accuracy: 0.7028985619544983\n",
      "Best Accuracy: 0.7137681245803833\n",
      "Trimmed x_d with Best Selected Features:\n",
      "(1380, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_pca_for_feature_selection(X, desired_variance_ratio):\n",
    "    pca = PCA(n_components=desired_variance_ratio)\n",
    "    reduced_features = pca.fit_transform(X)\n",
    "    return reduced_features, pca\n",
    "\n",
    "def pca(X, y, test_size=0.2, random_state=42):\n",
    "\n",
    "    variance_ratios = [i / 1000 for i in range(990, 1000)]\n",
    "    max_accuracy = 0.0\n",
    "    best_selected_features = None\n",
    "    best_pca_model = None\n",
    "\n",
    "    for desired_variance_ratio in variance_ratios:\n",
    "        reduced_features, pca_model = apply_pca_for_feature_selection(X, desired_variance_ratio)\n",
    "        print(f\"var: {desired_variance_ratio}\")\n",
    "        print(np.shape(reduced_features))\n",
    "\n",
    "        # Split the dataset into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Build the ANN model\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            keras.layers.Dense(32, activation='relu'),\n",
    "            keras.layers.Dense(14, activation='softmax')  # 14 output classes\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.05)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
    "        print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "        # Update max accuracy and store corresponding features\n",
    "        if test_acc > max_accuracy:\n",
    "            max_accuracy = test_acc\n",
    "            best_selected_features = reduced_features.copy()\n",
    "            best_pca_model = pca_model\n",
    "\n",
    "    # Apply inverse_transform to the full dataset with the best selected features\n",
    "    trimmed_x_d = best_pca_model.inverse_transform(best_selected_features)\n",
    "\n",
    "    return best_selected_features, max_accuracy\n",
    "\n",
    "# Example usage:\n",
    "reduced_x_d, max_accuracy = pca(x_d, y_d)\n",
    "print(f\"Best Accuracy: {max_accuracy}\")\n",
    "print(\"Trimmed x_d with Best Selected Features:\")\n",
    "print(np.shape(reduced_x_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bcf09",
   "metadata": {},
   "source": [
    "## ANN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca19135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "activation_0 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.01, 0.1, 0.3, 0.5], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "X = reduced_x_d\n",
    "y = y_d\n",
    "\n",
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Flatten())\n",
    "\n",
    "        # Tune the number of layers.\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "            model.add(\n",
    "                layers.Dense(\n",
    "                    # Tune number of units separately.\n",
    "                    units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                    activation=hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\", \"sigmoid\"]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        model.add(layers.Dense(14, activation=\"softmax\"))\n",
    "\n",
    "        learning_rate = hp.Choice(\"learning_rate\", [0.001, 0.01, 0.1, 0.3, 0.5])\n",
    "#         alpha = hp.Choice(\"alpha\", [0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate), #, momentum=alpha\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32, 64, 128]),\n",
    "            **kwargs,\n",
    "        )\n",
    "    \n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Create an instance of HyperParameters\n",
    "hp = kt.HyperParameters()\n",
    "\n",
    "# Build the model with default hyperparameters\n",
    "my_hypermodel = MyHyperModel()\n",
    "my_model = my_hypermodel.build(hp)\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel=my_hypermodel,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=50,\n",
    "    factor=2,\n",
    "    directory=\"Parameter Tuning\",\n",
    "    project_name=\"14 Classes - GLCM\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96f1d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 186 Complete [00h 00m 07s]\n",
      "val_accuracy: 0.4464285671710968\n",
      "\n",
      "Best val_accuracy So Far: 0.7142857313156128\n",
      "Total elapsed time: 00h 11m 37s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.05, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02db1ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in Parameter Tuning\\14 Classes - GLCM\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0075 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 416\n",
      "activation_0: tanh\n",
      "learning_rate: 0.01\n",
      "batch_size: 128\n",
      "units_1: 256\n",
      "activation_1: relu\n",
      "units_2: 512\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 25\n",
      "tuner/bracket: 5\n",
      "tuner/round: 5\n",
      "tuner/trial_id: 0072\n",
      "Score: 0.7142857313156128\n",
      "\n",
      "Trial 0122 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 448\n",
      "activation_0: relu\n",
      "learning_rate: 0.01\n",
      "batch_size: 16\n",
      "units_1: 32\n",
      "activation_1: sigmoid\n",
      "units_2: 256\n",
      "activation_2: tanh\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 25\n",
      "tuner/bracket: 4\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 0118\n",
      "Score: 0.6785714030265808\n",
      "\n",
      "Trial 0146 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 352\n",
      "activation_0: tanh\n",
      "learning_rate: 0.01\n",
      "batch_size: 16\n",
      "units_1: 96\n",
      "activation_1: sigmoid\n",
      "units_2: 352\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0140\n",
      "Score: 0.6785714030265808\n",
      "\n",
      "Trial 0070 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 416\n",
      "activation_0: tanh\n",
      "learning_rate: 0.01\n",
      "batch_size: 128\n",
      "units_1: 256\n",
      "activation_1: relu\n",
      "units_2: 512\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 13\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 5\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0057\n",
      "Score: 0.6607142686843872\n",
      "\n",
      "Trial 0072 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 416\n",
      "activation_0: tanh\n",
      "learning_rate: 0.01\n",
      "batch_size: 128\n",
      "units_1: 256\n",
      "activation_1: relu\n",
      "units_2: 512\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 5\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 0070\n",
      "Score: 0.6607142686843872\n",
      "\n",
      "Trial 0073 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 416\n",
      "activation_0: tanh\n",
      "learning_rate: 0.01\n",
      "batch_size: 16\n",
      "units_1: 256\n",
      "activation_1: relu\n",
      "units_2: 352\n",
      "activation_2: relu\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 5\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 0068\n",
      "Score: 0.6607142686843872\n",
      "\n",
      "Trial 0121 summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 64\n",
      "activation_0: relu\n",
      "learning_rate: 0.01\n",
      "batch_size: 16\n",
      "units_1: 160\n",
      "activation_1: sigmoid\n",
      "units_2: 384\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 25\n",
      "tuner/bracket: 4\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 0120\n",
      "Score: 0.6607142686843872\n",
      "\n",
      "Trial 0148 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 352\n",
      "activation_0: tanh\n",
      "learning_rate: 0.01\n",
      "batch_size: 16\n",
      "units_1: 96\n",
      "activation_1: sigmoid\n",
      "units_2: 352\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 25\n",
      "tuner/bracket: 3\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0146\n",
      "Score: 0.6607142686843872\n",
      "\n",
      "Trial 0120 summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 64\n",
      "activation_0: relu\n",
      "learning_rate: 0.01\n",
      "batch_size: 16\n",
      "units_1: 160\n",
      "activation_1: sigmoid\n",
      "units_2: 384\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 4\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0115\n",
      "Score: 0.6428571343421936\n",
      "\n",
      "Trial 0145 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 320\n",
      "activation_0: relu\n",
      "learning_rate: 0.01\n",
      "batch_size: 128\n",
      "units_1: 288\n",
      "activation_1: relu\n",
      "units_2: 128\n",
      "activation_2: relu\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0141\n",
      "Score: 0.6428571343421936\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e38f980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 15ms/step - loss: 2.6031 - accuracy: 0.2700 - val_loss: 1.9467 - val_accuracy: 0.3036\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.5761 - accuracy: 0.4637 - val_loss: 1.7274 - val_accuracy: 0.4107\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.3890 - accuracy: 0.5420 - val_loss: 1.4835 - val_accuracy: 0.5357\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.2725 - accuracy: 0.5677 - val_loss: 1.4628 - val_accuracy: 0.5179\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1824 - accuracy: 0.6145 - val_loss: 1.2957 - val_accuracy: 0.5714\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1630 - accuracy: 0.6126 - val_loss: 1.4071 - val_accuracy: 0.5536\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1141 - accuracy: 0.6193 - val_loss: 1.4229 - val_accuracy: 0.5179\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0636 - accuracy: 0.6384 - val_loss: 1.4343 - val_accuracy: 0.5714\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0145 - accuracy: 0.6574 - val_loss: 1.2803 - val_accuracy: 0.5893\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0058 - accuracy: 0.6613 - val_loss: 1.2839 - val_accuracy: 0.5536\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0178 - accuracy: 0.6422 - val_loss: 1.3627 - val_accuracy: 0.5893\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9947 - accuracy: 0.6584 - val_loss: 1.2791 - val_accuracy: 0.6071\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9210 - accuracy: 0.6794 - val_loss: 1.3268 - val_accuracy: 0.6250\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8989 - accuracy: 0.6899 - val_loss: 1.3125 - val_accuracy: 0.5893\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9069 - accuracy: 0.6851 - val_loss: 1.2695 - val_accuracy: 0.6250\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8742 - accuracy: 0.6880 - val_loss: 1.4450 - val_accuracy: 0.6250\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.8537 - accuracy: 0.7061 - val_loss: 1.4594 - val_accuracy: 0.5536\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8216 - accuracy: 0.7118 - val_loss: 1.1962 - val_accuracy: 0.6071\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7949 - accuracy: 0.7281 - val_loss: 1.1275 - val_accuracy: 0.6786\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8244 - accuracy: 0.7004 - val_loss: 1.3127 - val_accuracy: 0.5714\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8273 - accuracy: 0.7099 - val_loss: 1.0998 - val_accuracy: 0.6786\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8156 - accuracy: 0.7195 - val_loss: 1.1997 - val_accuracy: 0.6071\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8342 - accuracy: 0.6966 - val_loss: 1.3849 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8467 - accuracy: 0.7080 - val_loss: 1.2605 - val_accuracy: 0.6607\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8190 - accuracy: 0.7185 - val_loss: 1.1025 - val_accuracy: 0.7143\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8245 - accuracy: 0.7080 - val_loss: 1.2333 - val_accuracy: 0.6429\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8545 - accuracy: 0.6985 - val_loss: 1.3991 - val_accuracy: 0.5536\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8511 - accuracy: 0.6985 - val_loss: 1.2731 - val_accuracy: 0.6964\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8296 - accuracy: 0.7176 - val_loss: 1.2771 - val_accuracy: 0.6607\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8233 - accuracy: 0.7242 - val_loss: 1.2398 - val_accuracy: 0.6607\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9011 - accuracy: 0.7042 - val_loss: 1.4273 - val_accuracy: 0.5893\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8383 - accuracy: 0.7099 - val_loss: 1.1804 - val_accuracy: 0.6429\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9077 - accuracy: 0.6765 - val_loss: 1.1283 - val_accuracy: 0.6071\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9614 - accuracy: 0.6870 - val_loss: 1.5165 - val_accuracy: 0.5893\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8593 - accuracy: 0.6918 - val_loss: 1.4422 - val_accuracy: 0.5179\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8694 - accuracy: 0.6947 - val_loss: 1.4510 - val_accuracy: 0.5714\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8787 - accuracy: 0.6927 - val_loss: 1.4962 - val_accuracy: 0.5357\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8271 - accuracy: 0.7071 - val_loss: 1.3343 - val_accuracy: 0.5714\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.7823 - accuracy: 0.7357 - val_loss: 1.3495 - val_accuracy: 0.5357\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8389 - accuracy: 0.7147 - val_loss: 1.5029 - val_accuracy: 0.5536\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8206 - accuracy: 0.7023 - val_loss: 1.4441 - val_accuracy: 0.6071\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9641 - accuracy: 0.6660 - val_loss: 1.3589 - val_accuracy: 0.5714\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9356 - accuracy: 0.6632 - val_loss: 1.7900 - val_accuracy: 0.5714\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8935 - accuracy: 0.7032 - val_loss: 1.4837 - val_accuracy: 0.6429\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9038 - accuracy: 0.6794 - val_loss: 1.4917 - val_accuracy: 0.5357\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8846 - accuracy: 0.6975 - val_loss: 1.2935 - val_accuracy: 0.6429\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.9026 - accuracy: 0.6765 - val_loss: 1.7430 - val_accuracy: 0.5893\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9662 - accuracy: 0.6784 - val_loss: 1.5964 - val_accuracy: 0.5714\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9341 - accuracy: 0.6813 - val_loss: 1.4034 - val_accuracy: 0.5357\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.9528 - accuracy: 0.6803 - val_loss: 1.4793 - val_accuracy: 0.5714\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9578 - accuracy: 0.6689 - val_loss: 1.5689 - val_accuracy: 0.5714\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9802 - accuracy: 0.6861 - val_loss: 1.6805 - val_accuracy: 0.5536\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9830 - accuracy: 0.6727 - val_loss: 1.6187 - val_accuracy: 0.5714\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0253 - accuracy: 0.6555 - val_loss: 1.4801 - val_accuracy: 0.5893\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0174 - accuracy: 0.6765 - val_loss: 1.4609 - val_accuracy: 0.5714\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9674 - accuracy: 0.6937 - val_loss: 1.3106 - val_accuracy: 0.6071\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8873 - accuracy: 0.6956 - val_loss: 1.3376 - val_accuracy: 0.5714\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8813 - accuracy: 0.6889 - val_loss: 1.5834 - val_accuracy: 0.5357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9694 - accuracy: 0.6708 - val_loss: 1.3915 - val_accuracy: 0.5893\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9321 - accuracy: 0.6842 - val_loss: 1.5819 - val_accuracy: 0.5714\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9009 - accuracy: 0.6937 - val_loss: 1.3299 - val_accuracy: 0.5893\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.8724 - accuracy: 0.6975 - val_loss: 1.2569 - val_accuracy: 0.6964\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0146 - accuracy: 0.6393 - val_loss: 1.6728 - val_accuracy: 0.5536\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9518 - accuracy: 0.6889 - val_loss: 1.3662 - val_accuracy: 0.6607\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9807 - accuracy: 0.6899 - val_loss: 1.6642 - val_accuracy: 0.6071\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9364 - accuracy: 0.6927 - val_loss: 1.7429 - val_accuracy: 0.5893\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9248 - accuracy: 0.6956 - val_loss: 1.7747 - val_accuracy: 0.5536\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9510 - accuracy: 0.6823 - val_loss: 1.6522 - val_accuracy: 0.5179\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8969 - accuracy: 0.7042 - val_loss: 1.7767 - val_accuracy: 0.4464\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0267 - accuracy: 0.6641 - val_loss: 1.7707 - val_accuracy: 0.5714\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0508 - accuracy: 0.6508 - val_loss: 1.7361 - val_accuracy: 0.5714\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9350 - accuracy: 0.6803 - val_loss: 1.5473 - val_accuracy: 0.5536\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9194 - accuracy: 0.6918 - val_loss: 1.4001 - val_accuracy: 0.6429\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9446 - accuracy: 0.6994 - val_loss: 1.6571 - val_accuracy: 0.5179\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9827 - accuracy: 0.6641 - val_loss: 1.6409 - val_accuracy: 0.5357\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9558 - accuracy: 0.6918 - val_loss: 1.6742 - val_accuracy: 0.5536\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9351 - accuracy: 0.6918 - val_loss: 1.4050 - val_accuracy: 0.5893\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9948 - accuracy: 0.6660 - val_loss: 1.6093 - val_accuracy: 0.5357\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9766 - accuracy: 0.6546 - val_loss: 1.5216 - val_accuracy: 0.5357\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0289 - accuracy: 0.6632 - val_loss: 1.6654 - val_accuracy: 0.4286\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1404 - accuracy: 0.6307 - val_loss: 1.8256 - val_accuracy: 0.5000\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0854 - accuracy: 0.6336 - val_loss: 1.5477 - val_accuracy: 0.6071\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0528 - accuracy: 0.6384 - val_loss: 1.5984 - val_accuracy: 0.5179\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0703 - accuracy: 0.6345 - val_loss: 1.4707 - val_accuracy: 0.5893\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0138 - accuracy: 0.6498 - val_loss: 1.5874 - val_accuracy: 0.5357\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0095 - accuracy: 0.6756 - val_loss: 1.6707 - val_accuracy: 0.5357\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1176 - accuracy: 0.6412 - val_loss: 1.7983 - val_accuracy: 0.4821\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0951 - accuracy: 0.6393 - val_loss: 1.6846 - val_accuracy: 0.4464\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.2545 - accuracy: 0.5830 - val_loss: 1.8400 - val_accuracy: 0.4643\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1157 - accuracy: 0.6393 - val_loss: 1.6634 - val_accuracy: 0.4286\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1612 - accuracy: 0.6221 - val_loss: 1.4652 - val_accuracy: 0.5357\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0867 - accuracy: 0.6250 - val_loss: 1.5967 - val_accuracy: 0.5893\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1024 - accuracy: 0.6279 - val_loss: 1.5231 - val_accuracy: 0.5179\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0845 - accuracy: 0.6403 - val_loss: 1.5294 - val_accuracy: 0.5179\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1164 - accuracy: 0.6298 - val_loss: 1.7715 - val_accuracy: 0.5000\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1583 - accuracy: 0.6202 - val_loss: 1.7018 - val_accuracy: 0.5357\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1338 - accuracy: 0.6164 - val_loss: 1.6409 - val_accuracy: 0.4821\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.1852 - accuracy: 0.6116 - val_loss: 1.7113 - val_accuracy: 0.5357\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0957 - accuracy: 0.6279 - val_loss: 1.4506 - val_accuracy: 0.5179\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.0176 - accuracy: 0.6536 - val_loss: 1.4519 - val_accuracy: 0.4643\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.4700 - accuracy: 0.6087\n",
      "[test loss, test accuracy]: [1.4699928760528564, 0.6086956262588501]\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.05)\n",
    "\n",
    "# Check Result\n",
    "y_pred_a = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_a, axis=1)\n",
    "\n",
    "eval_result = model.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ded1ce49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72        19\n",
      "           1       0.44      0.86      0.59        14\n",
      "           2       0.90      0.82      0.86        22\n",
      "           3       0.43      0.25      0.32        24\n",
      "           4       0.86      0.90      0.88        20\n",
      "           5       0.57      0.38      0.46        21\n",
      "           6       1.00      0.91      0.95        22\n",
      "           7       0.59      0.81      0.68        21\n",
      "           8       0.24      0.38      0.29        21\n",
      "           9       0.45      0.43      0.44        21\n",
      "          10       0.71      0.50      0.59        20\n",
      "          11       0.39      0.60      0.47        15\n",
      "          12       0.75      0.20      0.32        15\n",
      "          13       0.89      0.81      0.85        21\n",
      "\n",
      "    accuracy                           0.61       276\n",
      "   macro avg       0.64      0.61      0.60       276\n",
      "weighted avg       0.65      0.61      0.61       276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class_list = set(y_test)\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "# print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=class_list, yticklabels=class_list)\n",
    "\n",
    "for i in range(len(conf_matrix)):\n",
    "    for j in range(len(conf_matrix[0])):\n",
    "        if i == j:\n",
    "            plt.text(j + 0.5, i + 0.5, str(conf_matrix[i, j]), ha='center', va='center', color='white')\n",
    "        else:\n",
    "            plt.text(j + 0.5, i + 0.5, str(conf_matrix[i, j]), ha='center', va='center', color='black')\n",
    "            \n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d440e015",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt('test_predictions.csv', np.hstack((y_test.reshape(-1, 1), y_pred.reshape(-1,1))), delimiter=',', fmt='%d')\n",
    "np.savetxt('actual labels.csv', y_test.reshape(-1,1), delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ada0d7e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Mapping:\n",
      "{'bacterial_leaf_blight': 0, 'bacterial_leaf_streak': 1, 'bakanae': 2, 'brown_spot': 3, 'grassy_stunt_virus': 4, 'healthy_rice_plant': 5, 'narrow_brown_spot': 6, 'ragged_stunt_virus': 7, 'rice_blast': 8, 'rice_false_smut': 9, 'sheath_blight': 10, 'sheath_rot': 11, 'stem_rot': 12, 'tungro_virus': 13}\n"
     ]
    }
   ],
   "source": [
    "# Display the mapping between class names and encoded numbers\n",
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"\\nClass Mapping:\")\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3359008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
