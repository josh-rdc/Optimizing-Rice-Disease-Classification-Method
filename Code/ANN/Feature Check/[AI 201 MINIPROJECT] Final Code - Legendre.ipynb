{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64653a26-b164-4036-93b5-be0f7358a8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auxiliaries (Importing Datafile, Checking Time)\n",
    "import os \n",
    "import time\n",
    "\n",
    "# Calculations\n",
    "import numpy as np\n",
    "\n",
    "# For Importing Data Files\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# For Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.filters import threshold_multiotsu\n",
    "from skimage import color\n",
    "\n",
    "# For Image Pre-Processing\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979b03d-2172-4465-999b-1e65d282be22",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create dataframe\n",
    "\n",
    "3 columns\n",
    "1. Disease Classification\n",
    "2. Image Name\n",
    "3. Image RGB Values (RGB Converted from BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce4c625-de79-4861-b12a-56db5eb4203e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def natural_sort_key(s):\n",
    "    \"\"\"Key function for natural sorting.\"\"\"\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "def create_dataframe_from_folders(root_folder):\n",
    "    # Initialize empty lists to store data for each column\n",
    "    folder_names = []\n",
    "    photo_names = []\n",
    "    photos = []\n",
    "\n",
    "    # Traverse through the root folder and its subfolders\n",
    "    for folder_name in sorted(os.listdir(root_folder)):\n",
    "        folder_path = os.path.join(root_folder, folder_name)\n",
    "\n",
    "        # Check if the entry in the root folder is a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            for photo_name in sorted(os.listdir(folder_path), key=natural_sort_key):\n",
    "                # Photos are in common image formats (e.g., jpg, png)\n",
    "                if photo_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    photo_path = os.path.join(folder_path, photo_name)\n",
    "\n",
    "                    # Read image data using cv2\n",
    "                    image_data = cv2.imread(photo_path)\n",
    "\n",
    "                    # Convert BGR to RGB\n",
    "                    image_data_rgb = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # Append data to the lists\n",
    "                    folder_names.append(folder_name)\n",
    "                    photo_names.append(photo_name)\n",
    "                    photos.append(image_data_rgb)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'class': folder_names,\n",
    "        'img_name': photo_names,\n",
    "        'rgb': photos\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af7bde-ac77-45fa-8c61-7a53363109b4",
   "metadata": {},
   "source": [
    "Get data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f698915-bb74-4ec7-8ff1-606c6471d686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   class                img_name  \\\n",
      "0  bacterial_leaf_blight  BLB_multi_leaf (1).jpg   \n",
      "1  bacterial_leaf_blight  BLB_multi_leaf (2).jpg   \n",
      "2  bacterial_leaf_blight  BLB_multi_leaf (3).jpg   \n",
      "3  bacterial_leaf_blight  BLB_multi_leaf (4).jpg   \n",
      "4  bacterial_leaf_blight  BLB_multi_leaf (5).jpg   \n",
      "\n",
      "                                                 rgb  \n",
      "0  [[[115, 152, 47], [78, 113, 9], [116, 147, 43]...  \n",
      "1  [[[198, 218, 97], [190, 210, 89], [189, 208, 9...  \n",
      "2  [[[137, 176, 51], [166, 203, 64], [165, 200, 3...  \n",
      "3  [[[192, 212, 143], [201, 222, 157], [187, 206,...  \n",
      "4  [[[132, 176, 27], [135, 180, 27], [161, 206, 4...  \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "extra_resized_folder_path = r\"C:\\Users\\Josh\\000 Files\\003 Mengg AI\\01a 201 (AI)\\03 Mini-Project\\resized_raw_images (14 Classes)\"\n",
    "extra_resized = create_dataframe_from_folders(extra_resized_folder_path)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(extra_resized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5426b9d-5fe5-470e-95cd-b9bd72e1875a",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Class Counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be65000-1849-48e3-b646-93efd2758afd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "bakanae                  100\n",
      "brown_spot               100\n",
      "grassy_stunt_virus       100\n",
      "healthy_rice_plant       100\n",
      "ragged_stunt_virus       100\n",
      "stem_rot                 100\n",
      "tungro_virus             100\n",
      "bacterial_leaf_streak     99\n",
      "rice_false_smut           99\n",
      "narrow_brown_spot         98\n",
      "rice_blast                98\n",
      "sheath_blight             98\n",
      "bacterial_leaf_blight     97\n",
      "sheath_rot                91\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_counts = extra_resized['class'].value_counts()\n",
    "\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac8225-3fe7-4259-a715-381055cfb61f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d99ab4-0a06-495a-8af2-2d2ca69d19a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Texture Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef73441b-106f-4a76-af4a-30d0192a58c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mahotas\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "def compute_glcm_features(df):\n",
    "    df['gray'] = df['rgb'].apply(lambda x: cv2.cvtColor(np.uint8(x), cv2.COLOR_RGB2GRAY))\n",
    "    def calculate_glcm(gray_image):\n",
    "        # Compute GLCM using mahotas\n",
    "        glcm = mahotas.features.haralick(gray_image.astype(np.uint8), ignore_zeros=True)\n",
    "        return glcm.mean(axis=0)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        gray_image = row['gray']\n",
    "        glcm_features = calculate_glcm(gray_image)\n",
    "        features.append(glcm_features)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Define meaningful column names for GLCM features\n",
    "    glcm_columns = [\n",
    "        'Angular Second Moment',\n",
    "        'Contrast',\n",
    "        'Correlation',\n",
    "        'Sum of Squares: Variance',\n",
    "        'Inverse Difference Moment',\n",
    "        'Sum Average',\n",
    "        'Sum Variance',\n",
    "        'Sum Entropy',\n",
    "        'Entropy',\n",
    "        'Difference Variance',\n",
    "        'Difference Entropy',\n",
    "        'Informational Measure of Correlation 1',\n",
    "        'Informational Measure of Correlation 2'\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame with the new GLCM features and meaningful column names\n",
    "    glcm_df = pd.DataFrame(features, columns=[f'GLCM_{col}' for col in glcm_columns])\n",
    "\n",
    "    # Concatenate the new DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([df, glcm_df], axis=1)\n",
    "\n",
    "    print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2b059a-fa17-4742-a4e9-61286361606a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 47.39 seconds\n"
     ]
    }
   ],
   "source": [
    "glcm = compute_glcm_features(extra_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564a00a9-a6ed-4d71-b79b-82ded4744d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove the first n columns\n",
    "f_glcm = glcm.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3555e8-efe3-4a70-8258-9bf9d11a2d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLCM_Angular Second Moment</th>\n",
       "      <th>GLCM_Contrast</th>\n",
       "      <th>GLCM_Correlation</th>\n",
       "      <th>GLCM_Sum of Squares: Variance</th>\n",
       "      <th>GLCM_Inverse Difference Moment</th>\n",
       "      <th>GLCM_Sum Average</th>\n",
       "      <th>GLCM_Sum Variance</th>\n",
       "      <th>GLCM_Sum Entropy</th>\n",
       "      <th>GLCM_Entropy</th>\n",
       "      <th>GLCM_Difference Variance</th>\n",
       "      <th>GLCM_Difference Entropy</th>\n",
       "      <th>GLCM_Informational Measure of Correlation 1</th>\n",
       "      <th>GLCM_Informational Measure of Correlation 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000093</td>\n",
       "      <td>1876.058313</td>\n",
       "      <td>0.693690</td>\n",
       "      <td>3062.090566</td>\n",
       "      <td>0.086892</td>\n",
       "      <td>281.006018</td>\n",
       "      <td>10372.303952</td>\n",
       "      <td>8.629042</td>\n",
       "      <td>14.243405</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>6.206143</td>\n",
       "      <td>-0.154215</td>\n",
       "      <td>0.950487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>1334.884861</td>\n",
       "      <td>0.797998</td>\n",
       "      <td>3303.508641</td>\n",
       "      <td>0.077155</td>\n",
       "      <td>276.277140</td>\n",
       "      <td>11879.149702</td>\n",
       "      <td>8.601060</td>\n",
       "      <td>14.082083</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>6.006272</td>\n",
       "      <td>-0.160813</td>\n",
       "      <td>0.952879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000140</td>\n",
       "      <td>1340.143870</td>\n",
       "      <td>0.744815</td>\n",
       "      <td>2625.859569</td>\n",
       "      <td>0.091972</td>\n",
       "      <td>293.873154</td>\n",
       "      <td>9163.294405</td>\n",
       "      <td>8.446153</td>\n",
       "      <td>13.764957</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>5.900897</td>\n",
       "      <td>-0.165284</td>\n",
       "      <td>0.954139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>1293.754229</td>\n",
       "      <td>0.776222</td>\n",
       "      <td>2890.458305</td>\n",
       "      <td>0.071097</td>\n",
       "      <td>289.332162</td>\n",
       "      <td>10268.078990</td>\n",
       "      <td>8.648174</td>\n",
       "      <td>14.226573</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>6.040155</td>\n",
       "      <td>-0.157107</td>\n",
       "      <td>0.952233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>1076.703851</td>\n",
       "      <td>0.693791</td>\n",
       "      <td>1757.932426</td>\n",
       "      <td>0.082002</td>\n",
       "      <td>284.620945</td>\n",
       "      <td>5955.025853</td>\n",
       "      <td>8.284213</td>\n",
       "      <td>13.795743</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>5.881877</td>\n",
       "      <td>-0.134893</td>\n",
       "      <td>0.923023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GLCM_Angular Second Moment  GLCM_Contrast  GLCM_Correlation  \\\n",
       "0                    0.000093    1876.058313          0.693690   \n",
       "1                    0.000090    1334.884861          0.797998   \n",
       "2                    0.000140    1340.143870          0.744815   \n",
       "3                    0.000071    1293.754229          0.776222   \n",
       "4                    0.000101    1076.703851          0.693791   \n",
       "\n",
       "   GLCM_Sum of Squares: Variance  GLCM_Inverse Difference Moment  \\\n",
       "0                    3062.090566                        0.086892   \n",
       "1                    3303.508641                        0.077155   \n",
       "2                    2625.859569                        0.091972   \n",
       "3                    2890.458305                        0.071097   \n",
       "4                    1757.932426                        0.082002   \n",
       "\n",
       "   GLCM_Sum Average  GLCM_Sum Variance  GLCM_Sum Entropy  GLCM_Entropy  \\\n",
       "0        281.006018       10372.303952          8.629042     14.243405   \n",
       "1        276.277140       11879.149702          8.601060     14.082083   \n",
       "2        293.873154        9163.294405          8.446153     13.764957   \n",
       "3        289.332162       10268.078990          8.648174     14.226573   \n",
       "4        284.620945        5955.025853          8.284213     13.795743   \n",
       "\n",
       "   GLCM_Difference Variance  GLCM_Difference Entropy  \\\n",
       "0                  0.000072                 6.206143   \n",
       "1                  0.000079                 6.006272   \n",
       "2                  0.000095                 5.900897   \n",
       "3                  0.000073                 6.040155   \n",
       "4                  0.000086                 5.881877   \n",
       "\n",
       "   GLCM_Informational Measure of Correlation 1  \\\n",
       "0                                    -0.154215   \n",
       "1                                    -0.160813   \n",
       "2                                    -0.165284   \n",
       "3                                    -0.157107   \n",
       "4                                    -0.134893   \n",
       "\n",
       "   GLCM_Informational Measure of Correlation 2  \n",
       "0                                     0.950487  \n",
       "1                                     0.952879  \n",
       "2                                     0.954139  \n",
       "3                                     0.952233  \n",
       "4                                     0.923023  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_glcm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21ad4b-7eb0-4110-bcb4-6e592e28d837",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Histogram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d94ca35f-9437-4b01-8009-1a0eed2ae870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import color\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "354c8451-629c-447d-beef-076db3311418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def values(dataframe):\n",
    "    # Ensure the 'rgb' column exists in the dataframe\n",
    "    dataframe['hsv'] = dataframe['rgb'].apply(lambda x: cv2.cvtColor(np.uint8(x), cv2.COLOR_RGB2HSV))\n",
    "    #dataframe['hsi'] = dataframe['rgb'].apply(lambda x: color.rgb2hsi(np.uint8(x)))\n",
    "    dataframe['lab'] = dataframe['rgb'].apply(lambda x: cv2.cvtColor(np.uint8(x), cv2.COLOR_RGB2LAB))\n",
    "\n",
    "    dataframe['red'] = dataframe['rgb'].apply(lambda rgb: rgb[:,:,0])\n",
    "    dataframe['green'] = dataframe['rgb'].apply(lambda rgb: rgb[:,:,1])\n",
    "    dataframe['blue'] = dataframe['rgb'].apply(lambda rgb: rgb[:,:,2])\n",
    "    \n",
    "    dataframe['hue_v'] = dataframe['hsv'].apply(lambda hsv: hsv[:,:,0])\n",
    "    dataframe['saturation_v'] = dataframe['hsv'].apply(lambda hsv: hsv[:,:,1])\n",
    "    dataframe['value_v'] = dataframe['hsv'].apply(lambda hsv: hsv[:,:,2])\n",
    "    \n",
    "    #dataframe['hue_i'] = dataframe['hsv'].apply(lambda hsi: hsi[:,:,0])\n",
    "    #dataframe['saturation_i'] = dataframe['hsv'].apply(lambda hsi: hsi[:,:,1])\n",
    "    #dataframe['intensity_i'] = dataframe['hsv'].apply(lambda hsi: hsi[:,:,2])\n",
    "    \n",
    "    dataframe['lightness'] = dataframe['lab'].apply(lambda lab: lab[:, :, 0])\n",
    "    dataframe['a'] = dataframe['lab'].apply(lambda lab: lab[:, :, 1])\n",
    "    dataframe['b'] = dataframe['lab'].apply(lambda lab: lab[:, :, 2])\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d5082c1-48a7-4a77-ae63-d276523f8173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_bins_to_dataframe(dataframe, column_name, num_bins):\n",
    "    # Extract the specified column\n",
    "    original_column = dataframe[column_name]\n",
    "\n",
    "    # Define bin edges based on the min and max values in the matrices\n",
    "    min_value = np.min([np.min(matrix) for matrix in original_column])\n",
    "    max_value = np.max([np.max(matrix) for matrix in original_column])\n",
    "\n",
    "    bin_edges = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "    # Create column names for the bins\n",
    "    bin_column_names = [f'{column_name}_{i}' for i in range(num_bins)]\n",
    "\n",
    "    # Iterate through each matrix in the original column\n",
    "    for i, matrix in enumerate(original_column):\n",
    "        # Digitize each element in the matrix into the corresponding bin\n",
    "        bin_indices = np.digitize(matrix.flatten(), bin_edges, right=True)\n",
    "\n",
    "        # Count the occurrences of each bin index\n",
    "        bin_counts = np.bincount(bin_indices, minlength=num_bins + 1)[1:]\n",
    "\n",
    "        # Add new columns to the DataFrame for each bin\n",
    "        for j, bin_column_name in enumerate(bin_column_names):\n",
    "            dataframe.loc[i, bin_column_name] = bin_counts[j]\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eeb1fba-0704-4f4b-b34a-2e2ef1cbd5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channels = values(extra_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1ab83c9-fc0e-4cff-9869-3fa8c12e4469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_bins_dataframe(dataframe, col, num_bins):\n",
    "    bins_dataframe = pd.DataFrame()\n",
    "\n",
    "    #print(col)\n",
    "    min_value = np.min([np.min(matrix) for matrix in dataframe[col]])\n",
    "    max_value = np.max([np.max(matrix) for matrix in dataframe[col]])\n",
    "\n",
    "    bin_edges = np.linspace(min_value, max_value, num_bins + 1)\n",
    "    bin_column_names = [f'{col}_{i}' for i in range(1, num_bins + 1)]\n",
    "\n",
    "    for matrix in dataframe[col]:\n",
    "        bin_indices = np.digitize(matrix.flatten(), bin_edges, right=True)\n",
    "        bin_counts = np.bincount(bin_indices, minlength=num_bins + 1)[1:]\n",
    "\n",
    "        bins_dataframe = pd.concat([bins_dataframe, pd.DataFrame(bin_counts).transpose()], axis=0, ignore_index=True)\n",
    "\n",
    "    bins_dataframe.columns = bin_column_names\n",
    "        \n",
    "    return bins_dataframe\n",
    "\n",
    "def hist_features(df, cols, bins):\n",
    "    complete_bins = pd.DataFrame()\n",
    "\n",
    "    for col in cols:\n",
    "        col_bins = create_bins_dataframe(df, col, bins)\n",
    "        complete_bins = pd.concat([complete_bins, col_bins], axis=1)\n",
    "\n",
    "    return complete_bins\n",
    "\n",
    "cols = ['red', 'green', 'blue', 'hue_v', 'saturation_v', 'value_v', 'lightness', 'a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ef1f4f2-7b29-4783-80bb-942502c3f276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_histogram = hist_features(channels, cols, 82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "200b7499-7dd8-4545-a930-7e28dd715d64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_1</th>\n",
       "      <th>red_2</th>\n",
       "      <th>red_3</th>\n",
       "      <th>red_4</th>\n",
       "      <th>red_5</th>\n",
       "      <th>red_6</th>\n",
       "      <th>red_7</th>\n",
       "      <th>red_8</th>\n",
       "      <th>red_9</th>\n",
       "      <th>red_10</th>\n",
       "      <th>...</th>\n",
       "      <th>b_73</th>\n",
       "      <th>b_74</th>\n",
       "      <th>b_75</th>\n",
       "      <th>b_76</th>\n",
       "      <th>b_77</th>\n",
       "      <th>b_78</th>\n",
       "      <th>b_79</th>\n",
       "      <th>b_80</th>\n",
       "      <th>b_81</th>\n",
       "      <th>b_82</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "      <td>93</td>\n",
       "      <td>143</td>\n",
       "      <td>223</td>\n",
       "      <td>231</td>\n",
       "      <td>305</td>\n",
       "      <td>314</td>\n",
       "      <td>330</td>\n",
       "      <td>432</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153</td>\n",
       "      <td>210</td>\n",
       "      <td>261</td>\n",
       "      <td>308</td>\n",
       "      <td>349</td>\n",
       "      <td>391</td>\n",
       "      <td>392</td>\n",
       "      <td>358</td>\n",
       "      <td>383</td>\n",
       "      <td>554</td>\n",
       "      <td>...</td>\n",
       "      <td>541</td>\n",
       "      <td>556</td>\n",
       "      <td>187</td>\n",
       "      <td>98</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>62</td>\n",
       "      <td>145</td>\n",
       "      <td>205</td>\n",
       "      <td>290</td>\n",
       "      <td>337</td>\n",
       "      <td>325</td>\n",
       "      <td>338</td>\n",
       "      <td>454</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>53</td>\n",
       "      <td>89</td>\n",
       "      <td>106</td>\n",
       "      <td>194</td>\n",
       "      <td>242</td>\n",
       "      <td>267</td>\n",
       "      <td>436</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>1417</td>\n",
       "      <td>1133</td>\n",
       "      <td>195</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 738 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   red_1  red_2  red_3  red_4  red_5  red_6  red_7  red_8  red_9  red_10  ...  \\\n",
       "0     19     47     93    143    223    231    305    314    330     432  ...   \n",
       "1    153    210    261    308    349    391    392    358    383     554  ...   \n",
       "2     14     35     62    145    205    290    337    325    338     454  ...   \n",
       "3     14     14     40     53     89    106    194    242    267     436  ...   \n",
       "4      0      1      3      4      1      6      6      9      8      31  ...   \n",
       "\n",
       "   b_73  b_74  b_75  b_76  b_77  b_78  b_79  b_80  b_81  b_82  \n",
       "0    73    27     0     0     0     0     0     0     0     0  \n",
       "1   541   556   187    98    47    33     0     0     0     0  \n",
       "2   157    51     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0     0  \n",
       "4  1417  1133   195    42     8     1     0     0     0     0  \n",
       "\n",
       "[5 rows x 738 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_histogram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58cb8f-65ac-44fc-99e6-58e42abfabfa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Color Moments Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8b0823e-3e5b-4f1c-95a6-8b8b4cc97e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import cv2\n",
    "\n",
    "def color_moments(dataframe, cols):\n",
    "    for col in cols:\n",
    "        print(col)\n",
    "        mean_values = []\n",
    "        variance_values = []\n",
    "        kurtosis_values = []\n",
    "        skewness_values = []\n",
    "\n",
    "        for img in dataframe[col]:\n",
    "            # 'img' is a 3D numpy array representing an RGB image\n",
    "            # Calculate statistics for each image\n",
    "            mean_channel = np.mean(img, axis=(0, 1))\n",
    "            variance_channel = np.var(img, axis=(0, 1))\n",
    "            kurtosis_channel = kurtosis(img, axis=(0, 1))\n",
    "            skewness_channel = skew(img, axis=(0, 1))\n",
    "\n",
    "            # Append values to respective lists\n",
    "            mean_values.append(mean_channel)\n",
    "            variance_values.append(variance_channel)\n",
    "            kurtosis_values.append(kurtosis_channel)\n",
    "            skewness_values.append(skewness_channel)\n",
    "\n",
    "        # Add the new columns to the DataFrame with channel names\n",
    "        for i in range(img.shape[2]):  # 'img' has shape (height, width, channels)\n",
    "            dataframe[f'{col}_channel_{i}_mean'] = [x[i] for x in mean_values]\n",
    "            dataframe[f'{col}_channel_{i}_variance'] = [x[i] for x in variance_values]\n",
    "            dataframe[f'{col}_channel_{i}_kurtosis'] = [x[i] for x in kurtosis_values]\n",
    "            dataframe[f'{col}_channel_{i}_skewness'] = [x[i] for x in skewness_values]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2cc9e6e-ddcb-41fe-aa4b-72e3f44afc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb\n",
      "hsv\n",
      "lab\n"
     ]
    }
   ],
   "source": [
    "color_df = color_moments(extra_resized, ['rgb','hsv','lab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c245d80-93b2-4cc8-82a3-77f40ab9b53b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_color = color_df.drop(columns=['class','img_name','rgb','gray','hsv','lab','red','green','blue', 'hue_v', 'saturation_v', 'value_v', 'lightness', 'a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b52a436e-88b8-43bc-b1d7-b17b428f0ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rgb_channel_0_mean</th>\n",
       "      <th>rgb_channel_0_variance</th>\n",
       "      <th>rgb_channel_0_kurtosis</th>\n",
       "      <th>rgb_channel_0_skewness</th>\n",
       "      <th>rgb_channel_1_mean</th>\n",
       "      <th>rgb_channel_1_variance</th>\n",
       "      <th>rgb_channel_1_kurtosis</th>\n",
       "      <th>rgb_channel_1_skewness</th>\n",
       "      <th>rgb_channel_2_mean</th>\n",
       "      <th>rgb_channel_2_variance</th>\n",
       "      <th>...</th>\n",
       "      <th>lab_channel_0_kurtosis</th>\n",
       "      <th>lab_channel_0_skewness</th>\n",
       "      <th>lab_channel_1_mean</th>\n",
       "      <th>lab_channel_1_variance</th>\n",
       "      <th>lab_channel_1_kurtosis</th>\n",
       "      <th>lab_channel_1_skewness</th>\n",
       "      <th>lab_channel_2_mean</th>\n",
       "      <th>lab_channel_2_variance</th>\n",
       "      <th>lab_channel_2_kurtosis</th>\n",
       "      <th>lab_channel_2_skewness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.110013</td>\n",
       "      <td>3078.218207</td>\n",
       "      <td>-0.689453</td>\n",
       "      <td>-0.192670</td>\n",
       "      <td>157.315011</td>\n",
       "      <td>3449.301597</td>\n",
       "      <td>-0.484632</td>\n",
       "      <td>-0.518085</td>\n",
       "      <td>67.831274</td>\n",
       "      <td>2087.782836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.296129</td>\n",
       "      <td>-0.606758</td>\n",
       "      <td>106.950853</td>\n",
       "      <td>61.335754</td>\n",
       "      <td>0.410354</td>\n",
       "      <td>0.917856</td>\n",
       "      <td>169.947226</td>\n",
       "      <td>203.966363</td>\n",
       "      <td>0.049806</td>\n",
       "      <td>-0.576146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132.208227</td>\n",
       "      <td>3961.755789</td>\n",
       "      <td>-1.130258</td>\n",
       "      <td>-0.302681</td>\n",
       "      <td>158.333586</td>\n",
       "      <td>3558.081163</td>\n",
       "      <td>-0.908913</td>\n",
       "      <td>-0.458059</td>\n",
       "      <td>50.368921</td>\n",
       "      <td>1453.704717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.781886</td>\n",
       "      <td>-0.534726</td>\n",
       "      <td>103.516083</td>\n",
       "      <td>21.999662</td>\n",
       "      <td>2.755215</td>\n",
       "      <td>1.327478</td>\n",
       "      <td>177.711097</td>\n",
       "      <td>198.029258</td>\n",
       "      <td>0.350495</td>\n",
       "      <td>-0.735196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143.534598</td>\n",
       "      <td>2684.064771</td>\n",
       "      <td>-0.122034</td>\n",
       "      <td>-0.726785</td>\n",
       "      <td>163.988122</td>\n",
       "      <td>3032.684210</td>\n",
       "      <td>0.098623</td>\n",
       "      <td>-0.932985</td>\n",
       "      <td>67.333745</td>\n",
       "      <td>1393.034182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395657</td>\n",
       "      <td>-1.041771</td>\n",
       "      <td>106.869300</td>\n",
       "      <td>39.706570</td>\n",
       "      <td>1.180393</td>\n",
       "      <td>1.184484</td>\n",
       "      <td>173.509228</td>\n",
       "      <td>201.620730</td>\n",
       "      <td>0.534828</td>\n",
       "      <td>-0.842716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134.806521</td>\n",
       "      <td>3150.853829</td>\n",
       "      <td>-0.752603</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>155.817223</td>\n",
       "      <td>2935.205771</td>\n",
       "      <td>-0.687641</td>\n",
       "      <td>-0.282174</td>\n",
       "      <td>112.783462</td>\n",
       "      <td>2356.163989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.573437</td>\n",
       "      <td>-0.355013</td>\n",
       "      <td>112.668666</td>\n",
       "      <td>26.981995</td>\n",
       "      <td>2.443752</td>\n",
       "      <td>0.798156</td>\n",
       "      <td>148.112783</td>\n",
       "      <td>56.290194</td>\n",
       "      <td>1.918221</td>\n",
       "      <td>-0.305414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133.956593</td>\n",
       "      <td>1928.268962</td>\n",
       "      <td>-0.601913</td>\n",
       "      <td>0.189324</td>\n",
       "      <td>165.174087</td>\n",
       "      <td>1995.183999</td>\n",
       "      <td>-0.653920</td>\n",
       "      <td>-0.250057</td>\n",
       "      <td>46.036372</td>\n",
       "      <td>1525.296728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.566688</td>\n",
       "      <td>-0.252408</td>\n",
       "      <td>101.051419</td>\n",
       "      <td>77.759832</td>\n",
       "      <td>0.647577</td>\n",
       "      <td>1.165816</td>\n",
       "      <td>180.711256</td>\n",
       "      <td>190.856598</td>\n",
       "      <td>0.315457</td>\n",
       "      <td>-0.751957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rgb_channel_0_mean  rgb_channel_0_variance  rgb_channel_0_kurtosis  \\\n",
       "0          135.110013             3078.218207               -0.689453   \n",
       "1          132.208227             3961.755789               -1.130258   \n",
       "2          143.534598             2684.064771               -0.122034   \n",
       "3          134.806521             3150.853829               -0.752603   \n",
       "4          133.956593             1928.268962               -0.601913   \n",
       "\n",
       "   rgb_channel_0_skewness  rgb_channel_1_mean  rgb_channel_1_variance  \\\n",
       "0               -0.192670          157.315011             3449.301597   \n",
       "1               -0.302681          158.333586             3558.081163   \n",
       "2               -0.726785          163.988122             3032.684210   \n",
       "3                0.011988          155.817223             2935.205771   \n",
       "4                0.189324          165.174087             1995.183999   \n",
       "\n",
       "   rgb_channel_1_kurtosis  rgb_channel_1_skewness  rgb_channel_2_mean  \\\n",
       "0               -0.484632               -0.518085           67.831274   \n",
       "1               -0.908913               -0.458059           50.368921   \n",
       "2                0.098623               -0.932985           67.333745   \n",
       "3               -0.687641               -0.282174          112.783462   \n",
       "4               -0.653920               -0.250057           46.036372   \n",
       "\n",
       "   rgb_channel_2_variance  ...  lab_channel_0_kurtosis  \\\n",
       "0             2087.782836  ...               -0.296129   \n",
       "1             1453.704717  ...               -0.781886   \n",
       "2             1393.034182  ...                0.395657   \n",
       "3             2356.163989  ...               -0.573437   \n",
       "4             1525.296728  ...               -0.566688   \n",
       "\n",
       "   lab_channel_0_skewness  lab_channel_1_mean  lab_channel_1_variance  \\\n",
       "0               -0.606758          106.950853               61.335754   \n",
       "1               -0.534726          103.516083               21.999662   \n",
       "2               -1.041771          106.869300               39.706570   \n",
       "3               -0.355013          112.668666               26.981995   \n",
       "4               -0.252408          101.051419               77.759832   \n",
       "\n",
       "   lab_channel_1_kurtosis  lab_channel_1_skewness  lab_channel_2_mean  \\\n",
       "0                0.410354                0.917856          169.947226   \n",
       "1                2.755215                1.327478          177.711097   \n",
       "2                1.180393                1.184484          173.509228   \n",
       "3                2.443752                0.798156          148.112783   \n",
       "4                0.647577                1.165816          180.711256   \n",
       "\n",
       "   lab_channel_2_variance  lab_channel_2_kurtosis  lab_channel_2_skewness  \n",
       "0              203.966363                0.049806               -0.576146  \n",
       "1              198.029258                0.350495               -0.735196  \n",
       "2              201.620730                0.534828               -0.842716  \n",
       "3               56.290194                1.918221               -0.305414  \n",
       "4              190.856598                0.315457               -0.751957  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_color.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374ff69-1bee-49c9-a127-38bf7b2154fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Zernike Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb2e3d8b-9bcb-4314-8271-fbda88d99c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mahotas.features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def zernike(df, zernike_order):\n",
    "    features_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        gray_img = row['gray']\n",
    "\n",
    "        # Check if the image is grayscale\n",
    "        if len(gray_img.shape) == 2:\n",
    "            # If grayscale, compute Zernike moments directly\n",
    "            moments = mahotas.features.zernike_moments(gray_img, radius=zernike_order)\n",
    "        else:\n",
    "            raise ValueError(\"Input image must be grayscale.\")\n",
    "\n",
    "        features_list.append(moments)\n",
    "\n",
    "    # Determine the number of features per image\n",
    "    num_features_per_image = len(features_list[0])\n",
    "\n",
    "    # Add features to the DataFrame\n",
    "    feature_columns = [f'feature_{i}' for i in range(num_features_per_image)]\n",
    "    df_features = pd.DataFrame(features_list, columns=feature_columns)\n",
    "    df_result = pd.concat([df, df_features], axis=1)\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2839e315-3874-4df5-9d76-5f159e11d6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.085352</td>\n",
       "      <td>0.096424</td>\n",
       "      <td>0.007132</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>0.035195</td>\n",
       "      <td>0.047294</td>\n",
       "      <td>0.034389</td>\n",
       "      <td>0.012664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021457</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>0.024865</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.017575</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.035992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.014304</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>0.037181</td>\n",
       "      <td>0.013888</td>\n",
       "      <td>0.019413</td>\n",
       "      <td>0.032812</td>\n",
       "      <td>0.021023</td>\n",
       "      <td>0.036471</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019798</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.033492</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.008411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.079290</td>\n",
       "      <td>0.030770</td>\n",
       "      <td>0.009481</td>\n",
       "      <td>0.047826</td>\n",
       "      <td>0.057547</td>\n",
       "      <td>0.014013</td>\n",
       "      <td>0.021884</td>\n",
       "      <td>0.035470</td>\n",
       "      <td>0.021808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028761</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>0.039179</td>\n",
       "      <td>0.039791</td>\n",
       "      <td>0.023068</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.024128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.037962</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.070988</td>\n",
       "      <td>0.044010</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>0.015647</td>\n",
       "      <td>0.008096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.012084</td>\n",
       "      <td>0.020084</td>\n",
       "      <td>0.021049</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.017935</td>\n",
       "      <td>0.029116</td>\n",
       "      <td>0.023423</td>\n",
       "      <td>0.023256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.041976</td>\n",
       "      <td>0.050413</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>0.035719</td>\n",
       "      <td>0.045422</td>\n",
       "      <td>0.056392</td>\n",
       "      <td>0.064779</td>\n",
       "      <td>0.029646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025135</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.041041</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.004188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0    0.31831   0.017224   0.085352   0.096424   0.007132   0.019454   \n",
       "1    0.31831   0.014304   0.028368   0.037181   0.013888   0.019413   \n",
       "2    0.31831   0.079290   0.030770   0.009481   0.047826   0.057547   \n",
       "3    0.31831   0.037962   0.004782   0.037121   0.070988   0.044010   \n",
       "4    0.31831   0.008223   0.041976   0.050413   0.029785   0.035719   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  ...  feature_15  feature_16  \\\n",
       "0   0.035195   0.047294   0.034389   0.012664  ...    0.021457    0.009277   \n",
       "1   0.032812   0.021023   0.036471   0.003854  ...    0.019798    0.021939   \n",
       "2   0.014013   0.021884   0.035470   0.021808  ...    0.028761    0.049967   \n",
       "3   0.004336   0.021578   0.015647   0.008096  ...    0.017136    0.012084   \n",
       "4   0.045422   0.056392   0.064779   0.029646  ...    0.025135    0.003060   \n",
       "\n",
       "   feature_17  feature_18  feature_19  feature_20  feature_21  feature_22  \\\n",
       "0    0.024865    0.004817    0.017575    0.003713    0.003742    0.016868   \n",
       "1    0.024250    0.033492    0.027907    0.000967    0.009835    0.017213   \n",
       "2    0.039179    0.039791    0.023068    0.031944    0.021562    0.009072   \n",
       "3    0.020084    0.021049    0.013913    0.004487    0.017935    0.029116   \n",
       "4    0.024900    0.048771    0.041041    0.005773    0.034733    0.012531   \n",
       "\n",
       "   feature_23  feature_24  \n",
       "0    0.031700    0.035992  \n",
       "1    0.006593    0.008411  \n",
       "2    0.019779    0.024128  \n",
       "3    0.023423    0.023256  \n",
       "4    0.014290    0.004188  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zernike_df = zernike(extra_resized, 10)\n",
    "f_zernike = zernike_df.iloc[:, 51:]\n",
    "f_zernike.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f9f14-f5c1-49e8-bc81-0ff472678dd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Legendre Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04c31faf-4303-4c2c-9116-db4207ca5f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extra_resized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c11c6966-8fce-4fe6-b283-0ae5a249459b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l_input = extra_resized.drop(columns=['class','hsv','lab','red','green','blue', 'hue_v', 'saturation_v', 'value_v', 'lightness', 'a', 'b'])\n",
    "l_input = l_input.iloc[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "797a48fb-2cbb-4377-a2c7-c851bf96b9de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>rgb</th>\n",
       "      <th>gray</th>\n",
       "      <th>rgb_channel_0_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLB_multi_leaf (1).jpg</td>\n",
       "      <td>[[[115, 152, 47], [78, 113, 9], [116, 147, 43]...</td>\n",
       "      <td>[[129, 91, 126, 196, 208, 157, 175, 128, 114, ...</td>\n",
       "      <td>135.110013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLB_multi_leaf (2).jpg</td>\n",
       "      <td>[[[198, 218, 97], [190, 210, 89], [189, 208, 9...</td>\n",
       "      <td>[[198, 190, 189, 183, 189, 189, 187, 187, 189,...</td>\n",
       "      <td>132.208227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLB_multi_leaf (3).jpg</td>\n",
       "      <td>[[[137, 176, 51], [166, 203, 64], [165, 200, 3...</td>\n",
       "      <td>[[150, 176, 171, 177, 193, 187, 142, 136, 135,...</td>\n",
       "      <td>143.534598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLB_multi_leaf (4).jpg</td>\n",
       "      <td>[[[192, 212, 143], [201, 222, 157], [187, 206,...</td>\n",
       "      <td>[[198, 208, 194, 198, 196, 159, 143, 145, 165,...</td>\n",
       "      <td>134.806521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BLB_multi_leaf (5).jpg</td>\n",
       "      <td>[[[132, 176, 27], [135, 180, 27], [161, 206, 4...</td>\n",
       "      <td>[[146, 149, 174, 126, 153, 153, 185, 201, 210,...</td>\n",
       "      <td>133.956593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 img_name                                                rgb  \\\n",
       "0  BLB_multi_leaf (1).jpg  [[[115, 152, 47], [78, 113, 9], [116, 147, 43]...   \n",
       "1  BLB_multi_leaf (2).jpg  [[[198, 218, 97], [190, 210, 89], [189, 208, 9...   \n",
       "2  BLB_multi_leaf (3).jpg  [[[137, 176, 51], [166, 203, 64], [165, 200, 3...   \n",
       "3  BLB_multi_leaf (4).jpg  [[[192, 212, 143], [201, 222, 157], [187, 206,...   \n",
       "4  BLB_multi_leaf (5).jpg  [[[132, 176, 27], [135, 180, 27], [161, 206, 4...   \n",
       "\n",
       "                                                gray  rgb_channel_0_mean  \n",
       "0  [[129, 91, 126, 196, 208, 157, 175, 128, 114, ...          135.110013  \n",
       "1  [[198, 190, 189, 183, 189, 189, 187, 187, 189,...          132.208227  \n",
       "2  [[150, 176, 171, 177, 193, 187, 142, 136, 135,...          143.534598  \n",
       "3  [[198, 208, 194, 198, 196, 159, 143, 145, 165,...          134.806521  \n",
       "4  [[146, 149, 174, 126, 153, 153, 185, 201, 210,...          133.956593  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eac178d0-3ed3-4c52-a778-5ae124e3d99c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from scipy.special import legendre\n",
    "\n",
    "def compute_legendre_features(rgb_image, degree):\n",
    "    # Get V channel from RGB image\n",
    "    v_channel = rgb_image[:, :, 2]\n",
    "\n",
    "    # Define x and y coordinates\n",
    "    x_size, y_size = v_channel.shape\n",
    "    x = np.linspace(-1, 1, x_size)\n",
    "    y = np.linspace(-1, 1, y_size)\n",
    "\n",
    "    # Compute Legendre features for the V channel\n",
    "    legendre_features = np.zeros((degree + 1, degree + 1))\n",
    "\n",
    "    for j in range(degree + 1):\n",
    "        for k in range(degree + 1):\n",
    "            legendre_features[j, k] = np.sum(v_channel * legendre(j)(x) * legendre(k)(y))\n",
    "\n",
    "    return legendre_features.flatten()\n",
    "\n",
    "def add_legendre(df, degree):\n",
    "    # Create a new DataFrame for Legendre features\n",
    "    legendre_columns = [f'v_legendre_{i}_{j}' for i in range(degree + 1) for j in range(degree + 1)]\n",
    "    legendre_df = pd.DataFrame(df['rgb'].apply(lambda rgb: compute_legendre_features(rgb, degree)).values.tolist(), columns=legendre_columns)\n",
    "\n",
    "    # Concatenate the new DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([df, legendre_df], axis=1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43846dc1-14e3-448d-adc4-a22e38c851ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "legendre_df = add_legendre(l_input, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b74b7ce3-9c49-42a7-95d2-ea2df961247b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_legendre_0_0</th>\n",
       "      <th>v_legendre_0_1</th>\n",
       "      <th>v_legendre_0_2</th>\n",
       "      <th>v_legendre_0_3</th>\n",
       "      <th>v_legendre_0_4</th>\n",
       "      <th>v_legendre_0_5</th>\n",
       "      <th>v_legendre_0_6</th>\n",
       "      <th>v_legendre_0_7</th>\n",
       "      <th>v_legendre_0_8</th>\n",
       "      <th>v_legendre_0_9</th>\n",
       "      <th>...</th>\n",
       "      <th>v_legendre_10_1</th>\n",
       "      <th>v_legendre_10_2</th>\n",
       "      <th>v_legendre_10_3</th>\n",
       "      <th>v_legendre_10_4</th>\n",
       "      <th>v_legendre_10_5</th>\n",
       "      <th>v_legendre_10_6</th>\n",
       "      <th>v_legendre_10_7</th>\n",
       "      <th>v_legendre_10_8</th>\n",
       "      <th>v_legendre_10_9</th>\n",
       "      <th>v_legendre_10_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3403502.0</td>\n",
       "      <td>97362.493274</td>\n",
       "      <td>-48201.759416</td>\n",
       "      <td>-43493.535531</td>\n",
       "      <td>-7187.545250</td>\n",
       "      <td>-13629.967522</td>\n",
       "      <td>26794.954058</td>\n",
       "      <td>-16840.577069</td>\n",
       "      <td>-19979.273476</td>\n",
       "      <td>3169.619967</td>\n",
       "      <td>...</td>\n",
       "      <td>11979.555139</td>\n",
       "      <td>6950.403894</td>\n",
       "      <td>-1337.870389</td>\n",
       "      <td>10980.581080</td>\n",
       "      <td>-2731.321916</td>\n",
       "      <td>13832.668995</td>\n",
       "      <td>-4770.100418</td>\n",
       "      <td>6167.284262</td>\n",
       "      <td>4327.878377</td>\n",
       "      <td>171054.141659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2527311.0</td>\n",
       "      <td>103013.053812</td>\n",
       "      <td>-36361.603672</td>\n",
       "      <td>5917.770393</td>\n",
       "      <td>47863.129349</td>\n",
       "      <td>1575.078268</td>\n",
       "      <td>12099.674282</td>\n",
       "      <td>-3201.383018</td>\n",
       "      <td>18315.225460</td>\n",
       "      <td>1422.844543</td>\n",
       "      <td>...</td>\n",
       "      <td>-821.672825</td>\n",
       "      <td>16762.645859</td>\n",
       "      <td>2097.910709</td>\n",
       "      <td>12284.262029</td>\n",
       "      <td>195.439018</td>\n",
       "      <td>20480.883830</td>\n",
       "      <td>3132.492664</td>\n",
       "      <td>11516.628122</td>\n",
       "      <td>8400.271678</td>\n",
       "      <td>134299.780209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3378538.0</td>\n",
       "      <td>57117.757848</td>\n",
       "      <td>-80632.660661</td>\n",
       "      <td>-510.363287</td>\n",
       "      <td>11451.263244</td>\n",
       "      <td>-12619.841113</td>\n",
       "      <td>-28828.907259</td>\n",
       "      <td>-769.368737</td>\n",
       "      <td>27917.794633</td>\n",
       "      <td>-6883.608118</td>\n",
       "      <td>...</td>\n",
       "      <td>3963.350675</td>\n",
       "      <td>18954.699088</td>\n",
       "      <td>-2074.126958</td>\n",
       "      <td>8085.322444</td>\n",
       "      <td>1051.648198</td>\n",
       "      <td>8771.579378</td>\n",
       "      <td>-2117.981725</td>\n",
       "      <td>3647.296353</td>\n",
       "      <td>5448.970980</td>\n",
       "      <td>164009.682234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5659023.0</td>\n",
       "      <td>-217617.215247</td>\n",
       "      <td>39749.222928</td>\n",
       "      <td>-6034.575324</td>\n",
       "      <td>13640.673159</td>\n",
       "      <td>21378.007074</td>\n",
       "      <td>44068.112251</td>\n",
       "      <td>-24151.624353</td>\n",
       "      <td>14049.614456</td>\n",
       "      <td>-33151.322155</td>\n",
       "      <td>...</td>\n",
       "      <td>-4619.286498</td>\n",
       "      <td>15514.090473</td>\n",
       "      <td>-6651.294307</td>\n",
       "      <td>31327.480610</td>\n",
       "      <td>553.852950</td>\n",
       "      <td>28293.237294</td>\n",
       "      <td>-4090.823341</td>\n",
       "      <td>27115.154433</td>\n",
       "      <td>-18929.538381</td>\n",
       "      <td>296838.144091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2309921.0</td>\n",
       "      <td>-148810.686099</td>\n",
       "      <td>-37569.159223</td>\n",
       "      <td>52055.054845</td>\n",
       "      <td>44925.442955</td>\n",
       "      <td>-23770.114279</td>\n",
       "      <td>17043.186742</td>\n",
       "      <td>11900.041479</td>\n",
       "      <td>1297.065850</td>\n",
       "      <td>-35581.018742</td>\n",
       "      <td>...</td>\n",
       "      <td>-9263.364484</td>\n",
       "      <td>11693.901946</td>\n",
       "      <td>-7594.119586</td>\n",
       "      <td>7040.301359</td>\n",
       "      <td>-2945.920523</td>\n",
       "      <td>20936.096634</td>\n",
       "      <td>1642.000010</td>\n",
       "      <td>5613.007761</td>\n",
       "      <td>-10798.603049</td>\n",
       "      <td>122542.843959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   v_legendre_0_0  v_legendre_0_1  v_legendre_0_2  v_legendre_0_3  \\\n",
       "0       3403502.0    97362.493274   -48201.759416   -43493.535531   \n",
       "1       2527311.0   103013.053812   -36361.603672     5917.770393   \n",
       "2       3378538.0    57117.757848   -80632.660661     -510.363287   \n",
       "3       5659023.0  -217617.215247    39749.222928    -6034.575324   \n",
       "4       2309921.0  -148810.686099   -37569.159223    52055.054845   \n",
       "\n",
       "   v_legendre_0_4  v_legendre_0_5  v_legendre_0_6  v_legendre_0_7  \\\n",
       "0    -7187.545250   -13629.967522    26794.954058   -16840.577069   \n",
       "1    47863.129349     1575.078268    12099.674282    -3201.383018   \n",
       "2    11451.263244   -12619.841113   -28828.907259     -769.368737   \n",
       "3    13640.673159    21378.007074    44068.112251   -24151.624353   \n",
       "4    44925.442955   -23770.114279    17043.186742    11900.041479   \n",
       "\n",
       "   v_legendre_0_8  v_legendre_0_9  ...  v_legendre_10_1  v_legendre_10_2  \\\n",
       "0   -19979.273476     3169.619967  ...     11979.555139      6950.403894   \n",
       "1    18315.225460     1422.844543  ...      -821.672825     16762.645859   \n",
       "2    27917.794633    -6883.608118  ...      3963.350675     18954.699088   \n",
       "3    14049.614456   -33151.322155  ...     -4619.286498     15514.090473   \n",
       "4     1297.065850   -35581.018742  ...     -9263.364484     11693.901946   \n",
       "\n",
       "   v_legendre_10_3  v_legendre_10_4  v_legendre_10_5  v_legendre_10_6  \\\n",
       "0     -1337.870389     10980.581080     -2731.321916     13832.668995   \n",
       "1      2097.910709     12284.262029       195.439018     20480.883830   \n",
       "2     -2074.126958      8085.322444      1051.648198      8771.579378   \n",
       "3     -6651.294307     31327.480610       553.852950     28293.237294   \n",
       "4     -7594.119586      7040.301359     -2945.920523     20936.096634   \n",
       "\n",
       "   v_legendre_10_7  v_legendre_10_8  v_legendre_10_9  v_legendre_10_10  \n",
       "0     -4770.100418      6167.284262      4327.878377     171054.141659  \n",
       "1      3132.492664     11516.628122      8400.271678     134299.780209  \n",
       "2     -2117.981725      3647.296353      5448.970980     164009.682234  \n",
       "3     -4090.823341     27115.154433    -18929.538381     296838.144091  \n",
       "4      1642.000010      5613.007761    -10798.603049     122542.843959  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_legendre = legendre_df.iloc[:, 4:]\n",
    "f_legendre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bb6a4-6f30-4f3c-8b50-0bad0e8b6c75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Complete Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db5931a6-4d94-44df-8c1a-82076732f0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame with just the selected column\n",
    "classes = pd.DataFrame(extra_resized['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5002378-98b0-4b70-ba29-a07ba57764f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#complete_features = pd.concat([classes,f_color,f_histogram, f_glcm], axis=1)\n",
    "complete_features = pd.concat([classes,f_histogram, f_glcm, f_color, f_legendre, f_zernike], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3453770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_features.to_csv('complete_features.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "060e531f-0a6e-455b-8dbf-07ca777d8c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>red_1</th>\n",
       "      <th>red_2</th>\n",
       "      <th>red_3</th>\n",
       "      <th>red_4</th>\n",
       "      <th>red_5</th>\n",
       "      <th>red_6</th>\n",
       "      <th>red_7</th>\n",
       "      <th>red_8</th>\n",
       "      <th>red_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "      <td>93</td>\n",
       "      <td>143</td>\n",
       "      <td>223</td>\n",
       "      <td>231</td>\n",
       "      <td>305</td>\n",
       "      <td>314</td>\n",
       "      <td>330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021457</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>0.024865</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.017575</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.035992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>153</td>\n",
       "      <td>210</td>\n",
       "      <td>261</td>\n",
       "      <td>308</td>\n",
       "      <td>349</td>\n",
       "      <td>391</td>\n",
       "      <td>392</td>\n",
       "      <td>358</td>\n",
       "      <td>383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019798</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.033492</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.008411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>62</td>\n",
       "      <td>145</td>\n",
       "      <td>205</td>\n",
       "      <td>290</td>\n",
       "      <td>337</td>\n",
       "      <td>325</td>\n",
       "      <td>338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028761</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>0.039179</td>\n",
       "      <td>0.039791</td>\n",
       "      <td>0.023068</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.024128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>53</td>\n",
       "      <td>89</td>\n",
       "      <td>106</td>\n",
       "      <td>194</td>\n",
       "      <td>242</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.012084</td>\n",
       "      <td>0.020084</td>\n",
       "      <td>0.021049</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.017935</td>\n",
       "      <td>0.029116</td>\n",
       "      <td>0.023423</td>\n",
       "      <td>0.023256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025135</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.041041</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.004188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 934 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   class  red_1  red_2  red_3  red_4  red_5  red_6  red_7  \\\n",
       "0  bacterial_leaf_blight     19     47     93    143    223    231    305   \n",
       "1  bacterial_leaf_blight    153    210    261    308    349    391    392   \n",
       "2  bacterial_leaf_blight     14     35     62    145    205    290    337   \n",
       "3  bacterial_leaf_blight     14     14     40     53     89    106    194   \n",
       "4  bacterial_leaf_blight      0      1      3      4      1      6      6   \n",
       "\n",
       "   red_8  red_9  ...  feature_15  feature_16  feature_17  feature_18  \\\n",
       "0    314    330  ...    0.021457    0.009277    0.024865    0.004817   \n",
       "1    358    383  ...    0.019798    0.021939    0.024250    0.033492   \n",
       "2    325    338  ...    0.028761    0.049967    0.039179    0.039791   \n",
       "3    242    267  ...    0.017136    0.012084    0.020084    0.021049   \n",
       "4      9      8  ...    0.025135    0.003060    0.024900    0.048771   \n",
       "\n",
       "   feature_19  feature_20  feature_21  feature_22  feature_23  feature_24  \n",
       "0    0.017575    0.003713    0.003742    0.016868    0.031700    0.035992  \n",
       "1    0.027907    0.000967    0.009835    0.017213    0.006593    0.008411  \n",
       "2    0.023068    0.031944    0.021562    0.009072    0.019779    0.024128  \n",
       "3    0.013913    0.004487    0.017935    0.029116    0.023423    0.023256  \n",
       "4    0.041041    0.005773    0.034733    0.012531    0.014290    0.004188  \n",
       "\n",
       "[5 rows x 934 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5fb4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "legendre_features = complete_features.iloc[:, [0] + list(range(788, 908))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93653fd4-c7c1-4763-ab89-2102d2f29c4c",
   "metadata": {},
   "source": [
    "# Data Normalization and One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80096471-c274-4870-bea1-05b14edc651f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_dataframe(df):\n",
    "    # Create a MinMaxScaler instance\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Extract numerical columns (only numerical columns need normalization)\n",
    "    numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "    # Apply normalization to each numerical column\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b235b92a-228e-4021-975a-b860cafec988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josh\\AppData\\Local\\Temp\\ipykernel_9260\\3205247050.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>v_legendre_0_0</th>\n",
       "      <th>v_legendre_0_1</th>\n",
       "      <th>v_legendre_0_2</th>\n",
       "      <th>v_legendre_0_3</th>\n",
       "      <th>v_legendre_0_4</th>\n",
       "      <th>v_legendre_0_5</th>\n",
       "      <th>v_legendre_0_6</th>\n",
       "      <th>v_legendre_0_7</th>\n",
       "      <th>v_legendre_0_8</th>\n",
       "      <th>...</th>\n",
       "      <th>v_legendre_10_0</th>\n",
       "      <th>v_legendre_10_1</th>\n",
       "      <th>v_legendre_10_2</th>\n",
       "      <th>v_legendre_10_3</th>\n",
       "      <th>v_legendre_10_4</th>\n",
       "      <th>v_legendre_10_5</th>\n",
       "      <th>v_legendre_10_6</th>\n",
       "      <th>v_legendre_10_7</th>\n",
       "      <th>v_legendre_10_8</th>\n",
       "      <th>v_legendre_10_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.244273</td>\n",
       "      <td>0.514443</td>\n",
       "      <td>0.207284</td>\n",
       "      <td>0.445034</td>\n",
       "      <td>0.639331</td>\n",
       "      <td>0.418836</td>\n",
       "      <td>0.337977</td>\n",
       "      <td>0.554474</td>\n",
       "      <td>0.532776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364484</td>\n",
       "      <td>0.580130</td>\n",
       "      <td>0.472978</td>\n",
       "      <td>0.501938</td>\n",
       "      <td>0.196798</td>\n",
       "      <td>0.464539</td>\n",
       "      <td>0.303120</td>\n",
       "      <td>0.480633</td>\n",
       "      <td>0.193679</td>\n",
       "      <td>0.524766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.168911</td>\n",
       "      <td>0.516024</td>\n",
       "      <td>0.212275</td>\n",
       "      <td>0.486521</td>\n",
       "      <td>0.684601</td>\n",
       "      <td>0.437653</td>\n",
       "      <td>0.319648</td>\n",
       "      <td>0.574691</td>\n",
       "      <td>0.609400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266923</td>\n",
       "      <td>0.511824</td>\n",
       "      <td>0.534295</td>\n",
       "      <td>0.525292</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.486452</td>\n",
       "      <td>0.345129</td>\n",
       "      <td>0.516569</td>\n",
       "      <td>0.216158</td>\n",
       "      <td>0.536907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.242125</td>\n",
       "      <td>0.503176</td>\n",
       "      <td>0.193613</td>\n",
       "      <td>0.481124</td>\n",
       "      <td>0.654658</td>\n",
       "      <td>0.420086</td>\n",
       "      <td>0.268601</td>\n",
       "      <td>0.578296</td>\n",
       "      <td>0.628614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278909</td>\n",
       "      <td>0.537356</td>\n",
       "      <td>0.547994</td>\n",
       "      <td>0.496934</td>\n",
       "      <td>0.181003</td>\n",
       "      <td>0.492862</td>\n",
       "      <td>0.271140</td>\n",
       "      <td>0.492693</td>\n",
       "      <td>0.183089</td>\n",
       "      <td>0.528109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.438271</td>\n",
       "      <td>0.426264</td>\n",
       "      <td>0.244358</td>\n",
       "      <td>0.476486</td>\n",
       "      <td>0.656458</td>\n",
       "      <td>0.462160</td>\n",
       "      <td>0.359520</td>\n",
       "      <td>0.543637</td>\n",
       "      <td>0.600865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333922</td>\n",
       "      <td>0.491560</td>\n",
       "      <td>0.526493</td>\n",
       "      <td>0.465821</td>\n",
       "      <td>0.307799</td>\n",
       "      <td>0.489135</td>\n",
       "      <td>0.394494</td>\n",
       "      <td>0.483722</td>\n",
       "      <td>0.281708</td>\n",
       "      <td>0.455433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.150213</td>\n",
       "      <td>0.445526</td>\n",
       "      <td>0.211766</td>\n",
       "      <td>0.525259</td>\n",
       "      <td>0.682185</td>\n",
       "      <td>0.406287</td>\n",
       "      <td>0.325814</td>\n",
       "      <td>0.597075</td>\n",
       "      <td>0.575348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225538</td>\n",
       "      <td>0.466780</td>\n",
       "      <td>0.502620</td>\n",
       "      <td>0.459413</td>\n",
       "      <td>0.175302</td>\n",
       "      <td>0.462932</td>\n",
       "      <td>0.348006</td>\n",
       "      <td>0.509791</td>\n",
       "      <td>0.191350</td>\n",
       "      <td>0.479673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   class  v_legendre_0_0  v_legendre_0_1  v_legendre_0_2  \\\n",
       "0  bacterial_leaf_blight        0.244273        0.514443        0.207284   \n",
       "1  bacterial_leaf_blight        0.168911        0.516024        0.212275   \n",
       "2  bacterial_leaf_blight        0.242125        0.503176        0.193613   \n",
       "3  bacterial_leaf_blight        0.438271        0.426264        0.244358   \n",
       "4  bacterial_leaf_blight        0.150213        0.445526        0.211766   \n",
       "\n",
       "   v_legendre_0_3  v_legendre_0_4  v_legendre_0_5  v_legendre_0_6  \\\n",
       "0        0.445034        0.639331        0.418836        0.337977   \n",
       "1        0.486521        0.684601        0.437653        0.319648   \n",
       "2        0.481124        0.654658        0.420086        0.268601   \n",
       "3        0.476486        0.656458        0.462160        0.359520   \n",
       "4        0.525259        0.682185        0.406287        0.325814   \n",
       "\n",
       "   v_legendre_0_7  v_legendre_0_8  ...  v_legendre_10_0  v_legendre_10_1  \\\n",
       "0        0.554474        0.532776  ...         0.364484         0.580130   \n",
       "1        0.574691        0.609400  ...         0.266923         0.511824   \n",
       "2        0.578296        0.628614  ...         0.278909         0.537356   \n",
       "3        0.543637        0.600865  ...         0.333922         0.491560   \n",
       "4        0.597075        0.575348  ...         0.225538         0.466780   \n",
       "\n",
       "   v_legendre_10_2  v_legendre_10_3  v_legendre_10_4  v_legendre_10_5  \\\n",
       "0         0.472978         0.501938         0.196798         0.464539   \n",
       "1         0.534295         0.525292         0.203910         0.486452   \n",
       "2         0.547994         0.496934         0.181003         0.492862   \n",
       "3         0.526493         0.465821         0.307799         0.489135   \n",
       "4         0.502620         0.459413         0.175302         0.462932   \n",
       "\n",
       "   v_legendre_10_6  v_legendre_10_7  v_legendre_10_8  v_legendre_10_9  \n",
       "0         0.303120         0.480633         0.193679         0.524766  \n",
       "1         0.345129         0.516569         0.216158         0.536907  \n",
       "2         0.271140         0.492693         0.183089         0.528109  \n",
       "3         0.394494         0.483722         0.281708         0.455433  \n",
       "4         0.348006         0.509791         0.191350         0.479673  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_features = normalize_dataframe(legendre_features)\n",
    "normalized_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b339da2-35a0-455d-8db7-266f056dd5ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josh\\AppData\\Local\\Temp\\ipykernel_9260\\1409264267.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  normalized_features['class'] = label_encoder.fit_transform(normalized_features['class'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "normalized_features['class'] = label_encoder.fit_transform(normalized_features['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "097defd8-e917-45e2-a877-dfe178bfe066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>v_legendre_0_0</th>\n",
       "      <th>v_legendre_0_1</th>\n",
       "      <th>v_legendre_0_2</th>\n",
       "      <th>v_legendre_0_3</th>\n",
       "      <th>v_legendre_0_4</th>\n",
       "      <th>v_legendre_0_5</th>\n",
       "      <th>v_legendre_0_6</th>\n",
       "      <th>v_legendre_0_7</th>\n",
       "      <th>v_legendre_0_8</th>\n",
       "      <th>...</th>\n",
       "      <th>v_legendre_10_0</th>\n",
       "      <th>v_legendre_10_1</th>\n",
       "      <th>v_legendre_10_2</th>\n",
       "      <th>v_legendre_10_3</th>\n",
       "      <th>v_legendre_10_4</th>\n",
       "      <th>v_legendre_10_5</th>\n",
       "      <th>v_legendre_10_6</th>\n",
       "      <th>v_legendre_10_7</th>\n",
       "      <th>v_legendre_10_8</th>\n",
       "      <th>v_legendre_10_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.244273</td>\n",
       "      <td>0.514443</td>\n",
       "      <td>0.207284</td>\n",
       "      <td>0.445034</td>\n",
       "      <td>0.639331</td>\n",
       "      <td>0.418836</td>\n",
       "      <td>0.337977</td>\n",
       "      <td>0.554474</td>\n",
       "      <td>0.532776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364484</td>\n",
       "      <td>0.580130</td>\n",
       "      <td>0.472978</td>\n",
       "      <td>0.501938</td>\n",
       "      <td>0.196798</td>\n",
       "      <td>0.464539</td>\n",
       "      <td>0.303120</td>\n",
       "      <td>0.480633</td>\n",
       "      <td>0.193679</td>\n",
       "      <td>0.524766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.168911</td>\n",
       "      <td>0.516024</td>\n",
       "      <td>0.212275</td>\n",
       "      <td>0.486521</td>\n",
       "      <td>0.684601</td>\n",
       "      <td>0.437653</td>\n",
       "      <td>0.319648</td>\n",
       "      <td>0.574691</td>\n",
       "      <td>0.609400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266923</td>\n",
       "      <td>0.511824</td>\n",
       "      <td>0.534295</td>\n",
       "      <td>0.525292</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.486452</td>\n",
       "      <td>0.345129</td>\n",
       "      <td>0.516569</td>\n",
       "      <td>0.216158</td>\n",
       "      <td>0.536907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.242125</td>\n",
       "      <td>0.503176</td>\n",
       "      <td>0.193613</td>\n",
       "      <td>0.481124</td>\n",
       "      <td>0.654658</td>\n",
       "      <td>0.420086</td>\n",
       "      <td>0.268601</td>\n",
       "      <td>0.578296</td>\n",
       "      <td>0.628614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278909</td>\n",
       "      <td>0.537356</td>\n",
       "      <td>0.547994</td>\n",
       "      <td>0.496934</td>\n",
       "      <td>0.181003</td>\n",
       "      <td>0.492862</td>\n",
       "      <td>0.271140</td>\n",
       "      <td>0.492693</td>\n",
       "      <td>0.183089</td>\n",
       "      <td>0.528109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.438271</td>\n",
       "      <td>0.426264</td>\n",
       "      <td>0.244358</td>\n",
       "      <td>0.476486</td>\n",
       "      <td>0.656458</td>\n",
       "      <td>0.462160</td>\n",
       "      <td>0.359520</td>\n",
       "      <td>0.543637</td>\n",
       "      <td>0.600865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333922</td>\n",
       "      <td>0.491560</td>\n",
       "      <td>0.526493</td>\n",
       "      <td>0.465821</td>\n",
       "      <td>0.307799</td>\n",
       "      <td>0.489135</td>\n",
       "      <td>0.394494</td>\n",
       "      <td>0.483722</td>\n",
       "      <td>0.281708</td>\n",
       "      <td>0.455433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.150213</td>\n",
       "      <td>0.445526</td>\n",
       "      <td>0.211766</td>\n",
       "      <td>0.525259</td>\n",
       "      <td>0.682185</td>\n",
       "      <td>0.406287</td>\n",
       "      <td>0.325814</td>\n",
       "      <td>0.597075</td>\n",
       "      <td>0.575348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225538</td>\n",
       "      <td>0.466780</td>\n",
       "      <td>0.502620</td>\n",
       "      <td>0.459413</td>\n",
       "      <td>0.175302</td>\n",
       "      <td>0.462932</td>\n",
       "      <td>0.348006</td>\n",
       "      <td>0.509791</td>\n",
       "      <td>0.191350</td>\n",
       "      <td>0.479673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  v_legendre_0_0  v_legendre_0_1  v_legendre_0_2  v_legendre_0_3  \\\n",
       "0      0        0.244273        0.514443        0.207284        0.445034   \n",
       "1      0        0.168911        0.516024        0.212275        0.486521   \n",
       "2      0        0.242125        0.503176        0.193613        0.481124   \n",
       "3      0        0.438271        0.426264        0.244358        0.476486   \n",
       "4      0        0.150213        0.445526        0.211766        0.525259   \n",
       "\n",
       "   v_legendre_0_4  v_legendre_0_5  v_legendre_0_6  v_legendre_0_7  \\\n",
       "0        0.639331        0.418836        0.337977        0.554474   \n",
       "1        0.684601        0.437653        0.319648        0.574691   \n",
       "2        0.654658        0.420086        0.268601        0.578296   \n",
       "3        0.656458        0.462160        0.359520        0.543637   \n",
       "4        0.682185        0.406287        0.325814        0.597075   \n",
       "\n",
       "   v_legendre_0_8  ...  v_legendre_10_0  v_legendre_10_1  v_legendre_10_2  \\\n",
       "0        0.532776  ...         0.364484         0.580130         0.472978   \n",
       "1        0.609400  ...         0.266923         0.511824         0.534295   \n",
       "2        0.628614  ...         0.278909         0.537356         0.547994   \n",
       "3        0.600865  ...         0.333922         0.491560         0.526493   \n",
       "4        0.575348  ...         0.225538         0.466780         0.502620   \n",
       "\n",
       "   v_legendre_10_3  v_legendre_10_4  v_legendre_10_5  v_legendre_10_6  \\\n",
       "0         0.501938         0.196798         0.464539         0.303120   \n",
       "1         0.525292         0.203910         0.486452         0.345129   \n",
       "2         0.496934         0.181003         0.492862         0.271140   \n",
       "3         0.465821         0.307799         0.489135         0.394494   \n",
       "4         0.459413         0.175302         0.462932         0.348006   \n",
       "\n",
       "   v_legendre_10_7  v_legendre_10_8  v_legendre_10_9  \n",
       "0         0.480633         0.193679         0.524766  \n",
       "1         0.516569         0.216158         0.536907  \n",
       "2         0.492693         0.183089         0.528109  \n",
       "3         0.483722         0.281708         0.455433  \n",
       "4         0.509791         0.191350         0.479673  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77638c94-04af-43fb-a1db-0548a9c497cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acc8851d-dc36-4634-8d2f-8a3016c14ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def custom_train_test_split(dataframe, target_column, test_size=0.2, random_state=None):\n",
    "    # Separate features and labels\n",
    "    X = dataframe.drop(target_column, axis=1)  # Features\n",
    "    y = dataframe[target_column]  # Labels\n",
    "\n",
    "    # Perform the train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53594f6d-e920-4b66-91d8-7ba1ad60e4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = custom_train_test_split(normalized_features, 'class', test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "165bd148-e43b-401d-94a1-24f08ca4ef46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features (all columns except the first)\n",
    "x_d = normalized_features.iloc[:, 1:].values\n",
    "\n",
    "# Extract target variable (first column)\n",
    "y_d = normalized_features.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b4890-0ac4-4c6d-b3c1-9acf0c770916",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "705fec05-d73e-470c-9a2b-d71b16d96f36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "33/33 [==============================] - 2s 15ms/step - loss: 2.6016 - accuracy: 0.1326 - val_loss: 2.5193 - val_accuracy: 0.1250\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3922 - accuracy: 0.1651 - val_loss: 2.4657 - val_accuracy: 0.1607\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3017 - accuracy: 0.1975 - val_loss: 2.4180 - val_accuracy: 0.1429\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2178 - accuracy: 0.2567 - val_loss: 2.3780 - val_accuracy: 0.1964\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1376 - accuracy: 0.3158 - val_loss: 2.3287 - val_accuracy: 0.2143\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0651 - accuracy: 0.3378 - val_loss: 2.2801 - val_accuracy: 0.1786\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0056 - accuracy: 0.3511 - val_loss: 2.2457 - val_accuracy: 0.1964\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9484 - accuracy: 0.3712 - val_loss: 2.2125 - val_accuracy: 0.1786\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9029 - accuracy: 0.3969 - val_loss: 2.1960 - val_accuracy: 0.2143\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8695 - accuracy: 0.3845 - val_loss: 2.1568 - val_accuracy: 0.2321\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8406 - accuracy: 0.4141 - val_loss: 2.1626 - val_accuracy: 0.2679\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8137 - accuracy: 0.4218 - val_loss: 2.1568 - val_accuracy: 0.2500\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7860 - accuracy: 0.4246 - val_loss: 2.1307 - val_accuracy: 0.2500\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7584 - accuracy: 0.4342 - val_loss: 2.1305 - val_accuracy: 0.2500\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7343 - accuracy: 0.4475 - val_loss: 2.1334 - val_accuracy: 0.2321\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7160 - accuracy: 0.4542 - val_loss: 2.1235 - val_accuracy: 0.2857\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6951 - accuracy: 0.4752 - val_loss: 2.1087 - val_accuracy: 0.3214\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6733 - accuracy: 0.4733 - val_loss: 2.1085 - val_accuracy: 0.2857\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6591 - accuracy: 0.4742 - val_loss: 2.1074 - val_accuracy: 0.2500\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6371 - accuracy: 0.4790 - val_loss: 2.1075 - val_accuracy: 0.2857\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6302 - accuracy: 0.4828 - val_loss: 2.1184 - val_accuracy: 0.2500\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6103 - accuracy: 0.4885 - val_loss: 2.1232 - val_accuracy: 0.2500\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5930 - accuracy: 0.4971 - val_loss: 2.1259 - val_accuracy: 0.2679\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5737 - accuracy: 0.5010 - val_loss: 2.1298 - val_accuracy: 0.2857\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5633 - accuracy: 0.5038 - val_loss: 2.1343 - val_accuracy: 0.2679\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5499 - accuracy: 0.5143 - val_loss: 2.1094 - val_accuracy: 0.3214\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5310 - accuracy: 0.5229 - val_loss: 2.1237 - val_accuracy: 0.3036\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5203 - accuracy: 0.5200 - val_loss: 2.1443 - val_accuracy: 0.3214\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5028 - accuracy: 0.5296 - val_loss: 2.1423 - val_accuracy: 0.3393\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4902 - accuracy: 0.5267 - val_loss: 2.1402 - val_accuracy: 0.2857\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4841 - accuracy: 0.5439 - val_loss: 2.1588 - val_accuracy: 0.3393\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4636 - accuracy: 0.5296 - val_loss: 2.1569 - val_accuracy: 0.3036\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4504 - accuracy: 0.5439 - val_loss: 2.1557 - val_accuracy: 0.3036\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4442 - accuracy: 0.5487 - val_loss: 2.1735 - val_accuracy: 0.2857\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4329 - accuracy: 0.5458 - val_loss: 2.1711 - val_accuracy: 0.3036\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4194 - accuracy: 0.5563 - val_loss: 2.1872 - val_accuracy: 0.2857\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4127 - accuracy: 0.5534 - val_loss: 2.1840 - val_accuracy: 0.2679\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3965 - accuracy: 0.5658 - val_loss: 2.1807 - val_accuracy: 0.3036\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3874 - accuracy: 0.5639 - val_loss: 2.2097 - val_accuracy: 0.2857\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3733 - accuracy: 0.5668 - val_loss: 2.1989 - val_accuracy: 0.3036\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3678 - accuracy: 0.5735 - val_loss: 2.2154 - val_accuracy: 0.3036\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3534 - accuracy: 0.5706 - val_loss: 2.2234 - val_accuracy: 0.2857\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3398 - accuracy: 0.5668 - val_loss: 2.2215 - val_accuracy: 0.2857\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3308 - accuracy: 0.5830 - val_loss: 2.2200 - val_accuracy: 0.2857\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3210 - accuracy: 0.5830 - val_loss: 2.2261 - val_accuracy: 0.2857\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3083 - accuracy: 0.5887 - val_loss: 2.2361 - val_accuracy: 0.3393\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3044 - accuracy: 0.5964 - val_loss: 2.2573 - val_accuracy: 0.3214\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2951 - accuracy: 0.5811 - val_loss: 2.2485 - val_accuracy: 0.2857\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2821 - accuracy: 0.5983 - val_loss: 2.2702 - val_accuracy: 0.3036\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2727 - accuracy: 0.6021 - val_loss: 2.2900 - val_accuracy: 0.2857\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2641 - accuracy: 0.6002 - val_loss: 2.3037 - val_accuracy: 0.3214\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2561 - accuracy: 0.6059 - val_loss: 2.3002 - val_accuracy: 0.3036\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2431 - accuracy: 0.6078 - val_loss: 2.3072 - val_accuracy: 0.3036\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2343 - accuracy: 0.6202 - val_loss: 2.3190 - val_accuracy: 0.2857\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2312 - accuracy: 0.6097 - val_loss: 2.3299 - val_accuracy: 0.2857\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2189 - accuracy: 0.6174 - val_loss: 2.3172 - val_accuracy: 0.2679\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2082 - accuracy: 0.6116 - val_loss: 2.3347 - val_accuracy: 0.2857\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2053 - accuracy: 0.6183 - val_loss: 2.3425 - val_accuracy: 0.2857\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1926 - accuracy: 0.6202 - val_loss: 2.3603 - val_accuracy: 0.2857\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1898 - accuracy: 0.6240 - val_loss: 2.3387 - val_accuracy: 0.3036\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1770 - accuracy: 0.6260 - val_loss: 2.3827 - val_accuracy: 0.2679\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1681 - accuracy: 0.6250 - val_loss: 2.3672 - val_accuracy: 0.2679\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1569 - accuracy: 0.6393 - val_loss: 2.3868 - val_accuracy: 0.2679\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1507 - accuracy: 0.6317 - val_loss: 2.3738 - val_accuracy: 0.3036\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1499 - accuracy: 0.6393 - val_loss: 2.3950 - val_accuracy: 0.3036\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1367 - accuracy: 0.6450 - val_loss: 2.4273 - val_accuracy: 0.2857\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1320 - accuracy: 0.6365 - val_loss: 2.4042 - val_accuracy: 0.2857\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1236 - accuracy: 0.6384 - val_loss: 2.4472 - val_accuracy: 0.2679\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1223 - accuracy: 0.6422 - val_loss: 2.4869 - val_accuracy: 0.2857\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1026 - accuracy: 0.6565 - val_loss: 2.4315 - val_accuracy: 0.3036\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0988 - accuracy: 0.6517 - val_loss: 2.4544 - val_accuracy: 0.2857\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0905 - accuracy: 0.6498 - val_loss: 2.4938 - val_accuracy: 0.2679\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0872 - accuracy: 0.6517 - val_loss: 2.4861 - val_accuracy: 0.2857\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0789 - accuracy: 0.6594 - val_loss: 2.4899 - val_accuracy: 0.2857\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0704 - accuracy: 0.6565 - val_loss: 2.4962 - val_accuracy: 0.2857\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0628 - accuracy: 0.6603 - val_loss: 2.5491 - val_accuracy: 0.2500\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0648 - accuracy: 0.6555 - val_loss: 2.5169 - val_accuracy: 0.2857\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0529 - accuracy: 0.6584 - val_loss: 2.5372 - val_accuracy: 0.2679\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0432 - accuracy: 0.6660 - val_loss: 2.5890 - val_accuracy: 0.2679\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0343 - accuracy: 0.6784 - val_loss: 2.5720 - val_accuracy: 0.3036\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0251 - accuracy: 0.6727 - val_loss: 2.5633 - val_accuracy: 0.2857\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0161 - accuracy: 0.6803 - val_loss: 2.5989 - val_accuracy: 0.2500\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0104 - accuracy: 0.6737 - val_loss: 2.6213 - val_accuracy: 0.2500\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0118 - accuracy: 0.6718 - val_loss: 2.6162 - val_accuracy: 0.2679\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0001 - accuracy: 0.6784 - val_loss: 2.6441 - val_accuracy: 0.2679\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9925 - accuracy: 0.6842 - val_loss: 2.6450 - val_accuracy: 0.2679\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9871 - accuracy: 0.6832 - val_loss: 2.6497 - val_accuracy: 0.2321\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9798 - accuracy: 0.6784 - val_loss: 2.7038 - val_accuracy: 0.2857\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9767 - accuracy: 0.6861 - val_loss: 2.7251 - val_accuracy: 0.2500\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9740 - accuracy: 0.6975 - val_loss: 2.7059 - val_accuracy: 0.3036\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9668 - accuracy: 0.6842 - val_loss: 2.7083 - val_accuracy: 0.2679\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9613 - accuracy: 0.6870 - val_loss: 2.7265 - val_accuracy: 0.2679\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9594 - accuracy: 0.6908 - val_loss: 2.7309 - val_accuracy: 0.2500\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9544 - accuracy: 0.6947 - val_loss: 2.7433 - val_accuracy: 0.2321\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9517 - accuracy: 0.6947 - val_loss: 2.7599 - val_accuracy: 0.2679\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9423 - accuracy: 0.6899 - val_loss: 2.8011 - val_accuracy: 0.2500\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9305 - accuracy: 0.7052 - val_loss: 2.7579 - val_accuracy: 0.2500\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9246 - accuracy: 0.7042 - val_loss: 2.8122 - val_accuracy: 0.2143\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9191 - accuracy: 0.7156 - val_loss: 2.8202 - val_accuracy: 0.2679\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9094 - accuracy: 0.7099 - val_loss: 2.8006 - val_accuracy: 0.2500\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.8536 - accuracy: 0.4094\n",
      "\n",
      "Test accuracy: 0.40942028164863586\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Data in x_d and y_d\n",
    "X = x_d\n",
    "y = y_d\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the ANN model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(14, activation='softmax')  # 14 output classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.05)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c715fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions = model.predict(X_test_scaled)\n",
    "# predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "# np.savetxt('test_predictions.csv', np.hstack((predicted_labels.reshape(-1, 1), y_test.reshape(-1,1))), delimiter=',', fmt='%d')\n",
    "# # np.savetxt('actual labels.csv', y_test.reshape(-1,1), delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157b35a",
   "metadata": {},
   "source": [
    "## Feature Selection PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9f8a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var: 0.99\n",
      "(1380, 12)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.5637 - accuracy: 0.1298 - val_loss: 2.5145 - val_accuracy: 0.1786\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3693 - accuracy: 0.2080 - val_loss: 2.4527 - val_accuracy: 0.1786\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2775 - accuracy: 0.2433 - val_loss: 2.3897 - val_accuracy: 0.1964\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1988 - accuracy: 0.2882 - val_loss: 2.3404 - val_accuracy: 0.2143\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1276 - accuracy: 0.2920 - val_loss: 2.2894 - val_accuracy: 0.2500\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0629 - accuracy: 0.3273 - val_loss: 2.2197 - val_accuracy: 0.2679\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0058 - accuracy: 0.3397 - val_loss: 2.1868 - val_accuracy: 0.2679\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9544 - accuracy: 0.3464 - val_loss: 2.1415 - val_accuracy: 0.2857\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9128 - accuracy: 0.3683 - val_loss: 2.1182 - val_accuracy: 0.2857\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8705 - accuracy: 0.3798 - val_loss: 2.1068 - val_accuracy: 0.2679\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8375 - accuracy: 0.4046 - val_loss: 2.0792 - val_accuracy: 0.2679\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8054 - accuracy: 0.3922 - val_loss: 2.0840 - val_accuracy: 0.2857\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7777 - accuracy: 0.4218 - val_loss: 2.0913 - val_accuracy: 0.3214\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7522 - accuracy: 0.4189 - val_loss: 2.0653 - val_accuracy: 0.2857\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7268 - accuracy: 0.4418 - val_loss: 2.0533 - val_accuracy: 0.3036\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7099 - accuracy: 0.4332 - val_loss: 2.0674 - val_accuracy: 0.2857\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6774 - accuracy: 0.4609 - val_loss: 2.0328 - val_accuracy: 0.3393\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6583 - accuracy: 0.4676 - val_loss: 2.0418 - val_accuracy: 0.3036\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6409 - accuracy: 0.4723 - val_loss: 2.0375 - val_accuracy: 0.3214\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6227 - accuracy: 0.4809 - val_loss: 2.0282 - val_accuracy: 0.3036\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6064 - accuracy: 0.4771 - val_loss: 2.0330 - val_accuracy: 0.3214\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5859 - accuracy: 0.4905 - val_loss: 2.0072 - val_accuracy: 0.3214\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5730 - accuracy: 0.4924 - val_loss: 2.0387 - val_accuracy: 0.3214\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5544 - accuracy: 0.4876 - val_loss: 2.0221 - val_accuracy: 0.3214\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5396 - accuracy: 0.5048 - val_loss: 2.0328 - val_accuracy: 0.3214\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5230 - accuracy: 0.5115 - val_loss: 2.0167 - val_accuracy: 0.3214\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5089 - accuracy: 0.5134 - val_loss: 1.9987 - val_accuracy: 0.3214\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4991 - accuracy: 0.5057 - val_loss: 2.0140 - val_accuracy: 0.3393\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4827 - accuracy: 0.5181 - val_loss: 2.0124 - val_accuracy: 0.3036\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4747 - accuracy: 0.5153 - val_loss: 2.0315 - val_accuracy: 0.3036\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4604 - accuracy: 0.5219 - val_loss: 1.9953 - val_accuracy: 0.3214\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4482 - accuracy: 0.5181 - val_loss: 1.9870 - val_accuracy: 0.3036\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4338 - accuracy: 0.5344 - val_loss: 1.9905 - val_accuracy: 0.3214\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4227 - accuracy: 0.5429 - val_loss: 1.9975 - val_accuracy: 0.3214\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4131 - accuracy: 0.5382 - val_loss: 2.0123 - val_accuracy: 0.3036\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3973 - accuracy: 0.5324 - val_loss: 1.9985 - val_accuracy: 0.3214\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3855 - accuracy: 0.5506 - val_loss: 2.0080 - val_accuracy: 0.3393\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3765 - accuracy: 0.5573 - val_loss: 1.9913 - val_accuracy: 0.3571\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3661 - accuracy: 0.5477 - val_loss: 2.0121 - val_accuracy: 0.3036\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3577 - accuracy: 0.5544 - val_loss: 1.9794 - val_accuracy: 0.3214\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3434 - accuracy: 0.5544 - val_loss: 2.0185 - val_accuracy: 0.3393\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3336 - accuracy: 0.5649 - val_loss: 2.0065 - val_accuracy: 0.3214\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3189 - accuracy: 0.5620 - val_loss: 1.9862 - val_accuracy: 0.3214\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3095 - accuracy: 0.5639 - val_loss: 1.9962 - val_accuracy: 0.3214\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3019 - accuracy: 0.5611 - val_loss: 1.9899 - val_accuracy: 0.3036\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2955 - accuracy: 0.5649 - val_loss: 1.9909 - val_accuracy: 0.3214\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2811 - accuracy: 0.5897 - val_loss: 2.0066 - val_accuracy: 0.3214\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2747 - accuracy: 0.5802 - val_loss: 1.9914 - val_accuracy: 0.3571\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2649 - accuracy: 0.5859 - val_loss: 1.9897 - val_accuracy: 0.3214\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2480 - accuracy: 0.5945 - val_loss: 2.0237 - val_accuracy: 0.3036\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2490 - accuracy: 0.5868 - val_loss: 2.0058 - val_accuracy: 0.3214\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2313 - accuracy: 0.5954 - val_loss: 1.9737 - val_accuracy: 0.3036\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2229 - accuracy: 0.5897 - val_loss: 1.9934 - val_accuracy: 0.3393\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2187 - accuracy: 0.5916 - val_loss: 1.9753 - val_accuracy: 0.3214\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2070 - accuracy: 0.6097 - val_loss: 1.9868 - val_accuracy: 0.3036\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1973 - accuracy: 0.5945 - val_loss: 2.0053 - val_accuracy: 0.3036\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1919 - accuracy: 0.6069 - val_loss: 2.0198 - val_accuracy: 0.3036\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1818 - accuracy: 0.6031 - val_loss: 1.9974 - val_accuracy: 0.3036\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1707 - accuracy: 0.6145 - val_loss: 2.0075 - val_accuracy: 0.3393\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1706 - accuracy: 0.6050 - val_loss: 2.0135 - val_accuracy: 0.3214\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1561 - accuracy: 0.6183 - val_loss: 1.9957 - val_accuracy: 0.3214\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1465 - accuracy: 0.6221 - val_loss: 2.0282 - val_accuracy: 0.3393\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1386 - accuracy: 0.6164 - val_loss: 2.0090 - val_accuracy: 0.3214\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1330 - accuracy: 0.6240 - val_loss: 2.0211 - val_accuracy: 0.3214\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1294 - accuracy: 0.6260 - val_loss: 1.9971 - val_accuracy: 0.3393\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1171 - accuracy: 0.6279 - val_loss: 2.0172 - val_accuracy: 0.3393\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1140 - accuracy: 0.6183 - val_loss: 2.0435 - val_accuracy: 0.3036\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1038 - accuracy: 0.6326 - val_loss: 2.0329 - val_accuracy: 0.3393\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0949 - accuracy: 0.6431 - val_loss: 2.0084 - val_accuracy: 0.3571\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0934 - accuracy: 0.6365 - val_loss: 2.0039 - val_accuracy: 0.3393\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0795 - accuracy: 0.6365 - val_loss: 2.0577 - val_accuracy: 0.2857\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0785 - accuracy: 0.6412 - val_loss: 2.0524 - val_accuracy: 0.3393\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0691 - accuracy: 0.6412 - val_loss: 2.0439 - val_accuracy: 0.3393\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0592 - accuracy: 0.6441 - val_loss: 2.0600 - val_accuracy: 0.3214\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0578 - accuracy: 0.6498 - val_loss: 2.0660 - val_accuracy: 0.3393\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0508 - accuracy: 0.6536 - val_loss: 2.0859 - val_accuracy: 0.3214\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0430 - accuracy: 0.6565 - val_loss: 2.0690 - val_accuracy: 0.3214\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0337 - accuracy: 0.6632 - val_loss: 2.0724 - val_accuracy: 0.3214\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0323 - accuracy: 0.6555 - val_loss: 2.0677 - val_accuracy: 0.3036\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0249 - accuracy: 0.6555 - val_loss: 2.0791 - val_accuracy: 0.3214\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0184 - accuracy: 0.6641 - val_loss: 2.0695 - val_accuracy: 0.3393\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0089 - accuracy: 0.6689 - val_loss: 2.1091 - val_accuracy: 0.3393\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0235 - accuracy: 0.6517 - val_loss: 2.0891 - val_accuracy: 0.3750\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0061 - accuracy: 0.6660 - val_loss: 2.1233 - val_accuracy: 0.3214\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9978 - accuracy: 0.6737 - val_loss: 2.1258 - val_accuracy: 0.3571\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9865 - accuracy: 0.6813 - val_loss: 2.0979 - val_accuracy: 0.3393\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9823 - accuracy: 0.6794 - val_loss: 2.1322 - val_accuracy: 0.3036\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9859 - accuracy: 0.6746 - val_loss: 2.1310 - val_accuracy: 0.3393\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9788 - accuracy: 0.6794 - val_loss: 2.1194 - val_accuracy: 0.3393\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9632 - accuracy: 0.6908 - val_loss: 2.1420 - val_accuracy: 0.3393\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9570 - accuracy: 0.6899 - val_loss: 2.1268 - val_accuracy: 0.3571\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9577 - accuracy: 0.6899 - val_loss: 2.1349 - val_accuracy: 0.3571\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9492 - accuracy: 0.6832 - val_loss: 2.1590 - val_accuracy: 0.3393\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9458 - accuracy: 0.7004 - val_loss: 2.1766 - val_accuracy: 0.3393\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9454 - accuracy: 0.6975 - val_loss: 2.1706 - val_accuracy: 0.3393\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9349 - accuracy: 0.7023 - val_loss: 2.2025 - val_accuracy: 0.3393\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9229 - accuracy: 0.7109 - val_loss: 2.1831 - val_accuracy: 0.3393\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9202 - accuracy: 0.7013 - val_loss: 2.1997 - val_accuracy: 0.3393\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9229 - accuracy: 0.7052 - val_loss: 2.1880 - val_accuracy: 0.3036\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9074 - accuracy: 0.7118 - val_loss: 2.2657 - val_accuracy: 0.2679\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2.8037 - accuracy: 0.4094\n",
      "\n",
      "Test accuracy: 0.40942028164863586\n",
      "var: 0.991\n",
      "(1380, 12)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.5542 - accuracy: 0.1355 - val_loss: 2.5011 - val_accuracy: 0.1786\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3526 - accuracy: 0.2118 - val_loss: 2.4088 - val_accuracy: 0.1964\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2600 - accuracy: 0.2500 - val_loss: 2.3521 - val_accuracy: 0.1964\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1790 - accuracy: 0.2786 - val_loss: 2.2802 - val_accuracy: 0.2500\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1075 - accuracy: 0.3073 - val_loss: 2.2436 - val_accuracy: 0.2857\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0486 - accuracy: 0.3254 - val_loss: 2.1884 - val_accuracy: 0.2500\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9969 - accuracy: 0.3454 - val_loss: 2.1750 - val_accuracy: 0.2500\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9583 - accuracy: 0.3540 - val_loss: 2.1452 - val_accuracy: 0.2679\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9219 - accuracy: 0.3550 - val_loss: 2.1246 - val_accuracy: 0.2321\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8955 - accuracy: 0.3683 - val_loss: 2.1191 - val_accuracy: 0.2679\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8633 - accuracy: 0.3836 - val_loss: 2.1265 - val_accuracy: 0.2679\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8365 - accuracy: 0.4017 - val_loss: 2.1244 - val_accuracy: 0.3214\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8091 - accuracy: 0.4065 - val_loss: 2.1305 - val_accuracy: 0.2857\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7883 - accuracy: 0.4113 - val_loss: 2.1166 - val_accuracy: 0.3036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7640 - accuracy: 0.4160 - val_loss: 2.1218 - val_accuracy: 0.2500\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7402 - accuracy: 0.4256 - val_loss: 2.1381 - val_accuracy: 0.2679\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7245 - accuracy: 0.4275 - val_loss: 2.1383 - val_accuracy: 0.2679\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7041 - accuracy: 0.4437 - val_loss: 2.1443 - val_accuracy: 0.2500\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6834 - accuracy: 0.4513 - val_loss: 2.1400 - val_accuracy: 0.3214\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6675 - accuracy: 0.4532 - val_loss: 2.1509 - val_accuracy: 0.2679\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6527 - accuracy: 0.4580 - val_loss: 2.1349 - val_accuracy: 0.2679\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6316 - accuracy: 0.4666 - val_loss: 2.1626 - val_accuracy: 0.2857\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6206 - accuracy: 0.4647 - val_loss: 2.1615 - val_accuracy: 0.2857\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6021 - accuracy: 0.4771 - val_loss: 2.1766 - val_accuracy: 0.3214\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5810 - accuracy: 0.4962 - val_loss: 2.1714 - val_accuracy: 0.3036\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5677 - accuracy: 0.4885 - val_loss: 2.1772 - val_accuracy: 0.3036\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5529 - accuracy: 0.5019 - val_loss: 2.1762 - val_accuracy: 0.3214\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5465 - accuracy: 0.5019 - val_loss: 2.1884 - val_accuracy: 0.2857\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5285 - accuracy: 0.5057 - val_loss: 2.2077 - val_accuracy: 0.3036\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5132 - accuracy: 0.5067 - val_loss: 2.2099 - val_accuracy: 0.3036\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4947 - accuracy: 0.5229 - val_loss: 2.2085 - val_accuracy: 0.2857\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4849 - accuracy: 0.5210 - val_loss: 2.2155 - val_accuracy: 0.2857\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4634 - accuracy: 0.5286 - val_loss: 2.2274 - val_accuracy: 0.3036\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4504 - accuracy: 0.5391 - val_loss: 2.2376 - val_accuracy: 0.2857\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4480 - accuracy: 0.5296 - val_loss: 2.2248 - val_accuracy: 0.3036\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4336 - accuracy: 0.5296 - val_loss: 2.2362 - val_accuracy: 0.2857\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4148 - accuracy: 0.5429 - val_loss: 2.2563 - val_accuracy: 0.2857\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3979 - accuracy: 0.5515 - val_loss: 2.2726 - val_accuracy: 0.2679\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3860 - accuracy: 0.5544 - val_loss: 2.2744 - val_accuracy: 0.2857\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3780 - accuracy: 0.5506 - val_loss: 2.2749 - val_accuracy: 0.2679\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3661 - accuracy: 0.5563 - val_loss: 2.3191 - val_accuracy: 0.2679\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3501 - accuracy: 0.5563 - val_loss: 2.3163 - val_accuracy: 0.2500\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3367 - accuracy: 0.5735 - val_loss: 2.3037 - val_accuracy: 0.2679\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3224 - accuracy: 0.5706 - val_loss: 2.3098 - val_accuracy: 0.2679\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3149 - accuracy: 0.5811 - val_loss: 2.3196 - val_accuracy: 0.2679\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3044 - accuracy: 0.5773 - val_loss: 2.3003 - val_accuracy: 0.2679\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2968 - accuracy: 0.5840 - val_loss: 2.3257 - val_accuracy: 0.2500\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2784 - accuracy: 0.5887 - val_loss: 2.3393 - val_accuracy: 0.2500\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2735 - accuracy: 0.6059 - val_loss: 2.3633 - val_accuracy: 0.2679\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2652 - accuracy: 0.5897 - val_loss: 2.3760 - val_accuracy: 0.2500\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2568 - accuracy: 0.6021 - val_loss: 2.3787 - val_accuracy: 0.2500\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2432 - accuracy: 0.6021 - val_loss: 2.3696 - val_accuracy: 0.2500\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2363 - accuracy: 0.6069 - val_loss: 2.3968 - val_accuracy: 0.2500\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2198 - accuracy: 0.6021 - val_loss: 2.3756 - val_accuracy: 0.2679\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2152 - accuracy: 0.6069 - val_loss: 2.3930 - val_accuracy: 0.2500\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2117 - accuracy: 0.6135 - val_loss: 2.4006 - val_accuracy: 0.2500\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1966 - accuracy: 0.6212 - val_loss: 2.4129 - val_accuracy: 0.2500\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1900 - accuracy: 0.6279 - val_loss: 2.4359 - val_accuracy: 0.2679\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1816 - accuracy: 0.6221 - val_loss: 2.4269 - val_accuracy: 0.2679\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1791 - accuracy: 0.6240 - val_loss: 2.4623 - val_accuracy: 0.2679\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1658 - accuracy: 0.6298 - val_loss: 2.4265 - val_accuracy: 0.2500\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1553 - accuracy: 0.6326 - val_loss: 2.4873 - val_accuracy: 0.2500\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1495 - accuracy: 0.6365 - val_loss: 2.4643 - val_accuracy: 0.2679\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1453 - accuracy: 0.6298 - val_loss: 2.4701 - val_accuracy: 0.2679\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1295 - accuracy: 0.6441 - val_loss: 2.4947 - val_accuracy: 0.2679\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1273 - accuracy: 0.6412 - val_loss: 2.4928 - val_accuracy: 0.2500\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1142 - accuracy: 0.6498 - val_loss: 2.5114 - val_accuracy: 0.2679\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1089 - accuracy: 0.6469 - val_loss: 2.5193 - val_accuracy: 0.2679\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1025 - accuracy: 0.6527 - val_loss: 2.5011 - val_accuracy: 0.2857\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0919 - accuracy: 0.6546 - val_loss: 2.5277 - val_accuracy: 0.2679\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0821 - accuracy: 0.6594 - val_loss: 2.5379 - val_accuracy: 0.2679\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0787 - accuracy: 0.6613 - val_loss: 2.5085 - val_accuracy: 0.2679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0706 - accuracy: 0.6679 - val_loss: 2.5454 - val_accuracy: 0.2679\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0635 - accuracy: 0.6660 - val_loss: 2.5522 - val_accuracy: 0.2500\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0592 - accuracy: 0.6698 - val_loss: 2.5550 - val_accuracy: 0.2857\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0473 - accuracy: 0.6746 - val_loss: 2.5684 - val_accuracy: 0.2857\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0438 - accuracy: 0.6651 - val_loss: 2.5883 - val_accuracy: 0.2679\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0376 - accuracy: 0.6775 - val_loss: 2.5939 - val_accuracy: 0.3036\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.0315 - accuracy: 0.6842 - val_loss: 2.6314 - val_accuracy: 0.2500\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0236 - accuracy: 0.6803 - val_loss: 2.6466 - val_accuracy: 0.2500\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0173 - accuracy: 0.6880 - val_loss: 2.6471 - val_accuracy: 0.2679\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0124 - accuracy: 0.6842 - val_loss: 2.6399 - val_accuracy: 0.2500\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0039 - accuracy: 0.6899 - val_loss: 2.6452 - val_accuracy: 0.2679\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0031 - accuracy: 0.6870 - val_loss: 2.6650 - val_accuracy: 0.2321\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9927 - accuracy: 0.6937 - val_loss: 2.6701 - val_accuracy: 0.2143\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9896 - accuracy: 0.6861 - val_loss: 2.6680 - val_accuracy: 0.2679\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9779 - accuracy: 0.6994 - val_loss: 2.6897 - val_accuracy: 0.2679\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9739 - accuracy: 0.6956 - val_loss: 2.7135 - val_accuracy: 0.2321\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9665 - accuracy: 0.7023 - val_loss: 2.7321 - val_accuracy: 0.2500\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9626 - accuracy: 0.7052 - val_loss: 2.7173 - val_accuracy: 0.2321\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9553 - accuracy: 0.7023 - val_loss: 2.7479 - val_accuracy: 0.2143\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9555 - accuracy: 0.7052 - val_loss: 2.7621 - val_accuracy: 0.2321\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9438 - accuracy: 0.7023 - val_loss: 2.7738 - val_accuracy: 0.2143\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9388 - accuracy: 0.6966 - val_loss: 2.7653 - val_accuracy: 0.2321\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9368 - accuracy: 0.7147 - val_loss: 2.7955 - val_accuracy: 0.2321\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9325 - accuracy: 0.7109 - val_loss: 2.8287 - val_accuracy: 0.2321\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9200 - accuracy: 0.7099 - val_loss: 2.8246 - val_accuracy: 0.2321\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9138 - accuracy: 0.7185 - val_loss: 2.8788 - val_accuracy: 0.2321\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9121 - accuracy: 0.7261 - val_loss: 2.8180 - val_accuracy: 0.2321\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9112 - accuracy: 0.7128 - val_loss: 2.8886 - val_accuracy: 0.2500\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.8294 - accuracy: 0.4203\n",
      "\n",
      "Test accuracy: 0.4202898442745209\n",
      "var: 0.992\n",
      "(1380, 13)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.6016 - accuracy: 0.1231 - val_loss: 2.4765 - val_accuracy: 0.1429\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3627 - accuracy: 0.2052 - val_loss: 2.3848 - val_accuracy: 0.1786\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2617 - accuracy: 0.2653 - val_loss: 2.3210 - val_accuracy: 0.2321\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1838 - accuracy: 0.2748 - val_loss: 2.2625 - val_accuracy: 0.2679\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1160 - accuracy: 0.2939 - val_loss: 2.2155 - val_accuracy: 0.2500\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0689 - accuracy: 0.3015 - val_loss: 2.1724 - val_accuracy: 0.2500\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0187 - accuracy: 0.3216 - val_loss: 2.1620 - val_accuracy: 0.2679\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9787 - accuracy: 0.3340 - val_loss: 2.1322 - val_accuracy: 0.2857\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9531 - accuracy: 0.3235 - val_loss: 2.1200 - val_accuracy: 0.2857\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9164 - accuracy: 0.3578 - val_loss: 2.1067 - val_accuracy: 0.3036\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8867 - accuracy: 0.3760 - val_loss: 2.0836 - val_accuracy: 0.3214\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8535 - accuracy: 0.3865 - val_loss: 2.0647 - val_accuracy: 0.3214\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8278 - accuracy: 0.3960 - val_loss: 2.0477 - val_accuracy: 0.3036\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7971 - accuracy: 0.3922 - val_loss: 2.0515 - val_accuracy: 0.3214\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7753 - accuracy: 0.4179 - val_loss: 2.0183 - val_accuracy: 0.3214\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7484 - accuracy: 0.4189 - val_loss: 2.0234 - val_accuracy: 0.2679\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7246 - accuracy: 0.4351 - val_loss: 2.0048 - val_accuracy: 0.3750\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7080 - accuracy: 0.4408 - val_loss: 1.9981 - val_accuracy: 0.3393\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6819 - accuracy: 0.4437 - val_loss: 1.9866 - val_accuracy: 0.3036\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6649 - accuracy: 0.4552 - val_loss: 1.9833 - val_accuracy: 0.3929\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6433 - accuracy: 0.4771 - val_loss: 1.9874 - val_accuracy: 0.3393\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6220 - accuracy: 0.4647 - val_loss: 1.9644 - val_accuracy: 0.3750\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6052 - accuracy: 0.4809 - val_loss: 1.9660 - val_accuracy: 0.3214\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5911 - accuracy: 0.4828 - val_loss: 1.9714 - val_accuracy: 0.3393\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5697 - accuracy: 0.4952 - val_loss: 1.9653 - val_accuracy: 0.3214\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5568 - accuracy: 0.4857 - val_loss: 1.9630 - val_accuracy: 0.3571\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5471 - accuracy: 0.5057 - val_loss: 1.9763 - val_accuracy: 0.3214\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5249 - accuracy: 0.5086 - val_loss: 1.9626 - val_accuracy: 0.3571\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5112 - accuracy: 0.5162 - val_loss: 1.9463 - val_accuracy: 0.3214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4969 - accuracy: 0.5210 - val_loss: 1.9565 - val_accuracy: 0.3393\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4871 - accuracy: 0.5115 - val_loss: 1.9789 - val_accuracy: 0.3036\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4715 - accuracy: 0.5296 - val_loss: 1.9713 - val_accuracy: 0.3393\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4618 - accuracy: 0.5210 - val_loss: 1.9776 - val_accuracy: 0.3214\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4487 - accuracy: 0.5305 - val_loss: 1.9754 - val_accuracy: 0.3393\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4445 - accuracy: 0.5353 - val_loss: 1.9848 - val_accuracy: 0.3214\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4255 - accuracy: 0.5315 - val_loss: 1.9892 - val_accuracy: 0.2857\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4144 - accuracy: 0.5525 - val_loss: 1.9780 - val_accuracy: 0.3036\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3960 - accuracy: 0.5573 - val_loss: 1.9812 - val_accuracy: 0.3214\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3840 - accuracy: 0.5525 - val_loss: 1.9923 - val_accuracy: 0.3036\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3768 - accuracy: 0.5620 - val_loss: 2.0186 - val_accuracy: 0.3036\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3663 - accuracy: 0.5592 - val_loss: 2.0004 - val_accuracy: 0.3214\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3526 - accuracy: 0.5687 - val_loss: 1.9885 - val_accuracy: 0.3036\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3407 - accuracy: 0.5706 - val_loss: 2.0321 - val_accuracy: 0.3036\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3274 - accuracy: 0.5792 - val_loss: 2.0020 - val_accuracy: 0.3571\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3216 - accuracy: 0.5706 - val_loss: 2.0489 - val_accuracy: 0.3214\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3148 - accuracy: 0.5754 - val_loss: 2.0260 - val_accuracy: 0.3214\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2986 - accuracy: 0.5916 - val_loss: 2.0247 - val_accuracy: 0.3036\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2870 - accuracy: 0.5906 - val_loss: 2.0269 - val_accuracy: 0.3393\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2801 - accuracy: 0.6002 - val_loss: 2.0241 - val_accuracy: 0.3571\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2645 - accuracy: 0.6040 - val_loss: 2.0383 - val_accuracy: 0.3214\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2561 - accuracy: 0.6002 - val_loss: 2.0470 - val_accuracy: 0.3750\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2533 - accuracy: 0.6021 - val_loss: 2.0576 - val_accuracy: 0.3393\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2416 - accuracy: 0.6002 - val_loss: 2.0575 - val_accuracy: 0.3571\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2335 - accuracy: 0.6050 - val_loss: 2.0565 - val_accuracy: 0.3036\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2192 - accuracy: 0.6164 - val_loss: 2.0372 - val_accuracy: 0.3214\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2192 - accuracy: 0.5983 - val_loss: 2.0567 - val_accuracy: 0.3750\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2037 - accuracy: 0.6164 - val_loss: 2.0539 - val_accuracy: 0.3571\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1999 - accuracy: 0.6193 - val_loss: 2.0335 - val_accuracy: 0.3750\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1843 - accuracy: 0.6279 - val_loss: 2.0505 - val_accuracy: 0.3571\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1743 - accuracy: 0.6221 - val_loss: 2.0603 - val_accuracy: 0.3571\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1701 - accuracy: 0.6355 - val_loss: 2.0449 - val_accuracy: 0.3750\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1623 - accuracy: 0.6269 - val_loss: 2.1196 - val_accuracy: 0.3393\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1506 - accuracy: 0.6317 - val_loss: 2.0780 - val_accuracy: 0.3750\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1404 - accuracy: 0.6317 - val_loss: 2.0760 - val_accuracy: 0.3393\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1310 - accuracy: 0.6393 - val_loss: 2.0863 - val_accuracy: 0.3750\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1294 - accuracy: 0.6384 - val_loss: 2.0652 - val_accuracy: 0.3750\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1271 - accuracy: 0.6345 - val_loss: 2.0744 - val_accuracy: 0.3929\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1125 - accuracy: 0.6384 - val_loss: 2.1150 - val_accuracy: 0.3393\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1024 - accuracy: 0.6336 - val_loss: 2.0892 - val_accuracy: 0.3929\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1003 - accuracy: 0.6460 - val_loss: 2.1326 - val_accuracy: 0.3571\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0859 - accuracy: 0.6565 - val_loss: 2.1297 - val_accuracy: 0.3393\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0796 - accuracy: 0.6555 - val_loss: 2.1684 - val_accuracy: 0.3571\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0707 - accuracy: 0.6517 - val_loss: 2.1158 - val_accuracy: 0.3929\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0691 - accuracy: 0.6527 - val_loss: 2.1501 - val_accuracy: 0.3571\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0556 - accuracy: 0.6565 - val_loss: 2.1312 - val_accuracy: 0.3571\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0480 - accuracy: 0.6613 - val_loss: 2.1641 - val_accuracy: 0.3750\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0473 - accuracy: 0.6546 - val_loss: 2.1129 - val_accuracy: 0.3929\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0397 - accuracy: 0.6603 - val_loss: 2.1224 - val_accuracy: 0.3750\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0321 - accuracy: 0.6641 - val_loss: 2.1468 - val_accuracy: 0.3571\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0293 - accuracy: 0.6594 - val_loss: 2.1752 - val_accuracy: 0.3393\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0200 - accuracy: 0.6651 - val_loss: 2.1486 - val_accuracy: 0.3571\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0154 - accuracy: 0.6689 - val_loss: 2.1673 - val_accuracy: 0.3571\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0079 - accuracy: 0.6746 - val_loss: 2.1786 - val_accuracy: 0.3750\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9938 - accuracy: 0.6813 - val_loss: 2.1919 - val_accuracy: 0.3393\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9887 - accuracy: 0.6908 - val_loss: 2.1861 - val_accuracy: 0.3750\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9910 - accuracy: 0.6813 - val_loss: 2.1851 - val_accuracy: 0.3571\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9874 - accuracy: 0.6813 - val_loss: 2.2406 - val_accuracy: 0.3571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9779 - accuracy: 0.6727 - val_loss: 2.2127 - val_accuracy: 0.3214\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9650 - accuracy: 0.6889 - val_loss: 2.2089 - val_accuracy: 0.3929\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9566 - accuracy: 0.6975 - val_loss: 2.2098 - val_accuracy: 0.3571\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9506 - accuracy: 0.6994 - val_loss: 2.2232 - val_accuracy: 0.3571\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9512 - accuracy: 0.6994 - val_loss: 2.2203 - val_accuracy: 0.3393\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9404 - accuracy: 0.7109 - val_loss: 2.2274 - val_accuracy: 0.3393\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9322 - accuracy: 0.7013 - val_loss: 2.1917 - val_accuracy: 0.3750\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9290 - accuracy: 0.7023 - val_loss: 2.2339 - val_accuracy: 0.3929\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9265 - accuracy: 0.7032 - val_loss: 2.2571 - val_accuracy: 0.3571\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9159 - accuracy: 0.7118 - val_loss: 2.2308 - val_accuracy: 0.3571\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9078 - accuracy: 0.7156 - val_loss: 2.2637 - val_accuracy: 0.3750\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9059 - accuracy: 0.7185 - val_loss: 2.2593 - val_accuracy: 0.3393\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9028 - accuracy: 0.7156 - val_loss: 2.2595 - val_accuracy: 0.3393\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2.8989 - accuracy: 0.3877\n",
      "\n",
      "Test accuracy: 0.38768115639686584\n",
      "var: 0.993\n",
      "(1380, 13)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 12ms/step - loss: 2.5666 - accuracy: 0.1431 - val_loss: 2.5325 - val_accuracy: 0.1429\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3265 - accuracy: 0.2261 - val_loss: 2.4306 - val_accuracy: 0.1607\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2245 - accuracy: 0.2567 - val_loss: 2.3788 - val_accuracy: 0.2143\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1595 - accuracy: 0.2719 - val_loss: 2.3248 - val_accuracy: 0.2143\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0980 - accuracy: 0.3082 - val_loss: 2.2712 - val_accuracy: 0.2857\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0404 - accuracy: 0.3368 - val_loss: 2.2400 - val_accuracy: 0.2679\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9907 - accuracy: 0.3511 - val_loss: 2.2104 - val_accuracy: 0.2857\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9535 - accuracy: 0.3740 - val_loss: 2.1973 - val_accuracy: 0.3036\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9114 - accuracy: 0.3884 - val_loss: 2.1792 - val_accuracy: 0.2857\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8768 - accuracy: 0.3979 - val_loss: 2.1756 - val_accuracy: 0.2500\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8459 - accuracy: 0.4055 - val_loss: 2.1707 - val_accuracy: 0.2857\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8184 - accuracy: 0.4141 - val_loss: 2.1738 - val_accuracy: 0.2679\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7864 - accuracy: 0.4246 - val_loss: 2.1851 - val_accuracy: 0.2679\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7640 - accuracy: 0.4351 - val_loss: 2.1657 - val_accuracy: 0.2500\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7338 - accuracy: 0.4504 - val_loss: 2.1848 - val_accuracy: 0.2679\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7136 - accuracy: 0.4561 - val_loss: 2.1818 - val_accuracy: 0.2679\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6882 - accuracy: 0.4504 - val_loss: 2.1736 - val_accuracy: 0.2679\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6657 - accuracy: 0.4685 - val_loss: 2.1985 - val_accuracy: 0.2679\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6451 - accuracy: 0.4761 - val_loss: 2.1652 - val_accuracy: 0.2679\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6282 - accuracy: 0.4809 - val_loss: 2.1905 - val_accuracy: 0.2679\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6132 - accuracy: 0.4857 - val_loss: 2.1870 - val_accuracy: 0.2679\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5878 - accuracy: 0.4885 - val_loss: 2.1828 - val_accuracy: 0.2679\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5642 - accuracy: 0.5057 - val_loss: 2.1980 - val_accuracy: 0.2857\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5479 - accuracy: 0.5038 - val_loss: 2.2040 - val_accuracy: 0.2857\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5318 - accuracy: 0.5105 - val_loss: 2.1968 - val_accuracy: 0.2857\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5172 - accuracy: 0.5134 - val_loss: 2.2117 - val_accuracy: 0.2857\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4988 - accuracy: 0.5200 - val_loss: 2.2007 - val_accuracy: 0.2857\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4845 - accuracy: 0.5172 - val_loss: 2.2157 - val_accuracy: 0.2679\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4696 - accuracy: 0.5239 - val_loss: 2.2045 - val_accuracy: 0.2857\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4517 - accuracy: 0.5372 - val_loss: 2.1985 - val_accuracy: 0.2857\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4404 - accuracy: 0.5344 - val_loss: 2.2211 - val_accuracy: 0.2857\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4307 - accuracy: 0.5372 - val_loss: 2.2128 - val_accuracy: 0.2857\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4140 - accuracy: 0.5506 - val_loss: 2.2393 - val_accuracy: 0.2857\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4024 - accuracy: 0.5420 - val_loss: 2.2592 - val_accuracy: 0.2679\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3897 - accuracy: 0.5496 - val_loss: 2.2458 - val_accuracy: 0.2857\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3770 - accuracy: 0.5573 - val_loss: 2.2495 - val_accuracy: 0.2857\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3604 - accuracy: 0.5620 - val_loss: 2.2343 - val_accuracy: 0.2679\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3487 - accuracy: 0.5697 - val_loss: 2.2160 - val_accuracy: 0.3036\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3450 - accuracy: 0.5639 - val_loss: 2.2468 - val_accuracy: 0.2857\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3302 - accuracy: 0.5792 - val_loss: 2.2567 - val_accuracy: 0.2857\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3172 - accuracy: 0.5821 - val_loss: 2.2369 - val_accuracy: 0.2857\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3006 - accuracy: 0.5859 - val_loss: 2.2408 - val_accuracy: 0.2857\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2934 - accuracy: 0.5954 - val_loss: 2.2409 - val_accuracy: 0.3214\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2851 - accuracy: 0.5992 - val_loss: 2.2613 - val_accuracy: 0.2679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2713 - accuracy: 0.6011 - val_loss: 2.2610 - val_accuracy: 0.2857\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2642 - accuracy: 0.6002 - val_loss: 2.2294 - val_accuracy: 0.3036\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2572 - accuracy: 0.5954 - val_loss: 2.2512 - val_accuracy: 0.2679\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2491 - accuracy: 0.6040 - val_loss: 2.2609 - val_accuracy: 0.2857\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2278 - accuracy: 0.6031 - val_loss: 2.2670 - val_accuracy: 0.2857\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2304 - accuracy: 0.6231 - val_loss: 2.2777 - val_accuracy: 0.3036\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2167 - accuracy: 0.6107 - val_loss: 2.2624 - val_accuracy: 0.2857\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2033 - accuracy: 0.6135 - val_loss: 2.2874 - val_accuracy: 0.2500\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1956 - accuracy: 0.6212 - val_loss: 2.2964 - val_accuracy: 0.3036\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1864 - accuracy: 0.6298 - val_loss: 2.2669 - val_accuracy: 0.2857\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1769 - accuracy: 0.6260 - val_loss: 2.2938 - val_accuracy: 0.3036\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1691 - accuracy: 0.6307 - val_loss: 2.2924 - val_accuracy: 0.2679\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1573 - accuracy: 0.6374 - val_loss: 2.3152 - val_accuracy: 0.2857\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1479 - accuracy: 0.6403 - val_loss: 2.2957 - val_accuracy: 0.3214\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1414 - accuracy: 0.6422 - val_loss: 2.3371 - val_accuracy: 0.2679\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1302 - accuracy: 0.6479 - val_loss: 2.3325 - val_accuracy: 0.2857\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1259 - accuracy: 0.6469 - val_loss: 2.3278 - val_accuracy: 0.2500\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1152 - accuracy: 0.6527 - val_loss: 2.3240 - val_accuracy: 0.2857\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1035 - accuracy: 0.6527 - val_loss: 2.3583 - val_accuracy: 0.2679\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0993 - accuracy: 0.6641 - val_loss: 2.3469 - val_accuracy: 0.2679\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0931 - accuracy: 0.6546 - val_loss: 2.3493 - val_accuracy: 0.2857\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0829 - accuracy: 0.6632 - val_loss: 2.3863 - val_accuracy: 0.3393\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0756 - accuracy: 0.6698 - val_loss: 2.4167 - val_accuracy: 0.2679\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0690 - accuracy: 0.6689 - val_loss: 2.3711 - val_accuracy: 0.3036\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0666 - accuracy: 0.6632 - val_loss: 2.4110 - val_accuracy: 0.2679\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0504 - accuracy: 0.6775 - val_loss: 2.3810 - val_accuracy: 0.2679\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0486 - accuracy: 0.6718 - val_loss: 2.4271 - val_accuracy: 0.2679\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0457 - accuracy: 0.6746 - val_loss: 2.3812 - val_accuracy: 0.2679\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0405 - accuracy: 0.6737 - val_loss: 2.4907 - val_accuracy: 0.2857\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0289 - accuracy: 0.6784 - val_loss: 2.4148 - val_accuracy: 0.2857\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0185 - accuracy: 0.6870 - val_loss: 2.4745 - val_accuracy: 0.3036\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0264 - accuracy: 0.6784 - val_loss: 2.4672 - val_accuracy: 0.2857\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0073 - accuracy: 0.6899 - val_loss: 2.4810 - val_accuracy: 0.2679\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0035 - accuracy: 0.6908 - val_loss: 2.5053 - val_accuracy: 0.2857\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9963 - accuracy: 0.6927 - val_loss: 2.4947 - val_accuracy: 0.3214\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9884 - accuracy: 0.6908 - val_loss: 2.5590 - val_accuracy: 0.2857\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9817 - accuracy: 0.6994 - val_loss: 2.5069 - val_accuracy: 0.2679\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9746 - accuracy: 0.6966 - val_loss: 2.5261 - val_accuracy: 0.2679\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9718 - accuracy: 0.6947 - val_loss: 2.5045 - val_accuracy: 0.2679\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9632 - accuracy: 0.6889 - val_loss: 2.5445 - val_accuracy: 0.2857\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9577 - accuracy: 0.6975 - val_loss: 2.5648 - val_accuracy: 0.2679\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9544 - accuracy: 0.6956 - val_loss: 2.5443 - val_accuracy: 0.3393\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9457 - accuracy: 0.7023 - val_loss: 2.5855 - val_accuracy: 0.2679\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9392 - accuracy: 0.6975 - val_loss: 2.6143 - val_accuracy: 0.2857\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9346 - accuracy: 0.7090 - val_loss: 2.5650 - val_accuracy: 0.2857\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9255 - accuracy: 0.7137 - val_loss: 2.6453 - val_accuracy: 0.2857\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9234 - accuracy: 0.7061 - val_loss: 2.6217 - val_accuracy: 0.2857\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9194 - accuracy: 0.7080 - val_loss: 2.6262 - val_accuracy: 0.3036\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9145 - accuracy: 0.7204 - val_loss: 2.6720 - val_accuracy: 0.2679\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9082 - accuracy: 0.7080 - val_loss: 2.6536 - val_accuracy: 0.2857\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9089 - accuracy: 0.7156 - val_loss: 2.6474 - val_accuracy: 0.3036\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9018 - accuracy: 0.7061 - val_loss: 2.6878 - val_accuracy: 0.2679\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8922 - accuracy: 0.7137 - val_loss: 2.6997 - val_accuracy: 0.2679\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8839 - accuracy: 0.7223 - val_loss: 2.7280 - val_accuracy: 0.2857\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8753 - accuracy: 0.7366 - val_loss: 2.6919 - val_accuracy: 0.2679\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.8727 - accuracy: 0.7271 - val_loss: 2.7870 - val_accuracy: 0.2857\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.9069 - accuracy: 0.4094\n",
      "\n",
      "Test accuracy: 0.40942028164863586\n",
      "var: 0.994\n",
      "(1380, 13)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.6179 - accuracy: 0.1288 - val_loss: 2.4560 - val_accuracy: 0.1786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3570 - accuracy: 0.2099 - val_loss: 2.3679 - val_accuracy: 0.1786\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2525 - accuracy: 0.2615 - val_loss: 2.3014 - val_accuracy: 0.2321\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1594 - accuracy: 0.3006 - val_loss: 2.2443 - val_accuracy: 0.2500\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0825 - accuracy: 0.3187 - val_loss: 2.1973 - val_accuracy: 0.3214\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0186 - accuracy: 0.3292 - val_loss: 2.1627 - val_accuracy: 0.3036\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9691 - accuracy: 0.3588 - val_loss: 2.1298 - val_accuracy: 0.2857\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9263 - accuracy: 0.3712 - val_loss: 2.1301 - val_accuracy: 0.2500\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8930 - accuracy: 0.3903 - val_loss: 2.0978 - val_accuracy: 0.2500\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8559 - accuracy: 0.3845 - val_loss: 2.0908 - val_accuracy: 0.2679\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8270 - accuracy: 0.3998 - val_loss: 2.0911 - val_accuracy: 0.2679\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8031 - accuracy: 0.4027 - val_loss: 2.0615 - val_accuracy: 0.3214\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7832 - accuracy: 0.4113 - val_loss: 2.0631 - val_accuracy: 0.3036\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7589 - accuracy: 0.4160 - val_loss: 2.0567 - val_accuracy: 0.3036\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7405 - accuracy: 0.4218 - val_loss: 2.0453 - val_accuracy: 0.3036\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7198 - accuracy: 0.4256 - val_loss: 2.0664 - val_accuracy: 0.3036\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6941 - accuracy: 0.4361 - val_loss: 2.0388 - val_accuracy: 0.3036\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6801 - accuracy: 0.4323 - val_loss: 2.0264 - val_accuracy: 0.3214\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.6653 - accuracy: 0.4456 - val_loss: 2.0164 - val_accuracy: 0.3036\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6513 - accuracy: 0.4466 - val_loss: 2.0166 - val_accuracy: 0.3036\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6274 - accuracy: 0.4571 - val_loss: 2.0240 - val_accuracy: 0.3036\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6123 - accuracy: 0.4637 - val_loss: 2.0249 - val_accuracy: 0.2857\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6000 - accuracy: 0.4695 - val_loss: 2.0146 - val_accuracy: 0.3036\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5826 - accuracy: 0.4771 - val_loss: 2.0202 - val_accuracy: 0.3393\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5754 - accuracy: 0.4695 - val_loss: 1.9829 - val_accuracy: 0.3214\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5534 - accuracy: 0.4809 - val_loss: 2.0042 - val_accuracy: 0.2857\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5401 - accuracy: 0.4924 - val_loss: 2.0204 - val_accuracy: 0.3214\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5278 - accuracy: 0.4885 - val_loss: 1.9977 - val_accuracy: 0.3214\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5091 - accuracy: 0.5010 - val_loss: 1.9842 - val_accuracy: 0.3393\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5017 - accuracy: 0.5010 - val_loss: 2.0122 - val_accuracy: 0.3214\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4908 - accuracy: 0.5048 - val_loss: 2.0233 - val_accuracy: 0.3214\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4817 - accuracy: 0.4971 - val_loss: 1.9997 - val_accuracy: 0.3393\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4655 - accuracy: 0.5095 - val_loss: 2.0158 - val_accuracy: 0.3571\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4577 - accuracy: 0.5153 - val_loss: 2.0056 - val_accuracy: 0.3214\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4457 - accuracy: 0.5219 - val_loss: 2.0224 - val_accuracy: 0.3214\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4335 - accuracy: 0.5219 - val_loss: 1.9931 - val_accuracy: 0.3393\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4233 - accuracy: 0.5277 - val_loss: 2.0046 - val_accuracy: 0.3036\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4108 - accuracy: 0.5353 - val_loss: 1.9852 - val_accuracy: 0.3393\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4032 - accuracy: 0.5334 - val_loss: 2.0003 - val_accuracy: 0.3214\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3903 - accuracy: 0.5353 - val_loss: 1.9878 - val_accuracy: 0.3393\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3822 - accuracy: 0.5344 - val_loss: 1.9807 - val_accuracy: 0.3393\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3715 - accuracy: 0.5420 - val_loss: 2.0088 - val_accuracy: 0.3571\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3634 - accuracy: 0.5534 - val_loss: 1.9847 - val_accuracy: 0.3036\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3525 - accuracy: 0.5525 - val_loss: 1.9826 - val_accuracy: 0.3393\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3455 - accuracy: 0.5601 - val_loss: 1.9877 - val_accuracy: 0.3036\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3289 - accuracy: 0.5630 - val_loss: 1.9963 - val_accuracy: 0.3214\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3259 - accuracy: 0.5677 - val_loss: 1.9815 - val_accuracy: 0.2857\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3166 - accuracy: 0.5630 - val_loss: 1.9490 - val_accuracy: 0.3571\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3074 - accuracy: 0.5697 - val_loss: 1.9888 - val_accuracy: 0.3214\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2964 - accuracy: 0.5840 - val_loss: 1.9864 - val_accuracy: 0.2857\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2864 - accuracy: 0.5821 - val_loss: 2.0112 - val_accuracy: 0.3214\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2803 - accuracy: 0.5878 - val_loss: 1.9752 - val_accuracy: 0.3036\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2728 - accuracy: 0.5849 - val_loss: 1.9601 - val_accuracy: 0.2857\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2643 - accuracy: 0.5935 - val_loss: 1.9833 - val_accuracy: 0.3036\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2525 - accuracy: 0.5916 - val_loss: 1.9831 - val_accuracy: 0.2500\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.2494 - accuracy: 0.5983 - val_loss: 1.9871 - val_accuracy: 0.2857\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2426 - accuracy: 0.5964 - val_loss: 1.9506 - val_accuracy: 0.2857\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2370 - accuracy: 0.6011 - val_loss: 1.9760 - val_accuracy: 0.2857\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2185 - accuracy: 0.6135 - val_loss: 1.9688 - val_accuracy: 0.2857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2149 - accuracy: 0.6145 - val_loss: 1.9733 - val_accuracy: 0.3214\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2061 - accuracy: 0.6183 - val_loss: 1.9387 - val_accuracy: 0.3214\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2048 - accuracy: 0.6135 - val_loss: 1.9769 - val_accuracy: 0.3036\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1919 - accuracy: 0.6279 - val_loss: 1.9696 - val_accuracy: 0.3036\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1869 - accuracy: 0.6193 - val_loss: 2.0351 - val_accuracy: 0.2857\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1850 - accuracy: 0.6174 - val_loss: 2.0009 - val_accuracy: 0.2857\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1751 - accuracy: 0.6317 - val_loss: 1.9967 - val_accuracy: 0.3036\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1655 - accuracy: 0.6298 - val_loss: 2.0192 - val_accuracy: 0.2500\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1574 - accuracy: 0.6345 - val_loss: 1.9941 - val_accuracy: 0.2679\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1400 - accuracy: 0.6393 - val_loss: 1.9952 - val_accuracy: 0.3214\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1421 - accuracy: 0.6393 - val_loss: 2.0170 - val_accuracy: 0.2679\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1378 - accuracy: 0.6441 - val_loss: 1.9785 - val_accuracy: 0.2679\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1324 - accuracy: 0.6479 - val_loss: 2.0035 - val_accuracy: 0.2500\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1226 - accuracy: 0.6517 - val_loss: 1.9996 - val_accuracy: 0.3214\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1106 - accuracy: 0.6479 - val_loss: 2.0050 - val_accuracy: 0.3036\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1129 - accuracy: 0.6469 - val_loss: 2.0200 - val_accuracy: 0.2857\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0985 - accuracy: 0.6613 - val_loss: 1.9827 - val_accuracy: 0.2857\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0921 - accuracy: 0.6584 - val_loss: 2.0105 - val_accuracy: 0.2679\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0871 - accuracy: 0.6613 - val_loss: 2.0257 - val_accuracy: 0.2679\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0746 - accuracy: 0.6584 - val_loss: 2.0326 - val_accuracy: 0.3036\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0766 - accuracy: 0.6594 - val_loss: 2.0193 - val_accuracy: 0.2679\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0733 - accuracy: 0.6613 - val_loss: 2.0354 - val_accuracy: 0.2679\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0589 - accuracy: 0.6746 - val_loss: 2.0107 - val_accuracy: 0.2857\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0615 - accuracy: 0.6679 - val_loss: 2.0347 - val_accuracy: 0.2500\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0482 - accuracy: 0.6708 - val_loss: 2.0356 - val_accuracy: 0.2857\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0385 - accuracy: 0.6727 - val_loss: 2.0387 - val_accuracy: 0.2679\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0360 - accuracy: 0.6708 - val_loss: 2.0433 - val_accuracy: 0.2679\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0354 - accuracy: 0.6756 - val_loss: 1.9996 - val_accuracy: 0.2679\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0261 - accuracy: 0.6737 - val_loss: 2.0141 - val_accuracy: 0.2500\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0259 - accuracy: 0.6689 - val_loss: 2.0250 - val_accuracy: 0.3036\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0168 - accuracy: 0.6679 - val_loss: 2.0944 - val_accuracy: 0.2679\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0160 - accuracy: 0.6784 - val_loss: 2.0478 - val_accuracy: 0.2679\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0087 - accuracy: 0.6737 - val_loss: 2.0301 - val_accuracy: 0.2500\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0089 - accuracy: 0.6746 - val_loss: 2.0539 - val_accuracy: 0.2679\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9947 - accuracy: 0.6756 - val_loss: 2.0562 - val_accuracy: 0.2679\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.9843 - accuracy: 0.6889 - val_loss: 2.0727 - val_accuracy: 0.2500\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9778 - accuracy: 0.6823 - val_loss: 2.0527 - val_accuracy: 0.2679\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9799 - accuracy: 0.6861 - val_loss: 2.0402 - val_accuracy: 0.3036\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.9715 - accuracy: 0.6947 - val_loss: 2.0541 - val_accuracy: 0.2679\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9692 - accuracy: 0.6823 - val_loss: 2.1071 - val_accuracy: 0.2857\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9648 - accuracy: 0.6918 - val_loss: 2.0682 - val_accuracy: 0.3036\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2.6406 - accuracy: 0.4094\n",
      "\n",
      "Test accuracy: 0.40942028164863586\n",
      "var: 0.995\n",
      "(1380, 14)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 1s 13ms/step - loss: 2.5694 - accuracy: 0.1517 - val_loss: 2.4938 - val_accuracy: 0.1786\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3655 - accuracy: 0.2385 - val_loss: 2.4165 - val_accuracy: 0.2500\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2683 - accuracy: 0.2758 - val_loss: 2.3524 - val_accuracy: 0.2500\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1863 - accuracy: 0.2834 - val_loss: 2.2921 - val_accuracy: 0.2679\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1183 - accuracy: 0.2977 - val_loss: 2.2458 - val_accuracy: 0.3036\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0598 - accuracy: 0.3149 - val_loss: 2.1912 - val_accuracy: 0.3214\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0064 - accuracy: 0.3378 - val_loss: 2.1683 - val_accuracy: 0.3214\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9590 - accuracy: 0.3445 - val_loss: 2.1334 - val_accuracy: 0.3036\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9153 - accuracy: 0.3693 - val_loss: 2.1081 - val_accuracy: 0.3571\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8817 - accuracy: 0.3721 - val_loss: 2.0876 - val_accuracy: 0.2679\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8531 - accuracy: 0.3969 - val_loss: 2.0785 - val_accuracy: 0.3393\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8212 - accuracy: 0.4008 - val_loss: 2.0640 - val_accuracy: 0.2857\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7936 - accuracy: 0.4122 - val_loss: 2.0589 - val_accuracy: 0.3036\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7713 - accuracy: 0.4227 - val_loss: 2.0571 - val_accuracy: 0.3036\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7511 - accuracy: 0.4256 - val_loss: 2.0527 - val_accuracy: 0.3036\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7277 - accuracy: 0.4380 - val_loss: 2.0745 - val_accuracy: 0.3393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7100 - accuracy: 0.4399 - val_loss: 2.0612 - val_accuracy: 0.3036\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6885 - accuracy: 0.4418 - val_loss: 2.0558 - val_accuracy: 0.3214\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6705 - accuracy: 0.4532 - val_loss: 2.0631 - val_accuracy: 0.3036\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6524 - accuracy: 0.4590 - val_loss: 2.0640 - val_accuracy: 0.2857\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6362 - accuracy: 0.4656 - val_loss: 2.0710 - val_accuracy: 0.3214\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6213 - accuracy: 0.4733 - val_loss: 2.0840 - val_accuracy: 0.2857\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5989 - accuracy: 0.4771 - val_loss: 2.0884 - val_accuracy: 0.3214\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5872 - accuracy: 0.4847 - val_loss: 2.0921 - val_accuracy: 0.2679\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5735 - accuracy: 0.4838 - val_loss: 2.0994 - val_accuracy: 0.3036\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5637 - accuracy: 0.4885 - val_loss: 2.0937 - val_accuracy: 0.2857\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5455 - accuracy: 0.5057 - val_loss: 2.0957 - val_accuracy: 0.2857\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5299 - accuracy: 0.5105 - val_loss: 2.1282 - val_accuracy: 0.2679\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5213 - accuracy: 0.5038 - val_loss: 2.1208 - val_accuracy: 0.2857\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5064 - accuracy: 0.5095 - val_loss: 2.1313 - val_accuracy: 0.2857\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4884 - accuracy: 0.5143 - val_loss: 2.1289 - val_accuracy: 0.2679\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4786 - accuracy: 0.5181 - val_loss: 2.1279 - val_accuracy: 0.2500\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4649 - accuracy: 0.5219 - val_loss: 2.1295 - val_accuracy: 0.2857\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4601 - accuracy: 0.5153 - val_loss: 2.1604 - val_accuracy: 0.2857\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4462 - accuracy: 0.5372 - val_loss: 2.1647 - val_accuracy: 0.2857\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4306 - accuracy: 0.5477 - val_loss: 2.1541 - val_accuracy: 0.2857\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4206 - accuracy: 0.5334 - val_loss: 2.1835 - val_accuracy: 0.2857\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4113 - accuracy: 0.5448 - val_loss: 2.1670 - val_accuracy: 0.3036\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4002 - accuracy: 0.5468 - val_loss: 2.1736 - val_accuracy: 0.2857\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3878 - accuracy: 0.5582 - val_loss: 2.1876 - val_accuracy: 0.3214\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3744 - accuracy: 0.5544 - val_loss: 2.2076 - val_accuracy: 0.2857\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3690 - accuracy: 0.5573 - val_loss: 2.2054 - val_accuracy: 0.3036\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3616 - accuracy: 0.5668 - val_loss: 2.2069 - val_accuracy: 0.2679\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.3503 - accuracy: 0.5658 - val_loss: 2.2309 - val_accuracy: 0.3036\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3399 - accuracy: 0.5802 - val_loss: 2.2137 - val_accuracy: 0.3214\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3279 - accuracy: 0.5782 - val_loss: 2.2352 - val_accuracy: 0.2500\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3193 - accuracy: 0.5859 - val_loss: 2.2145 - val_accuracy: 0.2679\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3052 - accuracy: 0.5802 - val_loss: 2.2538 - val_accuracy: 0.2143\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3046 - accuracy: 0.5849 - val_loss: 2.2472 - val_accuracy: 0.2857\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2887 - accuracy: 0.5859 - val_loss: 2.2325 - val_accuracy: 0.2857\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2820 - accuracy: 0.5878 - val_loss: 2.2585 - val_accuracy: 0.2679\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2684 - accuracy: 0.5887 - val_loss: 2.2699 - val_accuracy: 0.2857\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2650 - accuracy: 0.5859 - val_loss: 2.2661 - val_accuracy: 0.3214\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2558 - accuracy: 0.5935 - val_loss: 2.2451 - val_accuracy: 0.2500\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2505 - accuracy: 0.6031 - val_loss: 2.2625 - val_accuracy: 0.2679\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2363 - accuracy: 0.6107 - val_loss: 2.2917 - val_accuracy: 0.2857\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2231 - accuracy: 0.6078 - val_loss: 2.2691 - val_accuracy: 0.2857\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2114 - accuracy: 0.6145 - val_loss: 2.3039 - val_accuracy: 0.3036\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2112 - accuracy: 0.6097 - val_loss: 2.3171 - val_accuracy: 0.2500\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2019 - accuracy: 0.6145 - val_loss: 2.3083 - val_accuracy: 0.2857\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1889 - accuracy: 0.6212 - val_loss: 2.3316 - val_accuracy: 0.2679\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1869 - accuracy: 0.6174 - val_loss: 2.3102 - val_accuracy: 0.2857\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1803 - accuracy: 0.6193 - val_loss: 2.3081 - val_accuracy: 0.2679\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1682 - accuracy: 0.6231 - val_loss: 2.3272 - val_accuracy: 0.2857\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1557 - accuracy: 0.6250 - val_loss: 2.3330 - val_accuracy: 0.2857\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1433 - accuracy: 0.6279 - val_loss: 2.3347 - val_accuracy: 0.2500\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1399 - accuracy: 0.6307 - val_loss: 2.3010 - val_accuracy: 0.2857\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1276 - accuracy: 0.6460 - val_loss: 2.3314 - val_accuracy: 0.3214\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1302 - accuracy: 0.6384 - val_loss: 2.3359 - val_accuracy: 0.3036\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1196 - accuracy: 0.6460 - val_loss: 2.3319 - val_accuracy: 0.2857\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1079 - accuracy: 0.6450 - val_loss: 2.3126 - val_accuracy: 0.2857\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1025 - accuracy: 0.6422 - val_loss: 2.4002 - val_accuracy: 0.2500\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0954 - accuracy: 0.6460 - val_loss: 2.3137 - val_accuracy: 0.2679\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0822 - accuracy: 0.6555 - val_loss: 2.3567 - val_accuracy: 0.2679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0775 - accuracy: 0.6641 - val_loss: 2.3612 - val_accuracy: 0.3036\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0740 - accuracy: 0.6603 - val_loss: 2.3717 - val_accuracy: 0.2500\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0664 - accuracy: 0.6622 - val_loss: 2.3712 - val_accuracy: 0.2321\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0594 - accuracy: 0.6574 - val_loss: 2.3815 - val_accuracy: 0.2857\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0473 - accuracy: 0.6632 - val_loss: 2.3790 - val_accuracy: 0.2500\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0426 - accuracy: 0.6651 - val_loss: 2.4036 - val_accuracy: 0.2321\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0349 - accuracy: 0.6698 - val_loss: 2.4491 - val_accuracy: 0.2143\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0308 - accuracy: 0.6641 - val_loss: 2.4179 - val_accuracy: 0.2321\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0266 - accuracy: 0.6813 - val_loss: 2.4150 - val_accuracy: 0.2857\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0149 - accuracy: 0.6861 - val_loss: 2.4231 - val_accuracy: 0.2857\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0122 - accuracy: 0.6765 - val_loss: 2.4549 - val_accuracy: 0.2679\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0013 - accuracy: 0.6842 - val_loss: 2.4096 - val_accuracy: 0.2500\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9945 - accuracy: 0.6832 - val_loss: 2.3612 - val_accuracy: 0.2500\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9832 - accuracy: 0.6927 - val_loss: 2.4317 - val_accuracy: 0.2679\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9814 - accuracy: 0.7013 - val_loss: 2.4444 - val_accuracy: 0.2143\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9683 - accuracy: 0.6889 - val_loss: 2.4067 - val_accuracy: 0.2500\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9627 - accuracy: 0.7052 - val_loss: 2.4315 - val_accuracy: 0.2500\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9582 - accuracy: 0.6956 - val_loss: 2.4377 - val_accuracy: 0.2500\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9560 - accuracy: 0.7042 - val_loss: 2.4279 - val_accuracy: 0.2143\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9402 - accuracy: 0.7071 - val_loss: 2.4379 - val_accuracy: 0.2679\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9353 - accuracy: 0.7042 - val_loss: 2.4145 - val_accuracy: 0.2321\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9297 - accuracy: 0.7080 - val_loss: 2.4466 - val_accuracy: 0.2500\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9260 - accuracy: 0.7071 - val_loss: 2.4804 - val_accuracy: 0.2500\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9143 - accuracy: 0.7147 - val_loss: 2.4664 - val_accuracy: 0.2321\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9068 - accuracy: 0.7214 - val_loss: 2.4683 - val_accuracy: 0.3036\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9056 - accuracy: 0.7195 - val_loss: 2.4571 - val_accuracy: 0.2679\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.8730 - accuracy: 0.3877\n",
      "\n",
      "Test accuracy: 0.38768115639686584\n",
      "var: 0.996\n",
      "(1380, 14)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.6774 - accuracy: 0.1221 - val_loss: 2.4286 - val_accuracy: 0.1964\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3929 - accuracy: 0.1899 - val_loss: 2.3610 - val_accuracy: 0.1429\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2767 - accuracy: 0.2290 - val_loss: 2.3105 - val_accuracy: 0.1964\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1805 - accuracy: 0.2824 - val_loss: 2.2556 - val_accuracy: 0.2679\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1025 - accuracy: 0.3044 - val_loss: 2.2218 - val_accuracy: 0.2679\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0413 - accuracy: 0.3302 - val_loss: 2.1922 - val_accuracy: 0.2500\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9868 - accuracy: 0.3521 - val_loss: 2.1859 - val_accuracy: 0.2500\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9458 - accuracy: 0.3702 - val_loss: 2.1684 - val_accuracy: 0.2679\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9088 - accuracy: 0.3855 - val_loss: 2.1566 - val_accuracy: 0.3036\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8670 - accuracy: 0.3960 - val_loss: 2.1365 - val_accuracy: 0.3214\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8374 - accuracy: 0.4170 - val_loss: 2.1380 - val_accuracy: 0.3036\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8106 - accuracy: 0.4189 - val_loss: 2.1290 - val_accuracy: 0.3393\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7906 - accuracy: 0.4237 - val_loss: 2.1269 - val_accuracy: 0.3214\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7595 - accuracy: 0.4361 - val_loss: 2.1381 - val_accuracy: 0.3393\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7370 - accuracy: 0.4418 - val_loss: 2.1182 - val_accuracy: 0.3214\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7192 - accuracy: 0.4475 - val_loss: 2.1237 - val_accuracy: 0.3393\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6937 - accuracy: 0.4532 - val_loss: 2.1261 - val_accuracy: 0.3393\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6728 - accuracy: 0.4666 - val_loss: 2.1178 - val_accuracy: 0.3214\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6596 - accuracy: 0.4723 - val_loss: 2.1307 - val_accuracy: 0.3214\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6394 - accuracy: 0.4761 - val_loss: 2.1070 - val_accuracy: 0.3036\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6249 - accuracy: 0.4714 - val_loss: 2.1049 - val_accuracy: 0.3214\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6013 - accuracy: 0.4895 - val_loss: 2.1111 - val_accuracy: 0.2500\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5839 - accuracy: 0.4905 - val_loss: 2.0982 - val_accuracy: 0.3036\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5728 - accuracy: 0.4943 - val_loss: 2.1079 - val_accuracy: 0.2857\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5554 - accuracy: 0.5000 - val_loss: 2.0872 - val_accuracy: 0.2500\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5339 - accuracy: 0.5153 - val_loss: 2.0991 - val_accuracy: 0.2679\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5206 - accuracy: 0.5115 - val_loss: 2.0690 - val_accuracy: 0.3571\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5104 - accuracy: 0.5086 - val_loss: 2.0945 - val_accuracy: 0.2679\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4936 - accuracy: 0.5191 - val_loss: 2.1017 - val_accuracy: 0.2679\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4828 - accuracy: 0.5181 - val_loss: 2.0974 - val_accuracy: 0.2857\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4664 - accuracy: 0.5210 - val_loss: 2.1064 - val_accuracy: 0.3036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4529 - accuracy: 0.5401 - val_loss: 2.1014 - val_accuracy: 0.2679\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4416 - accuracy: 0.5372 - val_loss: 2.0910 - val_accuracy: 0.2679\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4376 - accuracy: 0.5353 - val_loss: 2.1091 - val_accuracy: 0.3036\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4204 - accuracy: 0.5305 - val_loss: 2.1179 - val_accuracy: 0.2679\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4035 - accuracy: 0.5487 - val_loss: 2.1384 - val_accuracy: 0.2857\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3999 - accuracy: 0.5534 - val_loss: 2.1211 - val_accuracy: 0.2500\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3805 - accuracy: 0.5563 - val_loss: 2.1082 - val_accuracy: 0.2679\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3668 - accuracy: 0.5563 - val_loss: 2.1122 - val_accuracy: 0.2857\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3545 - accuracy: 0.5620 - val_loss: 2.1783 - val_accuracy: 0.2679\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3419 - accuracy: 0.5754 - val_loss: 2.0964 - val_accuracy: 0.2500\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3343 - accuracy: 0.5735 - val_loss: 2.1363 - val_accuracy: 0.2857\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3218 - accuracy: 0.5763 - val_loss: 2.1259 - val_accuracy: 0.2679\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3058 - accuracy: 0.5878 - val_loss: 2.1265 - val_accuracy: 0.2679\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3063 - accuracy: 0.5897 - val_loss: 2.1347 - val_accuracy: 0.2500\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2898 - accuracy: 0.5926 - val_loss: 2.1338 - val_accuracy: 0.2679\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2756 - accuracy: 0.6011 - val_loss: 2.1327 - val_accuracy: 0.2857\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2692 - accuracy: 0.6059 - val_loss: 2.1195 - val_accuracy: 0.2857\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2562 - accuracy: 0.6078 - val_loss: 2.1702 - val_accuracy: 0.2679\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2500 - accuracy: 0.6078 - val_loss: 2.1499 - val_accuracy: 0.2679\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2399 - accuracy: 0.6183 - val_loss: 2.1630 - val_accuracy: 0.3036\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2338 - accuracy: 0.6059 - val_loss: 2.1901 - val_accuracy: 0.2857\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2225 - accuracy: 0.6174 - val_loss: 2.1641 - val_accuracy: 0.2679\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2238 - accuracy: 0.6088 - val_loss: 2.1720 - val_accuracy: 0.2500\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2064 - accuracy: 0.6202 - val_loss: 2.1922 - val_accuracy: 0.3036\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1979 - accuracy: 0.6212 - val_loss: 2.2189 - val_accuracy: 0.3036\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1881 - accuracy: 0.6240 - val_loss: 2.2024 - val_accuracy: 0.2857\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1805 - accuracy: 0.6269 - val_loss: 2.2197 - val_accuracy: 0.2857\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1770 - accuracy: 0.6288 - val_loss: 2.2012 - val_accuracy: 0.2500\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1663 - accuracy: 0.6355 - val_loss: 2.2116 - val_accuracy: 0.2857\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1546 - accuracy: 0.6345 - val_loss: 2.1900 - val_accuracy: 0.2857\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1448 - accuracy: 0.6365 - val_loss: 2.2090 - val_accuracy: 0.2679\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1469 - accuracy: 0.6422 - val_loss: 2.2376 - val_accuracy: 0.2679\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1345 - accuracy: 0.6441 - val_loss: 2.2507 - val_accuracy: 0.2857\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1225 - accuracy: 0.6613 - val_loss: 2.2519 - val_accuracy: 0.2857\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1183 - accuracy: 0.6393 - val_loss: 2.2499 - val_accuracy: 0.2679\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1083 - accuracy: 0.6450 - val_loss: 2.2689 - val_accuracy: 0.2679\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1080 - accuracy: 0.6460 - val_loss: 2.2181 - val_accuracy: 0.2679\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0972 - accuracy: 0.6708 - val_loss: 2.2753 - val_accuracy: 0.2500\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0913 - accuracy: 0.6555 - val_loss: 2.3073 - val_accuracy: 0.2500\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0850 - accuracy: 0.6574 - val_loss: 2.2787 - val_accuracy: 0.2679\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0783 - accuracy: 0.6613 - val_loss: 2.2912 - val_accuracy: 0.2500\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0727 - accuracy: 0.6698 - val_loss: 2.3309 - val_accuracy: 0.2857\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0709 - accuracy: 0.6689 - val_loss: 2.3159 - val_accuracy: 0.2500\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0651 - accuracy: 0.6679 - val_loss: 2.3203 - val_accuracy: 0.2500\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0545 - accuracy: 0.6651 - val_loss: 2.3320 - val_accuracy: 0.2679\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0504 - accuracy: 0.6737 - val_loss: 2.3000 - val_accuracy: 0.2679\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0444 - accuracy: 0.6689 - val_loss: 2.3477 - val_accuracy: 0.2679\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0326 - accuracy: 0.6870 - val_loss: 2.3320 - val_accuracy: 0.2679\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0332 - accuracy: 0.6727 - val_loss: 2.3365 - val_accuracy: 0.2679\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0242 - accuracy: 0.6698 - val_loss: 2.3667 - val_accuracy: 0.2679\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0262 - accuracy: 0.6784 - val_loss: 2.3677 - val_accuracy: 0.2679\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0187 - accuracy: 0.6756 - val_loss: 2.4260 - val_accuracy: 0.2500\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0150 - accuracy: 0.6784 - val_loss: 2.3806 - val_accuracy: 0.2679\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0025 - accuracy: 0.6889 - val_loss: 2.3795 - val_accuracy: 0.2679\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0029 - accuracy: 0.6718 - val_loss: 2.4461 - val_accuracy: 0.2857\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9887 - accuracy: 0.6918 - val_loss: 2.3969 - val_accuracy: 0.2679\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9846 - accuracy: 0.6842 - val_loss: 2.4332 - val_accuracy: 0.2679\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9740 - accuracy: 0.6899 - val_loss: 2.4837 - val_accuracy: 0.2857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9735 - accuracy: 0.7042 - val_loss: 2.4198 - val_accuracy: 0.2500\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9643 - accuracy: 0.6966 - val_loss: 2.4300 - val_accuracy: 0.2679\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9607 - accuracy: 0.6947 - val_loss: 2.4985 - val_accuracy: 0.2857\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9644 - accuracy: 0.6975 - val_loss: 2.4522 - val_accuracy: 0.2679\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9601 - accuracy: 0.6861 - val_loss: 2.5024 - val_accuracy: 0.2857\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9463 - accuracy: 0.7032 - val_loss: 2.4913 - val_accuracy: 0.2679\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9440 - accuracy: 0.7004 - val_loss: 2.5095 - val_accuracy: 0.2679\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9458 - accuracy: 0.7023 - val_loss: 2.5011 - val_accuracy: 0.2679\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9353 - accuracy: 0.6870 - val_loss: 2.5363 - val_accuracy: 0.2679\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9358 - accuracy: 0.7032 - val_loss: 2.5219 - val_accuracy: 0.2679\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9199 - accuracy: 0.7137 - val_loss: 2.5332 - val_accuracy: 0.2500\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3.0035 - accuracy: 0.4130\n",
      "\n",
      "Test accuracy: 0.41304346919059753\n",
      "var: 0.997\n",
      "(1380, 14)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 12ms/step - loss: 2.5748 - accuracy: 0.1269 - val_loss: 2.5571 - val_accuracy: 0.1250\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3272 - accuracy: 0.2204 - val_loss: 2.4453 - val_accuracy: 0.1607\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2367 - accuracy: 0.2586 - val_loss: 2.3719 - val_accuracy: 0.1964\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1645 - accuracy: 0.2834 - val_loss: 2.3046 - val_accuracy: 0.1786\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1107 - accuracy: 0.2891 - val_loss: 2.2545 - val_accuracy: 0.2679\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0574 - accuracy: 0.3206 - val_loss: 2.2088 - val_accuracy: 0.2857\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0113 - accuracy: 0.3244 - val_loss: 2.1638 - val_accuracy: 0.2857\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9758 - accuracy: 0.3550 - val_loss: 2.1496 - val_accuracy: 0.3214\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9329 - accuracy: 0.3683 - val_loss: 2.1403 - val_accuracy: 0.3036\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9000 - accuracy: 0.3845 - val_loss: 2.1151 - val_accuracy: 0.3393\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8678 - accuracy: 0.3874 - val_loss: 2.1149 - val_accuracy: 0.3036\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8344 - accuracy: 0.3865 - val_loss: 2.0999 - val_accuracy: 0.3036\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8070 - accuracy: 0.4160 - val_loss: 2.1053 - val_accuracy: 0.3214\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7816 - accuracy: 0.4246 - val_loss: 2.1090 - val_accuracy: 0.3036\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7623 - accuracy: 0.4332 - val_loss: 2.0976 - val_accuracy: 0.3214\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7389 - accuracy: 0.4389 - val_loss: 2.1223 - val_accuracy: 0.3393\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7200 - accuracy: 0.4475 - val_loss: 2.1047 - val_accuracy: 0.3214\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7009 - accuracy: 0.4475 - val_loss: 2.1126 - val_accuracy: 0.3214\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6781 - accuracy: 0.4542 - val_loss: 2.1322 - val_accuracy: 0.3214\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6596 - accuracy: 0.4685 - val_loss: 2.1236 - val_accuracy: 0.3036\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6405 - accuracy: 0.4781 - val_loss: 2.1076 - val_accuracy: 0.3214\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6222 - accuracy: 0.4876 - val_loss: 2.1206 - val_accuracy: 0.3036\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6087 - accuracy: 0.4866 - val_loss: 2.1072 - val_accuracy: 0.3214\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5979 - accuracy: 0.4943 - val_loss: 2.1315 - val_accuracy: 0.3036\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5790 - accuracy: 0.4952 - val_loss: 2.1222 - val_accuracy: 0.3214\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5662 - accuracy: 0.5105 - val_loss: 2.1180 - val_accuracy: 0.3214\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5536 - accuracy: 0.5105 - val_loss: 2.1467 - val_accuracy: 0.3036\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5333 - accuracy: 0.5057 - val_loss: 2.1293 - val_accuracy: 0.3036\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5212 - accuracy: 0.5124 - val_loss: 2.1314 - val_accuracy: 0.3393\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5113 - accuracy: 0.5134 - val_loss: 2.1475 - val_accuracy: 0.3214\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4994 - accuracy: 0.5153 - val_loss: 2.1425 - val_accuracy: 0.3036\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4899 - accuracy: 0.5191 - val_loss: 2.1533 - val_accuracy: 0.2857\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4712 - accuracy: 0.5191 - val_loss: 2.1336 - val_accuracy: 0.3036\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4597 - accuracy: 0.5267 - val_loss: 2.1261 - val_accuracy: 0.3214\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4495 - accuracy: 0.5344 - val_loss: 2.1637 - val_accuracy: 0.3214\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4356 - accuracy: 0.5258 - val_loss: 2.1558 - val_accuracy: 0.3214\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4190 - accuracy: 0.5391 - val_loss: 2.1588 - val_accuracy: 0.3214\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4184 - accuracy: 0.5372 - val_loss: 2.1721 - val_accuracy: 0.3214\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4000 - accuracy: 0.5401 - val_loss: 2.1785 - val_accuracy: 0.3036\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3912 - accuracy: 0.5458 - val_loss: 2.1909 - val_accuracy: 0.3036\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3843 - accuracy: 0.5506 - val_loss: 2.1294 - val_accuracy: 0.3214\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3715 - accuracy: 0.5429 - val_loss: 2.1532 - val_accuracy: 0.3036\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3603 - accuracy: 0.5544 - val_loss: 2.1281 - val_accuracy: 0.3036\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3493 - accuracy: 0.5668 - val_loss: 2.1430 - val_accuracy: 0.3036\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3316 - accuracy: 0.5553 - val_loss: 2.1529 - val_accuracy: 0.3036\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3230 - accuracy: 0.5649 - val_loss: 2.1576 - val_accuracy: 0.3393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3122 - accuracy: 0.5754 - val_loss: 2.1817 - val_accuracy: 0.3571\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3064 - accuracy: 0.5706 - val_loss: 2.1547 - val_accuracy: 0.3214\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3016 - accuracy: 0.5773 - val_loss: 2.1520 - val_accuracy: 0.3214\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2941 - accuracy: 0.5821 - val_loss: 2.1661 - val_accuracy: 0.3214\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2844 - accuracy: 0.5964 - val_loss: 2.1293 - val_accuracy: 0.3571\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2704 - accuracy: 0.5954 - val_loss: 2.1492 - val_accuracy: 0.3214\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2552 - accuracy: 0.5945 - val_loss: 2.1947 - val_accuracy: 0.3036\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2488 - accuracy: 0.5887 - val_loss: 2.1478 - val_accuracy: 0.3393\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2427 - accuracy: 0.6088 - val_loss: 2.1707 - val_accuracy: 0.3571\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2334 - accuracy: 0.5926 - val_loss: 2.1702 - val_accuracy: 0.3036\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2230 - accuracy: 0.6088 - val_loss: 2.2273 - val_accuracy: 0.3750\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2200 - accuracy: 0.6040 - val_loss: 2.1640 - val_accuracy: 0.3750\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2065 - accuracy: 0.6097 - val_loss: 2.1964 - val_accuracy: 0.3750\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2007 - accuracy: 0.6097 - val_loss: 2.1635 - val_accuracy: 0.3393\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1891 - accuracy: 0.6145 - val_loss: 2.1768 - val_accuracy: 0.3571\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1821 - accuracy: 0.6126 - val_loss: 2.1802 - val_accuracy: 0.3214\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1872 - accuracy: 0.6145 - val_loss: 2.1799 - val_accuracy: 0.3214\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1787 - accuracy: 0.6212 - val_loss: 2.1743 - val_accuracy: 0.3571\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1621 - accuracy: 0.6193 - val_loss: 2.2142 - val_accuracy: 0.3036\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1547 - accuracy: 0.6336 - val_loss: 2.1983 - val_accuracy: 0.3036\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1426 - accuracy: 0.6240 - val_loss: 2.2020 - val_accuracy: 0.3214\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1370 - accuracy: 0.6279 - val_loss: 2.2167 - val_accuracy: 0.3214\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1330 - accuracy: 0.6307 - val_loss: 2.2549 - val_accuracy: 0.2857\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1340 - accuracy: 0.6317 - val_loss: 2.2642 - val_accuracy: 0.3036\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1366 - accuracy: 0.6441 - val_loss: 2.2545 - val_accuracy: 0.3214\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1028 - accuracy: 0.6546 - val_loss: 2.2133 - val_accuracy: 0.2679\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0974 - accuracy: 0.6479 - val_loss: 2.2346 - val_accuracy: 0.3036\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0942 - accuracy: 0.6450 - val_loss: 2.2231 - val_accuracy: 0.3036\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0805 - accuracy: 0.6460 - val_loss: 2.2414 - val_accuracy: 0.2679\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0790 - accuracy: 0.6517 - val_loss: 2.2388 - val_accuracy: 0.3214\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0771 - accuracy: 0.6679 - val_loss: 2.2512 - val_accuracy: 0.2857\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0702 - accuracy: 0.6479 - val_loss: 2.2360 - val_accuracy: 0.3036\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0561 - accuracy: 0.6565 - val_loss: 2.2865 - val_accuracy: 0.3393\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0511 - accuracy: 0.6641 - val_loss: 2.2801 - val_accuracy: 0.2679\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0515 - accuracy: 0.6594 - val_loss: 2.2700 - val_accuracy: 0.3393\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0471 - accuracy: 0.6698 - val_loss: 2.2746 - val_accuracy: 0.3036\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0325 - accuracy: 0.6651 - val_loss: 2.2532 - val_accuracy: 0.2679\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0289 - accuracy: 0.6641 - val_loss: 2.2594 - val_accuracy: 0.2857\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0131 - accuracy: 0.6756 - val_loss: 2.2901 - val_accuracy: 0.3214\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0143 - accuracy: 0.6823 - val_loss: 2.2983 - val_accuracy: 0.3571\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0106 - accuracy: 0.6746 - val_loss: 2.2857 - val_accuracy: 0.3214\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0042 - accuracy: 0.6851 - val_loss: 2.3018 - val_accuracy: 0.3214\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9989 - accuracy: 0.6794 - val_loss: 2.2672 - val_accuracy: 0.3571\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0047 - accuracy: 0.6746 - val_loss: 2.3054 - val_accuracy: 0.3393\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9881 - accuracy: 0.6803 - val_loss: 2.3490 - val_accuracy: 0.3214\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9790 - accuracy: 0.6803 - val_loss: 2.3435 - val_accuracy: 0.3214\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9752 - accuracy: 0.6794 - val_loss: 2.3518 - val_accuracy: 0.2679\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9603 - accuracy: 0.6947 - val_loss: 2.3520 - val_accuracy: 0.3214\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9560 - accuracy: 0.6851 - val_loss: 2.3592 - val_accuracy: 0.2857\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9574 - accuracy: 0.6899 - val_loss: 2.3894 - val_accuracy: 0.3036\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9490 - accuracy: 0.6918 - val_loss: 2.3892 - val_accuracy: 0.2857\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9501 - accuracy: 0.6927 - val_loss: 2.3507 - val_accuracy: 0.3036\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9383 - accuracy: 0.7052 - val_loss: 2.3887 - val_accuracy: 0.3036\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9266 - accuracy: 0.7052 - val_loss: 2.3704 - val_accuracy: 0.3036\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2.8399 - accuracy: 0.4167\n",
      "\n",
      "Test accuracy: 0.4166666567325592\n",
      "var: 0.998\n",
      "(1380, 15)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.5920 - accuracy: 0.1240 - val_loss: 2.4386 - val_accuracy: 0.1786\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3829 - accuracy: 0.2071 - val_loss: 2.3675 - val_accuracy: 0.2679\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2767 - accuracy: 0.2719 - val_loss: 2.3152 - val_accuracy: 0.2321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1956 - accuracy: 0.2824 - val_loss: 2.2856 - val_accuracy: 0.2143\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1260 - accuracy: 0.3244 - val_loss: 2.2442 - val_accuracy: 0.3036\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0637 - accuracy: 0.3406 - val_loss: 2.2113 - val_accuracy: 0.2500\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0149 - accuracy: 0.3664 - val_loss: 2.2106 - val_accuracy: 0.3036\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9711 - accuracy: 0.3616 - val_loss: 2.1784 - val_accuracy: 0.3036\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9281 - accuracy: 0.4027 - val_loss: 2.1758 - val_accuracy: 0.3214\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8964 - accuracy: 0.3903 - val_loss: 2.1596 - val_accuracy: 0.3750\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8712 - accuracy: 0.4074 - val_loss: 2.1409 - val_accuracy: 0.3393\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8389 - accuracy: 0.4141 - val_loss: 2.1627 - val_accuracy: 0.3393\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8170 - accuracy: 0.4208 - val_loss: 2.1456 - val_accuracy: 0.3571\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7865 - accuracy: 0.4284 - val_loss: 2.1499 - val_accuracy: 0.3393\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7725 - accuracy: 0.4218 - val_loss: 2.1395 - val_accuracy: 0.3393\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7523 - accuracy: 0.4437 - val_loss: 2.1611 - val_accuracy: 0.3393\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7266 - accuracy: 0.4370 - val_loss: 2.1523 - val_accuracy: 0.3214\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7063 - accuracy: 0.4618 - val_loss: 2.1402 - val_accuracy: 0.3214\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6833 - accuracy: 0.4609 - val_loss: 2.1477 - val_accuracy: 0.3214\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6738 - accuracy: 0.4704 - val_loss: 2.1546 - val_accuracy: 0.3214\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6518 - accuracy: 0.4790 - val_loss: 2.1536 - val_accuracy: 0.3214\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6375 - accuracy: 0.4857 - val_loss: 2.1515 - val_accuracy: 0.3214\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6252 - accuracy: 0.4866 - val_loss: 2.1437 - val_accuracy: 0.2857\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6019 - accuracy: 0.4952 - val_loss: 2.1242 - val_accuracy: 0.3393\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5910 - accuracy: 0.5048 - val_loss: 2.1484 - val_accuracy: 0.3393\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5708 - accuracy: 0.5105 - val_loss: 2.1238 - val_accuracy: 0.3036\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5606 - accuracy: 0.5143 - val_loss: 2.1445 - val_accuracy: 0.3036\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5448 - accuracy: 0.5124 - val_loss: 2.1232 - val_accuracy: 0.3393\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5300 - accuracy: 0.5239 - val_loss: 2.1501 - val_accuracy: 0.3393\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5103 - accuracy: 0.5219 - val_loss: 2.1630 - val_accuracy: 0.3036\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5008 - accuracy: 0.5210 - val_loss: 2.1444 - val_accuracy: 0.3214\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4879 - accuracy: 0.5296 - val_loss: 2.1361 - val_accuracy: 0.3214\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4752 - accuracy: 0.5372 - val_loss: 2.1422 - val_accuracy: 0.2857\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4601 - accuracy: 0.5267 - val_loss: 2.1508 - val_accuracy: 0.3214\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4503 - accuracy: 0.5410 - val_loss: 2.1767 - val_accuracy: 0.2857\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4372 - accuracy: 0.5363 - val_loss: 2.1737 - val_accuracy: 0.2857\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4183 - accuracy: 0.5487 - val_loss: 2.1946 - val_accuracy: 0.2857\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4142 - accuracy: 0.5477 - val_loss: 2.1922 - val_accuracy: 0.3214\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3955 - accuracy: 0.5487 - val_loss: 2.2016 - val_accuracy: 0.2679\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3866 - accuracy: 0.5620 - val_loss: 2.1615 - val_accuracy: 0.2857\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3771 - accuracy: 0.5496 - val_loss: 2.1422 - val_accuracy: 0.2857\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3685 - accuracy: 0.5477 - val_loss: 2.1912 - val_accuracy: 0.2679\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3516 - accuracy: 0.5611 - val_loss: 2.1946 - val_accuracy: 0.2857\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3442 - accuracy: 0.5725 - val_loss: 2.1944 - val_accuracy: 0.2857\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3380 - accuracy: 0.5630 - val_loss: 2.1956 - val_accuracy: 0.3214\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3258 - accuracy: 0.5725 - val_loss: 2.1544 - val_accuracy: 0.3393\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3105 - accuracy: 0.5658 - val_loss: 2.2147 - val_accuracy: 0.3036\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3024 - accuracy: 0.5744 - val_loss: 2.2156 - val_accuracy: 0.3036\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3024 - accuracy: 0.5630 - val_loss: 2.2677 - val_accuracy: 0.2679\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3021 - accuracy: 0.5706 - val_loss: 2.2310 - val_accuracy: 0.2857\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2763 - accuracy: 0.5744 - val_loss: 2.2216 - val_accuracy: 0.3036\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2636 - accuracy: 0.5906 - val_loss: 2.2321 - val_accuracy: 0.2857\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2569 - accuracy: 0.5868 - val_loss: 2.2686 - val_accuracy: 0.2857\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2463 - accuracy: 0.5906 - val_loss: 2.2137 - val_accuracy: 0.3036\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2410 - accuracy: 0.5973 - val_loss: 2.2868 - val_accuracy: 0.2679\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2266 - accuracy: 0.5973 - val_loss: 2.2212 - val_accuracy: 0.3036\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2215 - accuracy: 0.5973 - val_loss: 2.2296 - val_accuracy: 0.3036\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2113 - accuracy: 0.5983 - val_loss: 2.2708 - val_accuracy: 0.3036\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2107 - accuracy: 0.5983 - val_loss: 2.2796 - val_accuracy: 0.2857\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1902 - accuracy: 0.6145 - val_loss: 2.2297 - val_accuracy: 0.2857\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1855 - accuracy: 0.6097 - val_loss: 2.2726 - val_accuracy: 0.3036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1745 - accuracy: 0.6212 - val_loss: 2.2674 - val_accuracy: 0.2857\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1674 - accuracy: 0.6135 - val_loss: 2.2841 - val_accuracy: 0.3036\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1743 - accuracy: 0.6069 - val_loss: 2.2893 - val_accuracy: 0.2857\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1488 - accuracy: 0.6240 - val_loss: 2.2894 - val_accuracy: 0.3036\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1438 - accuracy: 0.6279 - val_loss: 2.3473 - val_accuracy: 0.2679\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1351 - accuracy: 0.6384 - val_loss: 2.3154 - val_accuracy: 0.2857\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1323 - accuracy: 0.6298 - val_loss: 2.3334 - val_accuracy: 0.2857\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1239 - accuracy: 0.6298 - val_loss: 2.3228 - val_accuracy: 0.3036\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1135 - accuracy: 0.6374 - val_loss: 2.3708 - val_accuracy: 0.3036\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1066 - accuracy: 0.6307 - val_loss: 2.3843 - val_accuracy: 0.2857\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1045 - accuracy: 0.6231 - val_loss: 2.3360 - val_accuracy: 0.3036\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0977 - accuracy: 0.6317 - val_loss: 2.3853 - val_accuracy: 0.2857\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0971 - accuracy: 0.6307 - val_loss: 2.3534 - val_accuracy: 0.2857\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0770 - accuracy: 0.6517 - val_loss: 2.3591 - val_accuracy: 0.2857\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0668 - accuracy: 0.6393 - val_loss: 2.4304 - val_accuracy: 0.2857\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0651 - accuracy: 0.6498 - val_loss: 2.4000 - val_accuracy: 0.2857\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0577 - accuracy: 0.6460 - val_loss: 2.4037 - val_accuracy: 0.2857\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0541 - accuracy: 0.6536 - val_loss: 2.4297 - val_accuracy: 0.2857\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0486 - accuracy: 0.6489 - val_loss: 2.3881 - val_accuracy: 0.2857\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0369 - accuracy: 0.6679 - val_loss: 2.4530 - val_accuracy: 0.2500\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0313 - accuracy: 0.6632 - val_loss: 2.4237 - val_accuracy: 0.3214\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0272 - accuracy: 0.6622 - val_loss: 2.4337 - val_accuracy: 0.3036\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0197 - accuracy: 0.6555 - val_loss: 2.4737 - val_accuracy: 0.2679\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0129 - accuracy: 0.6708 - val_loss: 2.4630 - val_accuracy: 0.3036\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0068 - accuracy: 0.6679 - val_loss: 2.4871 - val_accuracy: 0.2679\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9998 - accuracy: 0.6737 - val_loss: 2.4731 - val_accuracy: 0.3393\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9919 - accuracy: 0.6813 - val_loss: 2.4913 - val_accuracy: 0.2857\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9877 - accuracy: 0.6718 - val_loss: 2.4768 - val_accuracy: 0.3214\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9809 - accuracy: 0.6803 - val_loss: 2.4252 - val_accuracy: 0.2679\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9770 - accuracy: 0.6794 - val_loss: 2.4844 - val_accuracy: 0.3214\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9751 - accuracy: 0.6775 - val_loss: 2.5088 - val_accuracy: 0.3214\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9694 - accuracy: 0.6746 - val_loss: 2.5971 - val_accuracy: 0.3036\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9639 - accuracy: 0.6842 - val_loss: 2.5479 - val_accuracy: 0.3393\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9777 - accuracy: 0.6784 - val_loss: 2.4915 - val_accuracy: 0.3214\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9553 - accuracy: 0.6851 - val_loss: 2.4718 - val_accuracy: 0.3393\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9500 - accuracy: 0.6823 - val_loss: 2.4859 - val_accuracy: 0.3393\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9473 - accuracy: 0.6870 - val_loss: 2.5166 - val_accuracy: 0.3571\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9377 - accuracy: 0.7004 - val_loss: 2.6096 - val_accuracy: 0.3214\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9270 - accuracy: 0.6947 - val_loss: 2.5684 - val_accuracy: 0.3036\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3.0051 - accuracy: 0.3949\n",
      "\n",
      "Test accuracy: 0.3949275314807892\n",
      "var: 0.999\n",
      "(1380, 16)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 16ms/step - loss: 2.6259 - accuracy: 0.1231 - val_loss: 2.4882 - val_accuracy: 0.1786\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3537 - accuracy: 0.2004 - val_loss: 2.4060 - val_accuracy: 0.2143\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2488 - accuracy: 0.2538 - val_loss: 2.3386 - val_accuracy: 0.2143\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1723 - accuracy: 0.2853 - val_loss: 2.2810 - val_accuracy: 0.2321\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1075 - accuracy: 0.3034 - val_loss: 2.2449 - val_accuracy: 0.2143\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0533 - accuracy: 0.3292 - val_loss: 2.2141 - val_accuracy: 0.2321\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9990 - accuracy: 0.3416 - val_loss: 2.1824 - val_accuracy: 0.2679\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9566 - accuracy: 0.3531 - val_loss: 2.1684 - val_accuracy: 0.2857\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9168 - accuracy: 0.3731 - val_loss: 2.1494 - val_accuracy: 0.2857\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8804 - accuracy: 0.3903 - val_loss: 2.1618 - val_accuracy: 0.2679\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8443 - accuracy: 0.3998 - val_loss: 2.1529 - val_accuracy: 0.2857\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8169 - accuracy: 0.3950 - val_loss: 2.1556 - val_accuracy: 0.2500\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7843 - accuracy: 0.4198 - val_loss: 2.1590 - val_accuracy: 0.2857\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7649 - accuracy: 0.4036 - val_loss: 2.1659 - val_accuracy: 0.2857\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7389 - accuracy: 0.4141 - val_loss: 2.1537 - val_accuracy: 0.2857\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7236 - accuracy: 0.4151 - val_loss: 2.1674 - val_accuracy: 0.2321\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6933 - accuracy: 0.4284 - val_loss: 2.1587 - val_accuracy: 0.2679\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6764 - accuracy: 0.4447 - val_loss: 2.1551 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1.6584 - accuracy: 0.4513 - val_loss: 2.1534 - val_accuracy: 0.2857\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6401 - accuracy: 0.4590 - val_loss: 2.1470 - val_accuracy: 0.2679\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6209 - accuracy: 0.4666 - val_loss: 2.1546 - val_accuracy: 0.2679\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6070 - accuracy: 0.4637 - val_loss: 2.1517 - val_accuracy: 0.2679\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5928 - accuracy: 0.4771 - val_loss: 2.1680 - val_accuracy: 0.2679\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5711 - accuracy: 0.4742 - val_loss: 2.1685 - val_accuracy: 0.2500\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5596 - accuracy: 0.4800 - val_loss: 2.1740 - val_accuracy: 0.2500\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5439 - accuracy: 0.4962 - val_loss: 2.1775 - val_accuracy: 0.2500\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5280 - accuracy: 0.4885 - val_loss: 2.1749 - val_accuracy: 0.2321\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5184 - accuracy: 0.4971 - val_loss: 2.1960 - val_accuracy: 0.2857\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5015 - accuracy: 0.5124 - val_loss: 2.1950 - val_accuracy: 0.2321\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4893 - accuracy: 0.5095 - val_loss: 2.2143 - val_accuracy: 0.2679\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4770 - accuracy: 0.5153 - val_loss: 2.1913 - val_accuracy: 0.2500\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4649 - accuracy: 0.5229 - val_loss: 2.2300 - val_accuracy: 0.2857\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4527 - accuracy: 0.5267 - val_loss: 2.2091 - val_accuracy: 0.2321\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4447 - accuracy: 0.5277 - val_loss: 2.1976 - val_accuracy: 0.2857\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4316 - accuracy: 0.5353 - val_loss: 2.2453 - val_accuracy: 0.2500\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4204 - accuracy: 0.5334 - val_loss: 2.1992 - val_accuracy: 0.2679\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4106 - accuracy: 0.5429 - val_loss: 2.2493 - val_accuracy: 0.2500\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3960 - accuracy: 0.5420 - val_loss: 2.2353 - val_accuracy: 0.2500\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3928 - accuracy: 0.5477 - val_loss: 2.2196 - val_accuracy: 0.2500\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3739 - accuracy: 0.5544 - val_loss: 2.2404 - val_accuracy: 0.2679\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3687 - accuracy: 0.5611 - val_loss: 2.2348 - val_accuracy: 0.2857\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3574 - accuracy: 0.5534 - val_loss: 2.2400 - val_accuracy: 0.2500\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3460 - accuracy: 0.5620 - val_loss: 2.2483 - val_accuracy: 0.2857\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3428 - accuracy: 0.5630 - val_loss: 2.2554 - val_accuracy: 0.2679\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3317 - accuracy: 0.5697 - val_loss: 2.2554 - val_accuracy: 0.2679\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3244 - accuracy: 0.5697 - val_loss: 2.2511 - val_accuracy: 0.2143\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3145 - accuracy: 0.5763 - val_loss: 2.2612 - val_accuracy: 0.2857\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3027 - accuracy: 0.5782 - val_loss: 2.2258 - val_accuracy: 0.2679\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2907 - accuracy: 0.5849 - val_loss: 2.2651 - val_accuracy: 0.2679\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2879 - accuracy: 0.5878 - val_loss: 2.2528 - val_accuracy: 0.2321\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2854 - accuracy: 0.5830 - val_loss: 2.2759 - val_accuracy: 0.2500\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2728 - accuracy: 0.5935 - val_loss: 2.2455 - val_accuracy: 0.2679\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2614 - accuracy: 0.5945 - val_loss: 2.2711 - val_accuracy: 0.2500\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2536 - accuracy: 0.5916 - val_loss: 2.2511 - val_accuracy: 0.2500\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2513 - accuracy: 0.6031 - val_loss: 2.2564 - val_accuracy: 0.2500\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2428 - accuracy: 0.6059 - val_loss: 2.2787 - val_accuracy: 0.2500\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2320 - accuracy: 0.6078 - val_loss: 2.3003 - val_accuracy: 0.2500\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2297 - accuracy: 0.6097 - val_loss: 2.2932 - val_accuracy: 0.2857\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2160 - accuracy: 0.6107 - val_loss: 2.2758 - val_accuracy: 0.2679\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2086 - accuracy: 0.6164 - val_loss: 2.2836 - val_accuracy: 0.2857\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2036 - accuracy: 0.6183 - val_loss: 2.2777 - val_accuracy: 0.3036\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1942 - accuracy: 0.6097 - val_loss: 2.2898 - val_accuracy: 0.2857\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1908 - accuracy: 0.6193 - val_loss: 2.2639 - val_accuracy: 0.2857\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1791 - accuracy: 0.6240 - val_loss: 2.2484 - val_accuracy: 0.2857\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1767 - accuracy: 0.6174 - val_loss: 2.2789 - val_accuracy: 0.2857\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1732 - accuracy: 0.6202 - val_loss: 2.2859 - val_accuracy: 0.2857\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1694 - accuracy: 0.6298 - val_loss: 2.3031 - val_accuracy: 0.2500\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1580 - accuracy: 0.6221 - val_loss: 2.2832 - val_accuracy: 0.2857\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1482 - accuracy: 0.6260 - val_loss: 2.2781 - val_accuracy: 0.2857\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1445 - accuracy: 0.6345 - val_loss: 2.3316 - val_accuracy: 0.2679\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1376 - accuracy: 0.6336 - val_loss: 2.2867 - val_accuracy: 0.2857\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.1321 - accuracy: 0.6384 - val_loss: 2.2722 - val_accuracy: 0.3214\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1271 - accuracy: 0.6384 - val_loss: 2.3302 - val_accuracy: 0.3036\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1171 - accuracy: 0.6508 - val_loss: 2.3146 - val_accuracy: 0.3036\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.1097 - accuracy: 0.6412 - val_loss: 2.3081 - val_accuracy: 0.3036\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0992 - accuracy: 0.6422 - val_loss: 2.3160 - val_accuracy: 0.3036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0988 - accuracy: 0.6527 - val_loss: 2.3341 - val_accuracy: 0.3036\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0951 - accuracy: 0.6460 - val_loss: 2.2992 - val_accuracy: 0.3036\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0844 - accuracy: 0.6546 - val_loss: 2.3358 - val_accuracy: 0.3036\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0817 - accuracy: 0.6565 - val_loss: 2.2989 - val_accuracy: 0.3036\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0801 - accuracy: 0.6403 - val_loss: 2.3180 - val_accuracy: 0.3036\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0775 - accuracy: 0.6517 - val_loss: 2.3356 - val_accuracy: 0.2857\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0667 - accuracy: 0.6613 - val_loss: 2.3669 - val_accuracy: 0.3214\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0603 - accuracy: 0.6670 - val_loss: 2.3310 - val_accuracy: 0.2857\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0565 - accuracy: 0.6660 - val_loss: 2.3386 - val_accuracy: 0.3036\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0487 - accuracy: 0.6651 - val_loss: 2.3491 - val_accuracy: 0.3036\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0410 - accuracy: 0.6651 - val_loss: 2.3308 - val_accuracy: 0.3214\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0317 - accuracy: 0.6670 - val_loss: 2.3703 - val_accuracy: 0.3036\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0305 - accuracy: 0.6756 - val_loss: 2.3344 - val_accuracy: 0.2857\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0215 - accuracy: 0.6746 - val_loss: 2.3428 - val_accuracy: 0.2857\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0162 - accuracy: 0.6718 - val_loss: 2.3677 - val_accuracy: 0.3036\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0154 - accuracy: 0.6784 - val_loss: 2.3521 - val_accuracy: 0.3036\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0120 - accuracy: 0.6641 - val_loss: 2.3510 - val_accuracy: 0.3393\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.0004 - accuracy: 0.6842 - val_loss: 2.3774 - val_accuracy: 0.3036\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9907 - accuracy: 0.6908 - val_loss: 2.3835 - val_accuracy: 0.3214\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9889 - accuracy: 0.6765 - val_loss: 2.4090 - val_accuracy: 0.3214\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9807 - accuracy: 0.6861 - val_loss: 2.3627 - val_accuracy: 0.3036\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9871 - accuracy: 0.6813 - val_loss: 2.4342 - val_accuracy: 0.3036\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9742 - accuracy: 0.6870 - val_loss: 2.4143 - val_accuracy: 0.3036\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.9727 - accuracy: 0.6832 - val_loss: 2.4010 - val_accuracy: 0.3214\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.8287 - accuracy: 0.4203\n",
      "\n",
      "Test accuracy: 0.4202898442745209\n",
      "Best Accuracy: 0.4202898442745209\n",
      "Trimmed x_d with Best Selected Features:\n",
      "(1380, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_pca_for_feature_selection(X, desired_variance_ratio):\n",
    "    pca = PCA(n_components=desired_variance_ratio)\n",
    "    reduced_features = pca.fit_transform(X)\n",
    "    return reduced_features, pca\n",
    "\n",
    "def pca(X, y, test_size=0.2, random_state=42):\n",
    "\n",
    "    variance_ratios = [i / 1000 for i in range(990, 1000)]\n",
    "    max_accuracy = 0.0\n",
    "    best_selected_features = None\n",
    "    best_pca_model = None\n",
    "\n",
    "    for desired_variance_ratio in variance_ratios:\n",
    "        reduced_features, pca_model = apply_pca_for_feature_selection(X, desired_variance_ratio)\n",
    "        print(f\"var: {desired_variance_ratio}\")\n",
    "        print(np.shape(reduced_features))\n",
    "\n",
    "        # Split the dataset into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Build the ANN model\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            keras.layers.Dense(32, activation='relu'),\n",
    "            keras.layers.Dense(14, activation='softmax')  # 14 output classes\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.05)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
    "        print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "        # Update max accuracy and store corresponding features\n",
    "        if test_acc > max_accuracy:\n",
    "            max_accuracy = test_acc\n",
    "            best_selected_features = reduced_features.copy()\n",
    "            best_pca_model = pca_model\n",
    "\n",
    "    # Apply inverse_transform to the full dataset with the best selected features\n",
    "    trimmed_x_d = best_pca_model.inverse_transform(best_selected_features)\n",
    "\n",
    "    return best_selected_features, max_accuracy\n",
    "\n",
    "# Example usage:\n",
    "reduced_x_d, max_accuracy = pca(x_d, y_d)\n",
    "print(f\"Best Accuracy: {max_accuracy}\")\n",
    "print(\"Trimmed x_d with Best Selected Features:\")\n",
    "print(np.shape(reduced_x_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bcf09",
   "metadata": {},
   "source": [
    "## ANN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca19135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "activation_0 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.01, 0.1, 0.3, 0.5], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "X = reduced_x_d\n",
    "y = y_d\n",
    "\n",
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Flatten())\n",
    "\n",
    "        # Tune the number of layers.\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "            model.add(\n",
    "                layers.Dense(\n",
    "                    # Tune number of units separately.\n",
    "                    units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                    activation=hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\", \"sigmoid\"]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        model.add(layers.Dense(14, activation=\"softmax\"))\n",
    "\n",
    "        learning_rate = hp.Choice(\"learning_rate\", [0.001, 0.01, 0.1, 0.3, 0.5])\n",
    "#         alpha = hp.Choice(\"alpha\", [0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate), #, momentum=alpha\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32, 64, 128]),\n",
    "            **kwargs,\n",
    "        )\n",
    "    \n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Create an instance of HyperParameters\n",
    "hp = kt.HyperParameters()\n",
    "\n",
    "# Build the model with default hyperparameters\n",
    "my_hypermodel = MyHyperModel()\n",
    "my_model = my_hypermodel.build(hp)\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel=my_hypermodel,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=50,\n",
    "    factor=2,\n",
    "    directory=\"Parameter Tuning\",\n",
    "    project_name=\"14 Classes - Legendre\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96f1d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 186 Complete [00h 00m 12s]\n",
      "val_accuracy: 0.1071428582072258\n",
      "\n",
      "Best val_accuracy So Far: 0.4642857015132904\n",
      "Total elapsed time: 00h 11m 33s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.05, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02db1ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in Parameter Tuning\\14 Classes - Legendre\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0175 summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 352\n",
      "activation_0: tanh\n",
      "learning_rate: 0.5\n",
      "units_1: 224\n",
      "activation_1: relu\n",
      "batch_size: 64\n",
      "units_2: 480\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 25\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0169\n",
      "Score: 0.4642857015132904\n",
      "\n",
      "Trial 0149 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 224\n",
      "activation_0: relu\n",
      "learning_rate: 0.01\n",
      "units_1: 320\n",
      "activation_1: relu\n",
      "batch_size: 32\n",
      "units_2: 96\n",
      "activation_2: relu\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 25\n",
      "tuner/bracket: 3\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0145\n",
      "Score: 0.4464285671710968\n",
      "\n",
      "Trial 0013 summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 448\n",
      "activation_0: relu\n",
      "learning_rate: 0.1\n",
      "units_1: 480\n",
      "activation_1: sigmoid\n",
      "batch_size: 32\n",
      "units_2: 256\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 5\n",
      "tuner/round: 0\n",
      "Score: 0.4285714328289032\n",
      "\n",
      "Trial 0069 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 288\n",
      "activation_0: relu\n",
      "learning_rate: 0.1\n",
      "units_1: 256\n",
      "activation_1: relu\n",
      "batch_size: 128\n",
      "tuner/epochs: 13\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 5\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0058\n",
      "units_2: 448\n",
      "activation_2: sigmoid\n",
      "Score: 0.4285714328289032\n",
      "\n",
      "Trial 0074 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 32\n",
      "activation_0: tanh\n",
      "learning_rate: 0.001\n",
      "units_1: 288\n",
      "activation_1: relu\n",
      "batch_size: 16\n",
      "units_2: 64\n",
      "activation_2: tanh\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 5\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 0067\n",
      "Score: 0.4285714328289032\n",
      "\n",
      "Trial 0101 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 192\n",
      "activation_0: relu\n",
      "learning_rate: 0.001\n",
      "units_1: 192\n",
      "activation_1: tanh\n",
      "batch_size: 32\n",
      "units_2: 352\n",
      "activation_2: tanh\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 4\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0097\n",
      "Score: 0.4285714328289032\n",
      "\n",
      "Trial 0147 summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 512\n",
      "activation_0: tanh\n",
      "learning_rate: 0.3\n",
      "units_1: 192\n",
      "activation_1: tanh\n",
      "batch_size: 16\n",
      "units_2: 512\n",
      "activation_2: tanh\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0138\n",
      "Score: 0.4285714328289032\n",
      "\n",
      "Trial 0176 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 288\n",
      "activation_0: tanh\n",
      "learning_rate: 0.001\n",
      "units_1: 32\n",
      "activation_1: sigmoid\n",
      "batch_size: 16\n",
      "units_2: 160\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 25\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0173\n",
      "Score: 0.4285714328289032\n",
      "\n",
      "Trial 0103 summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 64\n",
      "activation_0: sigmoid\n",
      "learning_rate: 0.1\n",
      "units_1: 384\n",
      "activation_1: relu\n",
      "batch_size: 64\n",
      "units_2: 384\n",
      "activation_2: relu\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 4\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0090\n",
      "Score: 0.4107142984867096\n",
      "\n",
      "Trial 0118 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 192\n",
      "activation_0: relu\n",
      "learning_rate: 0.001\n",
      "units_1: 192\n",
      "activation_1: tanh\n",
      "batch_size: 32\n",
      "units_2: 352\n",
      "activation_2: tanh\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 4\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0112\n",
      "Score: 0.4107142984867096\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e38f980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "33/33 [==============================] - 1s 12ms/step - loss: 56.8431 - accuracy: 0.1775 - val_loss: 46.3153 - val_accuracy: 0.1429\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 40.4362 - accuracy: 0.2796 - val_loss: 43.4816 - val_accuracy: 0.1964\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 37.4663 - accuracy: 0.2824 - val_loss: 49.2966 - val_accuracy: 0.2857\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 37.6710 - accuracy: 0.3025 - val_loss: 51.3795 - val_accuracy: 0.2500\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 39.7096 - accuracy: 0.3225 - val_loss: 37.8293 - val_accuracy: 0.3214\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 33.2027 - accuracy: 0.3721 - val_loss: 37.9122 - val_accuracy: 0.1786\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 39.1246 - accuracy: 0.3788 - val_loss: 47.3200 - val_accuracy: 0.2857\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 33.6852 - accuracy: 0.3760 - val_loss: 48.8435 - val_accuracy: 0.3214\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 30.6360 - accuracy: 0.4303 - val_loss: 53.2593 - val_accuracy: 0.2500\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 28.1651 - accuracy: 0.4466 - val_loss: 43.3654 - val_accuracy: 0.3036\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 32.7086 - accuracy: 0.4580 - val_loss: 60.8168 - val_accuracy: 0.3036\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 25.0231 - accuracy: 0.4943 - val_loss: 33.3888 - val_accuracy: 0.3750\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 30.7073 - accuracy: 0.4771 - val_loss: 50.3465 - val_accuracy: 0.2143\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 31.7391 - accuracy: 0.4790 - val_loss: 62.9582 - val_accuracy: 0.2321\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 26.7889 - accuracy: 0.5305 - val_loss: 54.5462 - val_accuracy: 0.2321\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 27.2742 - accuracy: 0.5086 - val_loss: 68.9378 - val_accuracy: 0.1786\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 25.5855 - accuracy: 0.5134 - val_loss: 50.7591 - val_accuracy: 0.2500\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 25.7987 - accuracy: 0.5429 - val_loss: 55.4092 - val_accuracy: 0.3036\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 25.6778 - accuracy: 0.5391 - val_loss: 61.1034 - val_accuracy: 0.3214\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 22.8087 - accuracy: 0.5668 - val_loss: 66.6920 - val_accuracy: 0.1607\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 26.9711 - accuracy: 0.5391 - val_loss: 57.1580 - val_accuracy: 0.2679\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 27.9480 - accuracy: 0.5620 - val_loss: 70.4322 - val_accuracy: 0.3036\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 28.8100 - accuracy: 0.5553 - val_loss: 69.3425 - val_accuracy: 0.1964\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 23.1283 - accuracy: 0.6097 - val_loss: 62.5435 - val_accuracy: 0.3214\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 19.7477 - accuracy: 0.6155 - val_loss: 48.4323 - val_accuracy: 0.3393\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 18.0990 - accuracy: 0.6336 - val_loss: 48.5032 - val_accuracy: 0.3750\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 20.3237 - accuracy: 0.6250 - val_loss: 65.4513 - val_accuracy: 0.3036\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 23.1824 - accuracy: 0.6355 - val_loss: 70.5331 - val_accuracy: 0.3036\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 18.0601 - accuracy: 0.6403 - val_loss: 68.0268 - val_accuracy: 0.2679\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 22.1532 - accuracy: 0.6240 - val_loss: 70.7611 - val_accuracy: 0.2679\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 21.6561 - accuracy: 0.6489 - val_loss: 76.6328 - val_accuracy: 0.3214\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 21.0581 - accuracy: 0.6326 - val_loss: 60.0590 - val_accuracy: 0.3214\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 19.8286 - accuracy: 0.6479 - val_loss: 63.6547 - val_accuracy: 0.3214\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 26.9774 - accuracy: 0.6288 - val_loss: 64.7026 - val_accuracy: 0.2857\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 24.0319 - accuracy: 0.6164 - val_loss: 68.8961 - val_accuracy: 0.3393\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 22.8460 - accuracy: 0.6613 - val_loss: 78.5756 - val_accuracy: 0.2500\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 23.0524 - accuracy: 0.6441 - val_loss: 74.5191 - val_accuracy: 0.3036\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 19.1960 - accuracy: 0.6689 - val_loss: 69.3561 - val_accuracy: 0.3036\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 23.7312 - accuracy: 0.6269 - val_loss: 86.8179 - val_accuracy: 0.3571\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 25.9293 - accuracy: 0.6135 - val_loss: 83.8397 - val_accuracy: 0.2857\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 23.2027 - accuracy: 0.6660 - val_loss: 86.5492 - val_accuracy: 0.3036\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 21.6269 - accuracy: 0.6660 - val_loss: 80.8042 - val_accuracy: 0.2857\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 18.9592 - accuracy: 0.6622 - val_loss: 80.3496 - val_accuracy: 0.2679\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 24.8034 - accuracy: 0.6565 - val_loss: 81.8991 - val_accuracy: 0.2500\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 20.6838 - accuracy: 0.6727 - val_loss: 85.9399 - val_accuracy: 0.2500\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 19.0174 - accuracy: 0.6899 - val_loss: 73.3871 - val_accuracy: 0.3393\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 18.4832 - accuracy: 0.7004 - val_loss: 102.8539 - val_accuracy: 0.2679\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 19.4558 - accuracy: 0.7080 - val_loss: 79.8552 - val_accuracy: 0.3214\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 21.8422 - accuracy: 0.7023 - val_loss: 98.4458 - val_accuracy: 0.2500\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 17.1319 - accuracy: 0.7090 - val_loss: 64.3120 - val_accuracy: 0.3929\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 20.3822 - accuracy: 0.6899 - val_loss: 105.7736 - val_accuracy: 0.3036\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 23.8910 - accuracy: 0.6842 - val_loss: 105.6020 - val_accuracy: 0.3393\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 23.3650 - accuracy: 0.6823 - val_loss: 81.9717 - val_accuracy: 0.2679\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 19.9114 - accuracy: 0.6994 - val_loss: 100.9394 - val_accuracy: 0.3929\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 18.2377 - accuracy: 0.7328 - val_loss: 110.4743 - val_accuracy: 0.2857\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 20.4630 - accuracy: 0.7347 - val_loss: 108.2652 - val_accuracy: 0.3750\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 13.7966 - accuracy: 0.7548 - val_loss: 100.0522 - val_accuracy: 0.2857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 14.4701 - accuracy: 0.7586 - val_loss: 96.8441 - val_accuracy: 0.3036\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 16.8138 - accuracy: 0.7443 - val_loss: 107.0900 - val_accuracy: 0.3393\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 17.0408 - accuracy: 0.7605 - val_loss: 90.9613 - val_accuracy: 0.3571\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 14.1055 - accuracy: 0.7624 - val_loss: 101.4069 - val_accuracy: 0.2679\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 14.4735 - accuracy: 0.7700 - val_loss: 100.3633 - val_accuracy: 0.3036\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 15.6242 - accuracy: 0.7615 - val_loss: 114.0709 - val_accuracy: 0.2679\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 18.5433 - accuracy: 0.7204 - val_loss: 96.2310 - val_accuracy: 0.3036\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 18.5734 - accuracy: 0.7328 - val_loss: 105.9321 - val_accuracy: 0.2679\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 21.3902 - accuracy: 0.7185 - val_loss: 105.1488 - val_accuracy: 0.3571\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 18.2894 - accuracy: 0.7443 - val_loss: 96.7354 - val_accuracy: 0.3036\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 16.6120 - accuracy: 0.7500 - val_loss: 103.6088 - val_accuracy: 0.2857\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 17.2027 - accuracy: 0.7538 - val_loss: 103.3817 - val_accuracy: 0.3036\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 18.5750 - accuracy: 0.7405 - val_loss: 103.6949 - val_accuracy: 0.2500\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 21.7748 - accuracy: 0.7433 - val_loss: 115.2689 - val_accuracy: 0.3036\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 20.6862 - accuracy: 0.7567 - val_loss: 112.9829 - val_accuracy: 0.3929\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 15.2269 - accuracy: 0.7901 - val_loss: 105.6058 - val_accuracy: 0.2857\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 12.6149 - accuracy: 0.8015 - val_loss: 102.6117 - val_accuracy: 0.3214\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 14.0219 - accuracy: 0.7929 - val_loss: 89.1380 - val_accuracy: 0.3929\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 15.7525 - accuracy: 0.7767 - val_loss: 97.9124 - val_accuracy: 0.4464\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 18.0947 - accuracy: 0.7538 - val_loss: 103.7398 - val_accuracy: 0.2500\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 14.7965 - accuracy: 0.7672 - val_loss: 98.1253 - val_accuracy: 0.3929\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 17.1897 - accuracy: 0.7805 - val_loss: 131.8383 - val_accuracy: 0.3393\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 13.6550 - accuracy: 0.7910 - val_loss: 112.7077 - val_accuracy: 0.3571\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 15.6562 - accuracy: 0.7968 - val_loss: 121.8137 - val_accuracy: 0.3750\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 14.0448 - accuracy: 0.7805 - val_loss: 107.3071 - val_accuracy: 0.3393\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 12.6836 - accuracy: 0.8225 - val_loss: 127.6462 - val_accuracy: 0.3393\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 11.8985 - accuracy: 0.8092 - val_loss: 103.0020 - val_accuracy: 0.4286\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 14.5486 - accuracy: 0.7872 - val_loss: 113.4454 - val_accuracy: 0.3393\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 15.7405 - accuracy: 0.7882 - val_loss: 116.2601 - val_accuracy: 0.3929\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 15.1920 - accuracy: 0.7853 - val_loss: 102.0177 - val_accuracy: 0.3393\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 16.9568 - accuracy: 0.7719 - val_loss: 128.9850 - val_accuracy: 0.3214\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 17.2461 - accuracy: 0.7786 - val_loss: 108.8582 - val_accuracy: 0.3571\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 15.3866 - accuracy: 0.7910 - val_loss: 104.4032 - val_accuracy: 0.2679\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 16.1204 - accuracy: 0.8006 - val_loss: 115.2799 - val_accuracy: 0.3214\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 15.0302 - accuracy: 0.8015 - val_loss: 120.7450 - val_accuracy: 0.3036\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 14.5386 - accuracy: 0.7939 - val_loss: 98.3141 - val_accuracy: 0.4286\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 14.8029 - accuracy: 0.8111 - val_loss: 128.0594 - val_accuracy: 0.3036\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 15.1686 - accuracy: 0.7891 - val_loss: 117.7692 - val_accuracy: 0.3750\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 15.3561 - accuracy: 0.7987 - val_loss: 128.7127 - val_accuracy: 0.4107\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 14.7835 - accuracy: 0.7987 - val_loss: 115.7125 - val_accuracy: 0.3393\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 15.7490 - accuracy: 0.7987 - val_loss: 132.1017 - val_accuracy: 0.3036\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 26.2116 - accuracy: 0.7490 - val_loss: 129.5506 - val_accuracy: 0.3036\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 22.1904 - accuracy: 0.7529 - val_loss: 140.5677 - val_accuracy: 0.2679\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 156.4175 - accuracy: 0.3841\n",
      "[test loss, test accuracy]: [156.41749572753906, 0.3840579688549042]\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.05)\n",
    "\n",
    "# Check Result\n",
    "y_pred_a = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_a, axis=1)\n",
    "\n",
    "eval_result = model.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ded1ce49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.53      0.35        19\n",
      "           1       0.00      0.00      0.00        14\n",
      "           2       0.33      0.36      0.35        22\n",
      "           3       0.32      0.33      0.33        24\n",
      "           4       0.54      0.75      0.63        20\n",
      "           5       0.38      0.14      0.21        21\n",
      "           6       0.95      0.91      0.93        22\n",
      "           7       0.29      0.29      0.29        21\n",
      "           8       0.17      0.19      0.18        21\n",
      "           9       0.25      0.52      0.34        21\n",
      "          10       0.73      0.55      0.63        20\n",
      "          11       0.33      0.33      0.33        15\n",
      "          12       0.00      0.00      0.00        15\n",
      "          13       0.50      0.24      0.32        21\n",
      "\n",
      "    accuracy                           0.38       276\n",
      "   macro avg       0.36      0.37      0.35       276\n",
      "weighted avg       0.38      0.38      0.36       276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class_list = set(y_test)\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "# print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=class_list, yticklabels=class_list)\n",
    "\n",
    "for i in range(len(conf_matrix)):\n",
    "    for j in range(len(conf_matrix[0])):\n",
    "        if i == j:\n",
    "            plt.text(j + 0.5, i + 0.5, str(conf_matrix[i, j]), ha='center', va='center', color='white')\n",
    "        else:\n",
    "            plt.text(j + 0.5, i + 0.5, str(conf_matrix[i, j]), ha='center', va='center', color='black')\n",
    "            \n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d440e015",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt('test_predictions.csv', np.hstack((y_test.reshape(-1, 1), y_pred.reshape(-1,1))), delimiter=',', fmt='%d')\n",
    "np.savetxt('actual labels.csv', y_test.reshape(-1,1), delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ada0d7e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Mapping:\n",
      "{'bacterial_leaf_blight': 0, 'bacterial_leaf_streak': 1, 'bakanae': 2, 'brown_spot': 3, 'grassy_stunt_virus': 4, 'healthy_rice_plant': 5, 'narrow_brown_spot': 6, 'ragged_stunt_virus': 7, 'rice_blast': 8, 'rice_false_smut': 9, 'sheath_blight': 10, 'sheath_rot': 11, 'stem_rot': 12, 'tungro_virus': 13}\n"
     ]
    }
   ],
   "source": [
    "# Display the mapping between class names and encoded numbers\n",
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"\\nClass Mapping:\")\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3359008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
