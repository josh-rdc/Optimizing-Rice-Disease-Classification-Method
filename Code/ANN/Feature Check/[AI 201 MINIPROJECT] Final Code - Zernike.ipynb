{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64653a26-b164-4036-93b5-be0f7358a8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auxiliaries (Importing Datafile, Checking Time)\n",
    "import os \n",
    "import time\n",
    "\n",
    "# Calculations\n",
    "import numpy as np\n",
    "\n",
    "# For Importing Data Files\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# For Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.filters import threshold_multiotsu\n",
    "from skimage import color\n",
    "\n",
    "# For Image Pre-Processing\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979b03d-2172-4465-999b-1e65d282be22",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create dataframe\n",
    "\n",
    "3 columns\n",
    "1. Disease Classification\n",
    "2. Image Name\n",
    "3. Image RGB Values (RGB Converted from BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce4c625-de79-4861-b12a-56db5eb4203e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def natural_sort_key(s):\n",
    "    \"\"\"Key function for natural sorting.\"\"\"\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "def create_dataframe_from_folders(root_folder):\n",
    "    # Initialize empty lists to store data for each column\n",
    "    folder_names = []\n",
    "    photo_names = []\n",
    "    photos = []\n",
    "\n",
    "    # Traverse through the root folder and its subfolders\n",
    "    for folder_name in sorted(os.listdir(root_folder)):\n",
    "        folder_path = os.path.join(root_folder, folder_name)\n",
    "\n",
    "        # Check if the entry in the root folder is a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            for photo_name in sorted(os.listdir(folder_path), key=natural_sort_key):\n",
    "                # Photos are in common image formats (e.g., jpg, png)\n",
    "                if photo_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    photo_path = os.path.join(folder_path, photo_name)\n",
    "\n",
    "                    # Read image data using cv2\n",
    "                    image_data = cv2.imread(photo_path)\n",
    "\n",
    "                    # Convert BGR to RGB\n",
    "                    image_data_rgb = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # Append data to the lists\n",
    "                    folder_names.append(folder_name)\n",
    "                    photo_names.append(photo_name)\n",
    "                    photos.append(image_data_rgb)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'class': folder_names,\n",
    "        'img_name': photo_names,\n",
    "        'rgb': photos\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af7bde-ac77-45fa-8c61-7a53363109b4",
   "metadata": {},
   "source": [
    "Get data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f698915-bb74-4ec7-8ff1-606c6471d686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   class                img_name  \\\n",
      "0  bacterial_leaf_blight  BLB_multi_leaf (1).jpg   \n",
      "1  bacterial_leaf_blight  BLB_multi_leaf (2).jpg   \n",
      "2  bacterial_leaf_blight  BLB_multi_leaf (3).jpg   \n",
      "3  bacterial_leaf_blight  BLB_multi_leaf (4).jpg   \n",
      "4  bacterial_leaf_blight  BLB_multi_leaf (5).jpg   \n",
      "\n",
      "                                                 rgb  \n",
      "0  [[[115, 152, 47], [78, 113, 9], [116, 147, 43]...  \n",
      "1  [[[198, 218, 97], [190, 210, 89], [189, 208, 9...  \n",
      "2  [[[137, 176, 51], [166, 203, 64], [165, 200, 3...  \n",
      "3  [[[192, 212, 143], [201, 222, 157], [187, 206,...  \n",
      "4  [[[132, 176, 27], [135, 180, 27], [161, 206, 4...  \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "extra_resized_folder_path = r\"C:\\Users\\Josh\\000 Files\\003 Mengg AI\\01a 201 (AI)\\03 Mini-Project\\resized_raw_images (14 Classes)\"\n",
    "extra_resized = create_dataframe_from_folders(extra_resized_folder_path)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(extra_resized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5426b9d-5fe5-470e-95cd-b9bd72e1875a",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Class Counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be65000-1849-48e3-b646-93efd2758afd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "bakanae                  100\n",
      "brown_spot               100\n",
      "grassy_stunt_virus       100\n",
      "healthy_rice_plant       100\n",
      "ragged_stunt_virus       100\n",
      "stem_rot                 100\n",
      "tungro_virus             100\n",
      "bacterial_leaf_streak     99\n",
      "rice_false_smut           99\n",
      "narrow_brown_spot         98\n",
      "rice_blast                98\n",
      "sheath_blight             98\n",
      "bacterial_leaf_blight     97\n",
      "sheath_rot                91\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_counts = extra_resized['class'].value_counts()\n",
    "\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac8225-3fe7-4259-a715-381055cfb61f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d99ab4-0a06-495a-8af2-2d2ca69d19a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Texture Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef73441b-106f-4a76-af4a-30d0192a58c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mahotas\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "def compute_glcm_features(df):\n",
    "    df['gray'] = df['rgb'].apply(lambda x: cv2.cvtColor(np.uint8(x), cv2.COLOR_RGB2GRAY))\n",
    "    def calculate_glcm(gray_image):\n",
    "        # Compute GLCM using mahotas\n",
    "        glcm = mahotas.features.haralick(gray_image.astype(np.uint8), ignore_zeros=True)\n",
    "        return glcm.mean(axis=0)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        gray_image = row['gray']\n",
    "        glcm_features = calculate_glcm(gray_image)\n",
    "        features.append(glcm_features)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Define meaningful column names for GLCM features\n",
    "    glcm_columns = [\n",
    "        'Angular Second Moment',\n",
    "        'Contrast',\n",
    "        'Correlation',\n",
    "        'Sum of Squares: Variance',\n",
    "        'Inverse Difference Moment',\n",
    "        'Sum Average',\n",
    "        'Sum Variance',\n",
    "        'Sum Entropy',\n",
    "        'Entropy',\n",
    "        'Difference Variance',\n",
    "        'Difference Entropy',\n",
    "        'Informational Measure of Correlation 1',\n",
    "        'Informational Measure of Correlation 2'\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame with the new GLCM features and meaningful column names\n",
    "    glcm_df = pd.DataFrame(features, columns=[f'GLCM_{col}' for col in glcm_columns])\n",
    "\n",
    "    # Concatenate the new DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([df, glcm_df], axis=1)\n",
    "\n",
    "    print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2b059a-fa17-4742-a4e9-61286361606a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 47.06 seconds\n"
     ]
    }
   ],
   "source": [
    "glcm = compute_glcm_features(extra_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564a00a9-a6ed-4d71-b79b-82ded4744d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove the first n columns\n",
    "f_glcm = glcm.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3555e8-efe3-4a70-8258-9bf9d11a2d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLCM_Angular Second Moment</th>\n",
       "      <th>GLCM_Contrast</th>\n",
       "      <th>GLCM_Correlation</th>\n",
       "      <th>GLCM_Sum of Squares: Variance</th>\n",
       "      <th>GLCM_Inverse Difference Moment</th>\n",
       "      <th>GLCM_Sum Average</th>\n",
       "      <th>GLCM_Sum Variance</th>\n",
       "      <th>GLCM_Sum Entropy</th>\n",
       "      <th>GLCM_Entropy</th>\n",
       "      <th>GLCM_Difference Variance</th>\n",
       "      <th>GLCM_Difference Entropy</th>\n",
       "      <th>GLCM_Informational Measure of Correlation 1</th>\n",
       "      <th>GLCM_Informational Measure of Correlation 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000093</td>\n",
       "      <td>1876.058313</td>\n",
       "      <td>0.693690</td>\n",
       "      <td>3062.090566</td>\n",
       "      <td>0.086892</td>\n",
       "      <td>281.006018</td>\n",
       "      <td>10372.303952</td>\n",
       "      <td>8.629042</td>\n",
       "      <td>14.243405</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>6.206143</td>\n",
       "      <td>-0.154215</td>\n",
       "      <td>0.950487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>1334.884861</td>\n",
       "      <td>0.797998</td>\n",
       "      <td>3303.508641</td>\n",
       "      <td>0.077155</td>\n",
       "      <td>276.277140</td>\n",
       "      <td>11879.149702</td>\n",
       "      <td>8.601060</td>\n",
       "      <td>14.082083</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>6.006272</td>\n",
       "      <td>-0.160813</td>\n",
       "      <td>0.952879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000140</td>\n",
       "      <td>1340.143870</td>\n",
       "      <td>0.744815</td>\n",
       "      <td>2625.859569</td>\n",
       "      <td>0.091972</td>\n",
       "      <td>293.873154</td>\n",
       "      <td>9163.294405</td>\n",
       "      <td>8.446153</td>\n",
       "      <td>13.764957</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>5.900897</td>\n",
       "      <td>-0.165284</td>\n",
       "      <td>0.954139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>1293.754229</td>\n",
       "      <td>0.776222</td>\n",
       "      <td>2890.458305</td>\n",
       "      <td>0.071097</td>\n",
       "      <td>289.332162</td>\n",
       "      <td>10268.078990</td>\n",
       "      <td>8.648174</td>\n",
       "      <td>14.226573</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>6.040155</td>\n",
       "      <td>-0.157107</td>\n",
       "      <td>0.952233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>1076.703851</td>\n",
       "      <td>0.693791</td>\n",
       "      <td>1757.932426</td>\n",
       "      <td>0.082002</td>\n",
       "      <td>284.620945</td>\n",
       "      <td>5955.025853</td>\n",
       "      <td>8.284213</td>\n",
       "      <td>13.795743</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>5.881877</td>\n",
       "      <td>-0.134893</td>\n",
       "      <td>0.923023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GLCM_Angular Second Moment  GLCM_Contrast  GLCM_Correlation  \\\n",
       "0                    0.000093    1876.058313          0.693690   \n",
       "1                    0.000090    1334.884861          0.797998   \n",
       "2                    0.000140    1340.143870          0.744815   \n",
       "3                    0.000071    1293.754229          0.776222   \n",
       "4                    0.000101    1076.703851          0.693791   \n",
       "\n",
       "   GLCM_Sum of Squares: Variance  GLCM_Inverse Difference Moment  \\\n",
       "0                    3062.090566                        0.086892   \n",
       "1                    3303.508641                        0.077155   \n",
       "2                    2625.859569                        0.091972   \n",
       "3                    2890.458305                        0.071097   \n",
       "4                    1757.932426                        0.082002   \n",
       "\n",
       "   GLCM_Sum Average  GLCM_Sum Variance  GLCM_Sum Entropy  GLCM_Entropy  \\\n",
       "0        281.006018       10372.303952          8.629042     14.243405   \n",
       "1        276.277140       11879.149702          8.601060     14.082083   \n",
       "2        293.873154        9163.294405          8.446153     13.764957   \n",
       "3        289.332162       10268.078990          8.648174     14.226573   \n",
       "4        284.620945        5955.025853          8.284213     13.795743   \n",
       "\n",
       "   GLCM_Difference Variance  GLCM_Difference Entropy  \\\n",
       "0                  0.000072                 6.206143   \n",
       "1                  0.000079                 6.006272   \n",
       "2                  0.000095                 5.900897   \n",
       "3                  0.000073                 6.040155   \n",
       "4                  0.000086                 5.881877   \n",
       "\n",
       "   GLCM_Informational Measure of Correlation 1  \\\n",
       "0                                    -0.154215   \n",
       "1                                    -0.160813   \n",
       "2                                    -0.165284   \n",
       "3                                    -0.157107   \n",
       "4                                    -0.134893   \n",
       "\n",
       "   GLCM_Informational Measure of Correlation 2  \n",
       "0                                     0.950487  \n",
       "1                                     0.952879  \n",
       "2                                     0.954139  \n",
       "3                                     0.952233  \n",
       "4                                     0.923023  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_glcm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21ad4b-7eb0-4110-bcb4-6e592e28d837",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Histogram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d94ca35f-9437-4b01-8009-1a0eed2ae870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import color\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "354c8451-629c-447d-beef-076db3311418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def values(dataframe):\n",
    "    # Ensure the 'rgb' column exists in the dataframe\n",
    "    dataframe['hsv'] = dataframe['rgb'].apply(lambda x: cv2.cvtColor(np.uint8(x), cv2.COLOR_RGB2HSV))\n",
    "    #dataframe['hsi'] = dataframe['rgb'].apply(lambda x: color.rgb2hsi(np.uint8(x)))\n",
    "    dataframe['lab'] = dataframe['rgb'].apply(lambda x: cv2.cvtColor(np.uint8(x), cv2.COLOR_RGB2LAB))\n",
    "\n",
    "    dataframe['red'] = dataframe['rgb'].apply(lambda rgb: rgb[:,:,0])\n",
    "    dataframe['green'] = dataframe['rgb'].apply(lambda rgb: rgb[:,:,1])\n",
    "    dataframe['blue'] = dataframe['rgb'].apply(lambda rgb: rgb[:,:,2])\n",
    "    \n",
    "    dataframe['hue_v'] = dataframe['hsv'].apply(lambda hsv: hsv[:,:,0])\n",
    "    dataframe['saturation_v'] = dataframe['hsv'].apply(lambda hsv: hsv[:,:,1])\n",
    "    dataframe['value_v'] = dataframe['hsv'].apply(lambda hsv: hsv[:,:,2])\n",
    "    \n",
    "    #dataframe['hue_i'] = dataframe['hsv'].apply(lambda hsi: hsi[:,:,0])\n",
    "    #dataframe['saturation_i'] = dataframe['hsv'].apply(lambda hsi: hsi[:,:,1])\n",
    "    #dataframe['intensity_i'] = dataframe['hsv'].apply(lambda hsi: hsi[:,:,2])\n",
    "    \n",
    "    dataframe['lightness'] = dataframe['lab'].apply(lambda lab: lab[:, :, 0])\n",
    "    dataframe['a'] = dataframe['lab'].apply(lambda lab: lab[:, :, 1])\n",
    "    dataframe['b'] = dataframe['lab'].apply(lambda lab: lab[:, :, 2])\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d5082c1-48a7-4a77-ae63-d276523f8173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_bins_to_dataframe(dataframe, column_name, num_bins):\n",
    "    # Extract the specified column\n",
    "    original_column = dataframe[column_name]\n",
    "\n",
    "    # Define bin edges based on the min and max values in the matrices\n",
    "    min_value = np.min([np.min(matrix) for matrix in original_column])\n",
    "    max_value = np.max([np.max(matrix) for matrix in original_column])\n",
    "\n",
    "    bin_edges = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "    # Create column names for the bins\n",
    "    bin_column_names = [f'{column_name}_{i}' for i in range(num_bins)]\n",
    "\n",
    "    # Iterate through each matrix in the original column\n",
    "    for i, matrix in enumerate(original_column):\n",
    "        # Digitize each element in the matrix into the corresponding bin\n",
    "        bin_indices = np.digitize(matrix.flatten(), bin_edges, right=True)\n",
    "\n",
    "        # Count the occurrences of each bin index\n",
    "        bin_counts = np.bincount(bin_indices, minlength=num_bins + 1)[1:]\n",
    "\n",
    "        # Add new columns to the DataFrame for each bin\n",
    "        for j, bin_column_name in enumerate(bin_column_names):\n",
    "            dataframe.loc[i, bin_column_name] = bin_counts[j]\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eeb1fba-0704-4f4b-b34a-2e2ef1cbd5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channels = values(extra_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1ab83c9-fc0e-4cff-9869-3fa8c12e4469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_bins_dataframe(dataframe, col, num_bins):\n",
    "    bins_dataframe = pd.DataFrame()\n",
    "\n",
    "    #print(col)\n",
    "    min_value = np.min([np.min(matrix) for matrix in dataframe[col]])\n",
    "    max_value = np.max([np.max(matrix) for matrix in dataframe[col]])\n",
    "\n",
    "    bin_edges = np.linspace(min_value, max_value, num_bins + 1)\n",
    "    bin_column_names = [f'{col}_{i}' for i in range(1, num_bins + 1)]\n",
    "\n",
    "    for matrix in dataframe[col]:\n",
    "        bin_indices = np.digitize(matrix.flatten(), bin_edges, right=True)\n",
    "        bin_counts = np.bincount(bin_indices, minlength=num_bins + 1)[1:]\n",
    "\n",
    "        bins_dataframe = pd.concat([bins_dataframe, pd.DataFrame(bin_counts).transpose()], axis=0, ignore_index=True)\n",
    "\n",
    "    bins_dataframe.columns = bin_column_names\n",
    "        \n",
    "    return bins_dataframe\n",
    "\n",
    "def hist_features(df, cols, bins):\n",
    "    complete_bins = pd.DataFrame()\n",
    "\n",
    "    for col in cols:\n",
    "        col_bins = create_bins_dataframe(df, col, bins)\n",
    "        complete_bins = pd.concat([complete_bins, col_bins], axis=1)\n",
    "\n",
    "    return complete_bins\n",
    "\n",
    "cols = ['red', 'green', 'blue', 'hue_v', 'saturation_v', 'value_v', 'lightness', 'a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ef1f4f2-7b29-4783-80bb-942502c3f276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_histogram = hist_features(channels, cols, 82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "200b7499-7dd8-4545-a930-7e28dd715d64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_1</th>\n",
       "      <th>red_2</th>\n",
       "      <th>red_3</th>\n",
       "      <th>red_4</th>\n",
       "      <th>red_5</th>\n",
       "      <th>red_6</th>\n",
       "      <th>red_7</th>\n",
       "      <th>red_8</th>\n",
       "      <th>red_9</th>\n",
       "      <th>red_10</th>\n",
       "      <th>...</th>\n",
       "      <th>b_73</th>\n",
       "      <th>b_74</th>\n",
       "      <th>b_75</th>\n",
       "      <th>b_76</th>\n",
       "      <th>b_77</th>\n",
       "      <th>b_78</th>\n",
       "      <th>b_79</th>\n",
       "      <th>b_80</th>\n",
       "      <th>b_81</th>\n",
       "      <th>b_82</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "      <td>93</td>\n",
       "      <td>143</td>\n",
       "      <td>223</td>\n",
       "      <td>231</td>\n",
       "      <td>305</td>\n",
       "      <td>314</td>\n",
       "      <td>330</td>\n",
       "      <td>432</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153</td>\n",
       "      <td>210</td>\n",
       "      <td>261</td>\n",
       "      <td>308</td>\n",
       "      <td>349</td>\n",
       "      <td>391</td>\n",
       "      <td>392</td>\n",
       "      <td>358</td>\n",
       "      <td>383</td>\n",
       "      <td>554</td>\n",
       "      <td>...</td>\n",
       "      <td>541</td>\n",
       "      <td>556</td>\n",
       "      <td>187</td>\n",
       "      <td>98</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>62</td>\n",
       "      <td>145</td>\n",
       "      <td>205</td>\n",
       "      <td>290</td>\n",
       "      <td>337</td>\n",
       "      <td>325</td>\n",
       "      <td>338</td>\n",
       "      <td>454</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>53</td>\n",
       "      <td>89</td>\n",
       "      <td>106</td>\n",
       "      <td>194</td>\n",
       "      <td>242</td>\n",
       "      <td>267</td>\n",
       "      <td>436</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>1417</td>\n",
       "      <td>1133</td>\n",
       "      <td>195</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 738 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   red_1  red_2  red_3  red_4  red_5  red_6  red_7  red_8  red_9  red_10  ...  \\\n",
       "0     19     47     93    143    223    231    305    314    330     432  ...   \n",
       "1    153    210    261    308    349    391    392    358    383     554  ...   \n",
       "2     14     35     62    145    205    290    337    325    338     454  ...   \n",
       "3     14     14     40     53     89    106    194    242    267     436  ...   \n",
       "4      0      1      3      4      1      6      6      9      8      31  ...   \n",
       "\n",
       "   b_73  b_74  b_75  b_76  b_77  b_78  b_79  b_80  b_81  b_82  \n",
       "0    73    27     0     0     0     0     0     0     0     0  \n",
       "1   541   556   187    98    47    33     0     0     0     0  \n",
       "2   157    51     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0     0  \n",
       "4  1417  1133   195    42     8     1     0     0     0     0  \n",
       "\n",
       "[5 rows x 738 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_histogram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58cb8f-65ac-44fc-99e6-58e42abfabfa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Color Moments Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8b0823e-3e5b-4f1c-95a6-8b8b4cc97e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import cv2\n",
    "\n",
    "def color_moments(dataframe, cols):\n",
    "    for col in cols:\n",
    "        print(col)\n",
    "        mean_values = []\n",
    "        variance_values = []\n",
    "        kurtosis_values = []\n",
    "        skewness_values = []\n",
    "\n",
    "        for img in dataframe[col]:\n",
    "            # 'img' is a 3D numpy array representing an RGB image\n",
    "            # Calculate statistics for each image\n",
    "            mean_channel = np.mean(img, axis=(0, 1))\n",
    "            variance_channel = np.var(img, axis=(0, 1))\n",
    "            kurtosis_channel = kurtosis(img, axis=(0, 1))\n",
    "            skewness_channel = skew(img, axis=(0, 1))\n",
    "\n",
    "            # Append values to respective lists\n",
    "            mean_values.append(mean_channel)\n",
    "            variance_values.append(variance_channel)\n",
    "            kurtosis_values.append(kurtosis_channel)\n",
    "            skewness_values.append(skewness_channel)\n",
    "\n",
    "        # Add the new columns to the DataFrame with channel names\n",
    "        for i in range(img.shape[2]):  # 'img' has shape (height, width, channels)\n",
    "            dataframe[f'{col}_channel_{i}_mean'] = [x[i] for x in mean_values]\n",
    "            dataframe[f'{col}_channel_{i}_variance'] = [x[i] for x in variance_values]\n",
    "            dataframe[f'{col}_channel_{i}_kurtosis'] = [x[i] for x in kurtosis_values]\n",
    "            dataframe[f'{col}_channel_{i}_skewness'] = [x[i] for x in skewness_values]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2cc9e6e-ddcb-41fe-aa4b-72e3f44afc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb\n",
      "hsv\n",
      "lab\n"
     ]
    }
   ],
   "source": [
    "color_df = color_moments(extra_resized, ['rgb','hsv','lab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c245d80-93b2-4cc8-82a3-77f40ab9b53b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_color = color_df.drop(columns=['class','img_name','rgb','gray','hsv','lab','red','green','blue', 'hue_v', 'saturation_v', 'value_v', 'lightness', 'a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b52a436e-88b8-43bc-b1d7-b17b428f0ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rgb_channel_0_mean</th>\n",
       "      <th>rgb_channel_0_variance</th>\n",
       "      <th>rgb_channel_0_kurtosis</th>\n",
       "      <th>rgb_channel_0_skewness</th>\n",
       "      <th>rgb_channel_1_mean</th>\n",
       "      <th>rgb_channel_1_variance</th>\n",
       "      <th>rgb_channel_1_kurtosis</th>\n",
       "      <th>rgb_channel_1_skewness</th>\n",
       "      <th>rgb_channel_2_mean</th>\n",
       "      <th>rgb_channel_2_variance</th>\n",
       "      <th>...</th>\n",
       "      <th>lab_channel_0_kurtosis</th>\n",
       "      <th>lab_channel_0_skewness</th>\n",
       "      <th>lab_channel_1_mean</th>\n",
       "      <th>lab_channel_1_variance</th>\n",
       "      <th>lab_channel_1_kurtosis</th>\n",
       "      <th>lab_channel_1_skewness</th>\n",
       "      <th>lab_channel_2_mean</th>\n",
       "      <th>lab_channel_2_variance</th>\n",
       "      <th>lab_channel_2_kurtosis</th>\n",
       "      <th>lab_channel_2_skewness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.110013</td>\n",
       "      <td>3078.218207</td>\n",
       "      <td>-0.689453</td>\n",
       "      <td>-0.192670</td>\n",
       "      <td>157.315011</td>\n",
       "      <td>3449.301597</td>\n",
       "      <td>-0.484632</td>\n",
       "      <td>-0.518085</td>\n",
       "      <td>67.831274</td>\n",
       "      <td>2087.782836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.296129</td>\n",
       "      <td>-0.606758</td>\n",
       "      <td>106.950853</td>\n",
       "      <td>61.335754</td>\n",
       "      <td>0.410354</td>\n",
       "      <td>0.917856</td>\n",
       "      <td>169.947226</td>\n",
       "      <td>203.966363</td>\n",
       "      <td>0.049806</td>\n",
       "      <td>-0.576146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132.208227</td>\n",
       "      <td>3961.755789</td>\n",
       "      <td>-1.130258</td>\n",
       "      <td>-0.302681</td>\n",
       "      <td>158.333586</td>\n",
       "      <td>3558.081163</td>\n",
       "      <td>-0.908913</td>\n",
       "      <td>-0.458059</td>\n",
       "      <td>50.368921</td>\n",
       "      <td>1453.704717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.781886</td>\n",
       "      <td>-0.534726</td>\n",
       "      <td>103.516083</td>\n",
       "      <td>21.999662</td>\n",
       "      <td>2.755215</td>\n",
       "      <td>1.327478</td>\n",
       "      <td>177.711097</td>\n",
       "      <td>198.029258</td>\n",
       "      <td>0.350495</td>\n",
       "      <td>-0.735196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143.534598</td>\n",
       "      <td>2684.064771</td>\n",
       "      <td>-0.122034</td>\n",
       "      <td>-0.726785</td>\n",
       "      <td>163.988122</td>\n",
       "      <td>3032.684210</td>\n",
       "      <td>0.098623</td>\n",
       "      <td>-0.932985</td>\n",
       "      <td>67.333745</td>\n",
       "      <td>1393.034182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395657</td>\n",
       "      <td>-1.041771</td>\n",
       "      <td>106.869300</td>\n",
       "      <td>39.706570</td>\n",
       "      <td>1.180393</td>\n",
       "      <td>1.184484</td>\n",
       "      <td>173.509228</td>\n",
       "      <td>201.620730</td>\n",
       "      <td>0.534828</td>\n",
       "      <td>-0.842716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134.806521</td>\n",
       "      <td>3150.853829</td>\n",
       "      <td>-0.752603</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>155.817223</td>\n",
       "      <td>2935.205771</td>\n",
       "      <td>-0.687641</td>\n",
       "      <td>-0.282174</td>\n",
       "      <td>112.783462</td>\n",
       "      <td>2356.163989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.573437</td>\n",
       "      <td>-0.355013</td>\n",
       "      <td>112.668666</td>\n",
       "      <td>26.981995</td>\n",
       "      <td>2.443752</td>\n",
       "      <td>0.798156</td>\n",
       "      <td>148.112783</td>\n",
       "      <td>56.290194</td>\n",
       "      <td>1.918221</td>\n",
       "      <td>-0.305414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133.956593</td>\n",
       "      <td>1928.268962</td>\n",
       "      <td>-0.601913</td>\n",
       "      <td>0.189324</td>\n",
       "      <td>165.174087</td>\n",
       "      <td>1995.183999</td>\n",
       "      <td>-0.653920</td>\n",
       "      <td>-0.250057</td>\n",
       "      <td>46.036372</td>\n",
       "      <td>1525.296728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.566688</td>\n",
       "      <td>-0.252408</td>\n",
       "      <td>101.051419</td>\n",
       "      <td>77.759832</td>\n",
       "      <td>0.647577</td>\n",
       "      <td>1.165816</td>\n",
       "      <td>180.711256</td>\n",
       "      <td>190.856598</td>\n",
       "      <td>0.315457</td>\n",
       "      <td>-0.751957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rgb_channel_0_mean  rgb_channel_0_variance  rgb_channel_0_kurtosis  \\\n",
       "0          135.110013             3078.218207               -0.689453   \n",
       "1          132.208227             3961.755789               -1.130258   \n",
       "2          143.534598             2684.064771               -0.122034   \n",
       "3          134.806521             3150.853829               -0.752603   \n",
       "4          133.956593             1928.268962               -0.601913   \n",
       "\n",
       "   rgb_channel_0_skewness  rgb_channel_1_mean  rgb_channel_1_variance  \\\n",
       "0               -0.192670          157.315011             3449.301597   \n",
       "1               -0.302681          158.333586             3558.081163   \n",
       "2               -0.726785          163.988122             3032.684210   \n",
       "3                0.011988          155.817223             2935.205771   \n",
       "4                0.189324          165.174087             1995.183999   \n",
       "\n",
       "   rgb_channel_1_kurtosis  rgb_channel_1_skewness  rgb_channel_2_mean  \\\n",
       "0               -0.484632               -0.518085           67.831274   \n",
       "1               -0.908913               -0.458059           50.368921   \n",
       "2                0.098623               -0.932985           67.333745   \n",
       "3               -0.687641               -0.282174          112.783462   \n",
       "4               -0.653920               -0.250057           46.036372   \n",
       "\n",
       "   rgb_channel_2_variance  ...  lab_channel_0_kurtosis  \\\n",
       "0             2087.782836  ...               -0.296129   \n",
       "1             1453.704717  ...               -0.781886   \n",
       "2             1393.034182  ...                0.395657   \n",
       "3             2356.163989  ...               -0.573437   \n",
       "4             1525.296728  ...               -0.566688   \n",
       "\n",
       "   lab_channel_0_skewness  lab_channel_1_mean  lab_channel_1_variance  \\\n",
       "0               -0.606758          106.950853               61.335754   \n",
       "1               -0.534726          103.516083               21.999662   \n",
       "2               -1.041771          106.869300               39.706570   \n",
       "3               -0.355013          112.668666               26.981995   \n",
       "4               -0.252408          101.051419               77.759832   \n",
       "\n",
       "   lab_channel_1_kurtosis  lab_channel_1_skewness  lab_channel_2_mean  \\\n",
       "0                0.410354                0.917856          169.947226   \n",
       "1                2.755215                1.327478          177.711097   \n",
       "2                1.180393                1.184484          173.509228   \n",
       "3                2.443752                0.798156          148.112783   \n",
       "4                0.647577                1.165816          180.711256   \n",
       "\n",
       "   lab_channel_2_variance  lab_channel_2_kurtosis  lab_channel_2_skewness  \n",
       "0              203.966363                0.049806               -0.576146  \n",
       "1              198.029258                0.350495               -0.735196  \n",
       "2              201.620730                0.534828               -0.842716  \n",
       "3               56.290194                1.918221               -0.305414  \n",
       "4              190.856598                0.315457               -0.751957  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_color.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374ff69-1bee-49c9-a127-38bf7b2154fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Zernike Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb2e3d8b-9bcb-4314-8271-fbda88d99c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mahotas.features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def zernike(df, zernike_order):\n",
    "    features_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        gray_img = row['gray']\n",
    "\n",
    "        # Check if the image is grayscale\n",
    "        if len(gray_img.shape) == 2:\n",
    "            # If grayscale, compute Zernike moments directly\n",
    "            moments = mahotas.features.zernike_moments(gray_img, radius=zernike_order)\n",
    "        else:\n",
    "            raise ValueError(\"Input image must be grayscale.\")\n",
    "\n",
    "        features_list.append(moments)\n",
    "\n",
    "    # Determine the number of features per image\n",
    "    num_features_per_image = len(features_list[0])\n",
    "\n",
    "    # Add features to the DataFrame\n",
    "    feature_columns = [f'feature_{i}' for i in range(num_features_per_image)]\n",
    "    df_features = pd.DataFrame(features_list, columns=feature_columns)\n",
    "    df_result = pd.concat([df, df_features], axis=1)\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2839e315-3874-4df5-9d76-5f159e11d6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.085352</td>\n",
       "      <td>0.096424</td>\n",
       "      <td>0.007132</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>0.035195</td>\n",
       "      <td>0.047294</td>\n",
       "      <td>0.034389</td>\n",
       "      <td>0.012664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021457</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>0.024865</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.017575</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.035992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.014304</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>0.037181</td>\n",
       "      <td>0.013888</td>\n",
       "      <td>0.019413</td>\n",
       "      <td>0.032812</td>\n",
       "      <td>0.021023</td>\n",
       "      <td>0.036471</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019798</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.033492</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.008411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.079290</td>\n",
       "      <td>0.030770</td>\n",
       "      <td>0.009481</td>\n",
       "      <td>0.047826</td>\n",
       "      <td>0.057547</td>\n",
       "      <td>0.014013</td>\n",
       "      <td>0.021884</td>\n",
       "      <td>0.035470</td>\n",
       "      <td>0.021808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028761</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>0.039179</td>\n",
       "      <td>0.039791</td>\n",
       "      <td>0.023068</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.024128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.037962</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.070988</td>\n",
       "      <td>0.044010</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>0.015647</td>\n",
       "      <td>0.008096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.012084</td>\n",
       "      <td>0.020084</td>\n",
       "      <td>0.021049</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.017935</td>\n",
       "      <td>0.029116</td>\n",
       "      <td>0.023423</td>\n",
       "      <td>0.023256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31831</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.041976</td>\n",
       "      <td>0.050413</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>0.035719</td>\n",
       "      <td>0.045422</td>\n",
       "      <td>0.056392</td>\n",
       "      <td>0.064779</td>\n",
       "      <td>0.029646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025135</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.041041</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.004188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0    0.31831   0.017224   0.085352   0.096424   0.007132   0.019454   \n",
       "1    0.31831   0.014304   0.028368   0.037181   0.013888   0.019413   \n",
       "2    0.31831   0.079290   0.030770   0.009481   0.047826   0.057547   \n",
       "3    0.31831   0.037962   0.004782   0.037121   0.070988   0.044010   \n",
       "4    0.31831   0.008223   0.041976   0.050413   0.029785   0.035719   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  ...  feature_15  feature_16  \\\n",
       "0   0.035195   0.047294   0.034389   0.012664  ...    0.021457    0.009277   \n",
       "1   0.032812   0.021023   0.036471   0.003854  ...    0.019798    0.021939   \n",
       "2   0.014013   0.021884   0.035470   0.021808  ...    0.028761    0.049967   \n",
       "3   0.004336   0.021578   0.015647   0.008096  ...    0.017136    0.012084   \n",
       "4   0.045422   0.056392   0.064779   0.029646  ...    0.025135    0.003060   \n",
       "\n",
       "   feature_17  feature_18  feature_19  feature_20  feature_21  feature_22  \\\n",
       "0    0.024865    0.004817    0.017575    0.003713    0.003742    0.016868   \n",
       "1    0.024250    0.033492    0.027907    0.000967    0.009835    0.017213   \n",
       "2    0.039179    0.039791    0.023068    0.031944    0.021562    0.009072   \n",
       "3    0.020084    0.021049    0.013913    0.004487    0.017935    0.029116   \n",
       "4    0.024900    0.048771    0.041041    0.005773    0.034733    0.012531   \n",
       "\n",
       "   feature_23  feature_24  \n",
       "0    0.031700    0.035992  \n",
       "1    0.006593    0.008411  \n",
       "2    0.019779    0.024128  \n",
       "3    0.023423    0.023256  \n",
       "4    0.014290    0.004188  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zernike_df = zernike(extra_resized, 10)\n",
    "f_zernike = zernike_df.iloc[:, 51:]\n",
    "f_zernike.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f9f14-f5c1-49e8-bc81-0ff472678dd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Legendre Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04c31faf-4303-4c2c-9116-db4207ca5f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extra_resized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c11c6966-8fce-4fe6-b283-0ae5a249459b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l_input = extra_resized.drop(columns=['class','hsv','lab','red','green','blue', 'hue_v', 'saturation_v', 'value_v', 'lightness', 'a', 'b'])\n",
    "l_input = l_input.iloc[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "797a48fb-2cbb-4377-a2c7-c851bf96b9de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>rgb</th>\n",
       "      <th>gray</th>\n",
       "      <th>rgb_channel_0_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLB_multi_leaf (1).jpg</td>\n",
       "      <td>[[[115, 152, 47], [78, 113, 9], [116, 147, 43]...</td>\n",
       "      <td>[[129, 91, 126, 196, 208, 157, 175, 128, 114, ...</td>\n",
       "      <td>135.110013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLB_multi_leaf (2).jpg</td>\n",
       "      <td>[[[198, 218, 97], [190, 210, 89], [189, 208, 9...</td>\n",
       "      <td>[[198, 190, 189, 183, 189, 189, 187, 187, 189,...</td>\n",
       "      <td>132.208227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLB_multi_leaf (3).jpg</td>\n",
       "      <td>[[[137, 176, 51], [166, 203, 64], [165, 200, 3...</td>\n",
       "      <td>[[150, 176, 171, 177, 193, 187, 142, 136, 135,...</td>\n",
       "      <td>143.534598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLB_multi_leaf (4).jpg</td>\n",
       "      <td>[[[192, 212, 143], [201, 222, 157], [187, 206,...</td>\n",
       "      <td>[[198, 208, 194, 198, 196, 159, 143, 145, 165,...</td>\n",
       "      <td>134.806521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BLB_multi_leaf (5).jpg</td>\n",
       "      <td>[[[132, 176, 27], [135, 180, 27], [161, 206, 4...</td>\n",
       "      <td>[[146, 149, 174, 126, 153, 153, 185, 201, 210,...</td>\n",
       "      <td>133.956593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 img_name                                                rgb  \\\n",
       "0  BLB_multi_leaf (1).jpg  [[[115, 152, 47], [78, 113, 9], [116, 147, 43]...   \n",
       "1  BLB_multi_leaf (2).jpg  [[[198, 218, 97], [190, 210, 89], [189, 208, 9...   \n",
       "2  BLB_multi_leaf (3).jpg  [[[137, 176, 51], [166, 203, 64], [165, 200, 3...   \n",
       "3  BLB_multi_leaf (4).jpg  [[[192, 212, 143], [201, 222, 157], [187, 206,...   \n",
       "4  BLB_multi_leaf (5).jpg  [[[132, 176, 27], [135, 180, 27], [161, 206, 4...   \n",
       "\n",
       "                                                gray  rgb_channel_0_mean  \n",
       "0  [[129, 91, 126, 196, 208, 157, 175, 128, 114, ...          135.110013  \n",
       "1  [[198, 190, 189, 183, 189, 189, 187, 187, 189,...          132.208227  \n",
       "2  [[150, 176, 171, 177, 193, 187, 142, 136, 135,...          143.534598  \n",
       "3  [[198, 208, 194, 198, 196, 159, 143, 145, 165,...          134.806521  \n",
       "4  [[146, 149, 174, 126, 153, 153, 185, 201, 210,...          133.956593  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eac178d0-3ed3-4c52-a778-5ae124e3d99c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from scipy.special import legendre\n",
    "\n",
    "def compute_legendre_features(rgb_image, degree):\n",
    "    # Get V channel from RGB image\n",
    "    v_channel = rgb_image[:, :, 2]\n",
    "\n",
    "    # Define x and y coordinates\n",
    "    x_size, y_size = v_channel.shape\n",
    "    x = np.linspace(-1, 1, x_size)\n",
    "    y = np.linspace(-1, 1, y_size)\n",
    "\n",
    "    # Compute Legendre features for the V channel\n",
    "    legendre_features = np.zeros((degree + 1, degree + 1))\n",
    "\n",
    "    for j in range(degree + 1):\n",
    "        for k in range(degree + 1):\n",
    "            legendre_features[j, k] = np.sum(v_channel * legendre(j)(x) * legendre(k)(y))\n",
    "\n",
    "    return legendre_features.flatten()\n",
    "\n",
    "def add_legendre(df, degree):\n",
    "    # Create a new DataFrame for Legendre features\n",
    "    legendre_columns = [f'v_legendre_{i}_{j}' for i in range(degree + 1) for j in range(degree + 1)]\n",
    "    legendre_df = pd.DataFrame(df['rgb'].apply(lambda rgb: compute_legendre_features(rgb, degree)).values.tolist(), columns=legendre_columns)\n",
    "\n",
    "    # Concatenate the new DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([df, legendre_df], axis=1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43846dc1-14e3-448d-adc4-a22e38c851ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "legendre_df = add_legendre(l_input, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b74b7ce3-9c49-42a7-95d2-ea2df961247b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_legendre_0_0</th>\n",
       "      <th>v_legendre_0_1</th>\n",
       "      <th>v_legendre_0_2</th>\n",
       "      <th>v_legendre_0_3</th>\n",
       "      <th>v_legendre_0_4</th>\n",
       "      <th>v_legendre_0_5</th>\n",
       "      <th>v_legendre_0_6</th>\n",
       "      <th>v_legendre_0_7</th>\n",
       "      <th>v_legendre_0_8</th>\n",
       "      <th>v_legendre_0_9</th>\n",
       "      <th>...</th>\n",
       "      <th>v_legendre_10_1</th>\n",
       "      <th>v_legendre_10_2</th>\n",
       "      <th>v_legendre_10_3</th>\n",
       "      <th>v_legendre_10_4</th>\n",
       "      <th>v_legendre_10_5</th>\n",
       "      <th>v_legendre_10_6</th>\n",
       "      <th>v_legendre_10_7</th>\n",
       "      <th>v_legendre_10_8</th>\n",
       "      <th>v_legendre_10_9</th>\n",
       "      <th>v_legendre_10_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3403502.0</td>\n",
       "      <td>97362.493274</td>\n",
       "      <td>-48201.759416</td>\n",
       "      <td>-43493.535531</td>\n",
       "      <td>-7187.545250</td>\n",
       "      <td>-13629.967522</td>\n",
       "      <td>26794.954058</td>\n",
       "      <td>-16840.577069</td>\n",
       "      <td>-19979.273476</td>\n",
       "      <td>3169.619967</td>\n",
       "      <td>...</td>\n",
       "      <td>11979.555139</td>\n",
       "      <td>6950.403894</td>\n",
       "      <td>-1337.870389</td>\n",
       "      <td>10980.581080</td>\n",
       "      <td>-2731.321916</td>\n",
       "      <td>13832.668995</td>\n",
       "      <td>-4770.100418</td>\n",
       "      <td>6167.284262</td>\n",
       "      <td>4327.878377</td>\n",
       "      <td>171054.141659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2527311.0</td>\n",
       "      <td>103013.053812</td>\n",
       "      <td>-36361.603672</td>\n",
       "      <td>5917.770393</td>\n",
       "      <td>47863.129349</td>\n",
       "      <td>1575.078268</td>\n",
       "      <td>12099.674282</td>\n",
       "      <td>-3201.383018</td>\n",
       "      <td>18315.225460</td>\n",
       "      <td>1422.844543</td>\n",
       "      <td>...</td>\n",
       "      <td>-821.672825</td>\n",
       "      <td>16762.645859</td>\n",
       "      <td>2097.910709</td>\n",
       "      <td>12284.262029</td>\n",
       "      <td>195.439018</td>\n",
       "      <td>20480.883830</td>\n",
       "      <td>3132.492664</td>\n",
       "      <td>11516.628122</td>\n",
       "      <td>8400.271678</td>\n",
       "      <td>134299.780209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3378538.0</td>\n",
       "      <td>57117.757848</td>\n",
       "      <td>-80632.660661</td>\n",
       "      <td>-510.363287</td>\n",
       "      <td>11451.263244</td>\n",
       "      <td>-12619.841113</td>\n",
       "      <td>-28828.907259</td>\n",
       "      <td>-769.368737</td>\n",
       "      <td>27917.794633</td>\n",
       "      <td>-6883.608118</td>\n",
       "      <td>...</td>\n",
       "      <td>3963.350675</td>\n",
       "      <td>18954.699088</td>\n",
       "      <td>-2074.126958</td>\n",
       "      <td>8085.322444</td>\n",
       "      <td>1051.648198</td>\n",
       "      <td>8771.579378</td>\n",
       "      <td>-2117.981725</td>\n",
       "      <td>3647.296353</td>\n",
       "      <td>5448.970980</td>\n",
       "      <td>164009.682234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5659023.0</td>\n",
       "      <td>-217617.215247</td>\n",
       "      <td>39749.222928</td>\n",
       "      <td>-6034.575324</td>\n",
       "      <td>13640.673159</td>\n",
       "      <td>21378.007074</td>\n",
       "      <td>44068.112251</td>\n",
       "      <td>-24151.624353</td>\n",
       "      <td>14049.614456</td>\n",
       "      <td>-33151.322155</td>\n",
       "      <td>...</td>\n",
       "      <td>-4619.286498</td>\n",
       "      <td>15514.090473</td>\n",
       "      <td>-6651.294307</td>\n",
       "      <td>31327.480610</td>\n",
       "      <td>553.852950</td>\n",
       "      <td>28293.237294</td>\n",
       "      <td>-4090.823341</td>\n",
       "      <td>27115.154433</td>\n",
       "      <td>-18929.538381</td>\n",
       "      <td>296838.144091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2309921.0</td>\n",
       "      <td>-148810.686099</td>\n",
       "      <td>-37569.159223</td>\n",
       "      <td>52055.054845</td>\n",
       "      <td>44925.442955</td>\n",
       "      <td>-23770.114279</td>\n",
       "      <td>17043.186742</td>\n",
       "      <td>11900.041479</td>\n",
       "      <td>1297.065850</td>\n",
       "      <td>-35581.018742</td>\n",
       "      <td>...</td>\n",
       "      <td>-9263.364484</td>\n",
       "      <td>11693.901946</td>\n",
       "      <td>-7594.119586</td>\n",
       "      <td>7040.301359</td>\n",
       "      <td>-2945.920523</td>\n",
       "      <td>20936.096634</td>\n",
       "      <td>1642.000010</td>\n",
       "      <td>5613.007761</td>\n",
       "      <td>-10798.603049</td>\n",
       "      <td>122542.843959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   v_legendre_0_0  v_legendre_0_1  v_legendre_0_2  v_legendre_0_3  \\\n",
       "0       3403502.0    97362.493274   -48201.759416   -43493.535531   \n",
       "1       2527311.0   103013.053812   -36361.603672     5917.770393   \n",
       "2       3378538.0    57117.757848   -80632.660661     -510.363287   \n",
       "3       5659023.0  -217617.215247    39749.222928    -6034.575324   \n",
       "4       2309921.0  -148810.686099   -37569.159223    52055.054845   \n",
       "\n",
       "   v_legendre_0_4  v_legendre_0_5  v_legendre_0_6  v_legendre_0_7  \\\n",
       "0    -7187.545250   -13629.967522    26794.954058   -16840.577069   \n",
       "1    47863.129349     1575.078268    12099.674282    -3201.383018   \n",
       "2    11451.263244   -12619.841113   -28828.907259     -769.368737   \n",
       "3    13640.673159    21378.007074    44068.112251   -24151.624353   \n",
       "4    44925.442955   -23770.114279    17043.186742    11900.041479   \n",
       "\n",
       "   v_legendre_0_8  v_legendre_0_9  ...  v_legendre_10_1  v_legendre_10_2  \\\n",
       "0   -19979.273476     3169.619967  ...     11979.555139      6950.403894   \n",
       "1    18315.225460     1422.844543  ...      -821.672825     16762.645859   \n",
       "2    27917.794633    -6883.608118  ...      3963.350675     18954.699088   \n",
       "3    14049.614456   -33151.322155  ...     -4619.286498     15514.090473   \n",
       "4     1297.065850   -35581.018742  ...     -9263.364484     11693.901946   \n",
       "\n",
       "   v_legendre_10_3  v_legendre_10_4  v_legendre_10_5  v_legendre_10_6  \\\n",
       "0     -1337.870389     10980.581080     -2731.321916     13832.668995   \n",
       "1      2097.910709     12284.262029       195.439018     20480.883830   \n",
       "2     -2074.126958      8085.322444      1051.648198      8771.579378   \n",
       "3     -6651.294307     31327.480610       553.852950     28293.237294   \n",
       "4     -7594.119586      7040.301359     -2945.920523     20936.096634   \n",
       "\n",
       "   v_legendre_10_7  v_legendre_10_8  v_legendre_10_9  v_legendre_10_10  \n",
       "0     -4770.100418      6167.284262      4327.878377     171054.141659  \n",
       "1      3132.492664     11516.628122      8400.271678     134299.780209  \n",
       "2     -2117.981725      3647.296353      5448.970980     164009.682234  \n",
       "3     -4090.823341     27115.154433    -18929.538381     296838.144091  \n",
       "4      1642.000010      5613.007761    -10798.603049     122542.843959  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_legendre = legendre_df.iloc[:, 4:]\n",
    "f_legendre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bb6a4-6f30-4f3c-8b50-0bad0e8b6c75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Complete Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db5931a6-4d94-44df-8c1a-82076732f0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame with just the selected column\n",
    "classes = pd.DataFrame(extra_resized['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5002378-98b0-4b70-ba29-a07ba57764f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#complete_features = pd.concat([classes,f_color,f_histogram, f_glcm], axis=1)\n",
    "complete_features = pd.concat([classes,f_histogram, f_glcm, f_color, f_legendre, f_zernike], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3453770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_features.to_csv('complete_features.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "060e531f-0a6e-455b-8dbf-07ca777d8c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>red_1</th>\n",
       "      <th>red_2</th>\n",
       "      <th>red_3</th>\n",
       "      <th>red_4</th>\n",
       "      <th>red_5</th>\n",
       "      <th>red_6</th>\n",
       "      <th>red_7</th>\n",
       "      <th>red_8</th>\n",
       "      <th>red_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "      <td>93</td>\n",
       "      <td>143</td>\n",
       "      <td>223</td>\n",
       "      <td>231</td>\n",
       "      <td>305</td>\n",
       "      <td>314</td>\n",
       "      <td>330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021457</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>0.024865</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.017575</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.035992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>153</td>\n",
       "      <td>210</td>\n",
       "      <td>261</td>\n",
       "      <td>308</td>\n",
       "      <td>349</td>\n",
       "      <td>391</td>\n",
       "      <td>392</td>\n",
       "      <td>358</td>\n",
       "      <td>383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019798</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.033492</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.008411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>62</td>\n",
       "      <td>145</td>\n",
       "      <td>205</td>\n",
       "      <td>290</td>\n",
       "      <td>337</td>\n",
       "      <td>325</td>\n",
       "      <td>338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028761</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>0.039179</td>\n",
       "      <td>0.039791</td>\n",
       "      <td>0.023068</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.024128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>53</td>\n",
       "      <td>89</td>\n",
       "      <td>106</td>\n",
       "      <td>194</td>\n",
       "      <td>242</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.012084</td>\n",
       "      <td>0.020084</td>\n",
       "      <td>0.021049</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.017935</td>\n",
       "      <td>0.029116</td>\n",
       "      <td>0.023423</td>\n",
       "      <td>0.023256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025135</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.041041</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.004188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 934 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   class  red_1  red_2  red_3  red_4  red_5  red_6  red_7  \\\n",
       "0  bacterial_leaf_blight     19     47     93    143    223    231    305   \n",
       "1  bacterial_leaf_blight    153    210    261    308    349    391    392   \n",
       "2  bacterial_leaf_blight     14     35     62    145    205    290    337   \n",
       "3  bacterial_leaf_blight     14     14     40     53     89    106    194   \n",
       "4  bacterial_leaf_blight      0      1      3      4      1      6      6   \n",
       "\n",
       "   red_8  red_9  ...  feature_15  feature_16  feature_17  feature_18  \\\n",
       "0    314    330  ...    0.021457    0.009277    0.024865    0.004817   \n",
       "1    358    383  ...    0.019798    0.021939    0.024250    0.033492   \n",
       "2    325    338  ...    0.028761    0.049967    0.039179    0.039791   \n",
       "3    242    267  ...    0.017136    0.012084    0.020084    0.021049   \n",
       "4      9      8  ...    0.025135    0.003060    0.024900    0.048771   \n",
       "\n",
       "   feature_19  feature_20  feature_21  feature_22  feature_23  feature_24  \n",
       "0    0.017575    0.003713    0.003742    0.016868    0.031700    0.035992  \n",
       "1    0.027907    0.000967    0.009835    0.017213    0.006593    0.008411  \n",
       "2    0.023068    0.031944    0.021562    0.009072    0.019779    0.024128  \n",
       "3    0.013913    0.004487    0.017935    0.029116    0.023423    0.023256  \n",
       "4    0.041041    0.005773    0.034733    0.012531    0.014290    0.004188  \n",
       "\n",
       "[5 rows x 934 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5fb4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "zernike_features = complete_features.iloc[:, [0] + list(range(909, 933))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93653fd4-c7c1-4763-ab89-2102d2f29c4c",
   "metadata": {},
   "source": [
    "# Data Normalization and One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80096471-c274-4870-bea1-05b14edc651f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_dataframe(df):\n",
    "    # Create a MinMaxScaler instance\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Extract numerical columns (only numerical columns need normalization)\n",
    "    numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "    # Apply normalization to each numerical column\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b235b92a-228e-4021-975a-b860cafec988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josh\\AppData\\Local\\Temp\\ipykernel_17796\\3205247050.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.057360</td>\n",
       "      <td>0.357953</td>\n",
       "      <td>0.438770</td>\n",
       "      <td>0.031918</td>\n",
       "      <td>0.087531</td>\n",
       "      <td>0.157434</td>\n",
       "      <td>0.207451</td>\n",
       "      <td>0.173957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084361</td>\n",
       "      <td>0.162035</td>\n",
       "      <td>0.067411</td>\n",
       "      <td>0.171821</td>\n",
       "      <td>0.034570</td>\n",
       "      <td>0.110094</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>0.102434</td>\n",
       "      <td>0.202999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.047567</td>\n",
       "      <td>0.118912</td>\n",
       "      <td>0.168614</td>\n",
       "      <td>0.062635</td>\n",
       "      <td>0.087346</td>\n",
       "      <td>0.146774</td>\n",
       "      <td>0.091407</td>\n",
       "      <td>0.184678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206615</td>\n",
       "      <td>0.149359</td>\n",
       "      <td>0.163611</td>\n",
       "      <td>0.167420</td>\n",
       "      <td>0.250202</td>\n",
       "      <td>0.175125</td>\n",
       "      <td>0.006643</td>\n",
       "      <td>0.046324</td>\n",
       "      <td>0.104561</td>\n",
       "      <td>0.039302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.265559</td>\n",
       "      <td>0.128987</td>\n",
       "      <td>0.042297</td>\n",
       "      <td>0.216937</td>\n",
       "      <td>0.261162</td>\n",
       "      <td>0.062658</td>\n",
       "      <td>0.095213</td>\n",
       "      <td>0.179524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161987</td>\n",
       "      <td>0.217850</td>\n",
       "      <td>0.376573</td>\n",
       "      <td>0.274281</td>\n",
       "      <td>0.297571</td>\n",
       "      <td>0.144669</td>\n",
       "      <td>0.227274</td>\n",
       "      <td>0.103086</td>\n",
       "      <td>0.054425</td>\n",
       "      <td>0.125273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>0.126925</td>\n",
       "      <td>0.019972</td>\n",
       "      <td>0.168341</td>\n",
       "      <td>0.322239</td>\n",
       "      <td>0.199460</td>\n",
       "      <td>0.019359</td>\n",
       "      <td>0.093859</td>\n",
       "      <td>0.077440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048934</td>\n",
       "      <td>0.129016</td>\n",
       "      <td>0.088736</td>\n",
       "      <td>0.137602</td>\n",
       "      <td>0.156629</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>0.031712</td>\n",
       "      <td>0.085532</td>\n",
       "      <td>0.177860</td>\n",
       "      <td>0.149036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bacterial_leaf_blight</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.027167</td>\n",
       "      <td>0.175996</td>\n",
       "      <td>0.228953</td>\n",
       "      <td>0.134911</td>\n",
       "      <td>0.161669</td>\n",
       "      <td>0.203198</td>\n",
       "      <td>0.247635</td>\n",
       "      <td>0.330465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164551</td>\n",
       "      <td>0.190142</td>\n",
       "      <td>0.020170</td>\n",
       "      <td>0.172070</td>\n",
       "      <td>0.365101</td>\n",
       "      <td>0.257791</td>\n",
       "      <td>0.040872</td>\n",
       "      <td>0.166836</td>\n",
       "      <td>0.075724</td>\n",
       "      <td>0.089486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   class  feature_0  feature_1  feature_2  feature_3  \\\n",
       "0  bacterial_leaf_blight   0.515625   0.057360   0.357953   0.438770   \n",
       "1  bacterial_leaf_blight   0.562500   0.047567   0.118912   0.168614   \n",
       "2  bacterial_leaf_blight   0.515625   0.265559   0.128987   0.042297   \n",
       "3  bacterial_leaf_blight   0.484375   0.126925   0.019972   0.168341   \n",
       "4  bacterial_leaf_blight   0.500000   0.027167   0.175996   0.228953   \n",
       "\n",
       "   feature_4  feature_5  feature_6  feature_7  feature_8  ...  feature_14  \\\n",
       "0   0.031918   0.087531   0.157434   0.207451   0.173957  ...    0.084361   \n",
       "1   0.062635   0.087346   0.146774   0.091407   0.184678  ...    0.206615   \n",
       "2   0.216937   0.261162   0.062658   0.095213   0.179524  ...    0.161987   \n",
       "3   0.322239   0.199460   0.019359   0.093859   0.077440  ...    0.048934   \n",
       "4   0.134911   0.161669   0.203198   0.247635   0.330465  ...    0.164551   \n",
       "\n",
       "   feature_15  feature_16  feature_17  feature_18  feature_19  feature_20  \\\n",
       "0    0.162035    0.067411    0.171821    0.034570    0.110094    0.026200   \n",
       "1    0.149359    0.163611    0.167420    0.250202    0.175125    0.006643   \n",
       "2    0.217850    0.376573    0.274281    0.297571    0.144669    0.227274   \n",
       "3    0.129016    0.088736    0.137602    0.156629    0.087048    0.031712   \n",
       "4    0.190142    0.020170    0.172070    0.365101    0.257791    0.040872   \n",
       "\n",
       "   feature_21  feature_22  feature_23  \n",
       "0    0.016835    0.102434    0.202999  \n",
       "1    0.046324    0.104561    0.039302  \n",
       "2    0.103086    0.054425    0.125273  \n",
       "3    0.085532    0.177860    0.149036  \n",
       "4    0.166836    0.075724    0.089486  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_features = normalize_dataframe(zernike_features)\n",
    "normalized_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b339da2-35a0-455d-8db7-266f056dd5ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josh\\AppData\\Local\\Temp\\ipykernel_17796\\1409264267.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  normalized_features['class'] = label_encoder.fit_transform(normalized_features['class'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "normalized_features['class'] = label_encoder.fit_transform(normalized_features['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "097defd8-e917-45e2-a877-dfe178bfe066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.057360</td>\n",
       "      <td>0.357953</td>\n",
       "      <td>0.438770</td>\n",
       "      <td>0.031918</td>\n",
       "      <td>0.087531</td>\n",
       "      <td>0.157434</td>\n",
       "      <td>0.207451</td>\n",
       "      <td>0.173957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084361</td>\n",
       "      <td>0.162035</td>\n",
       "      <td>0.067411</td>\n",
       "      <td>0.171821</td>\n",
       "      <td>0.034570</td>\n",
       "      <td>0.110094</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>0.102434</td>\n",
       "      <td>0.202999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.047567</td>\n",
       "      <td>0.118912</td>\n",
       "      <td>0.168614</td>\n",
       "      <td>0.062635</td>\n",
       "      <td>0.087346</td>\n",
       "      <td>0.146774</td>\n",
       "      <td>0.091407</td>\n",
       "      <td>0.184678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206615</td>\n",
       "      <td>0.149359</td>\n",
       "      <td>0.163611</td>\n",
       "      <td>0.167420</td>\n",
       "      <td>0.250202</td>\n",
       "      <td>0.175125</td>\n",
       "      <td>0.006643</td>\n",
       "      <td>0.046324</td>\n",
       "      <td>0.104561</td>\n",
       "      <td>0.039302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.265559</td>\n",
       "      <td>0.128987</td>\n",
       "      <td>0.042297</td>\n",
       "      <td>0.216937</td>\n",
       "      <td>0.261162</td>\n",
       "      <td>0.062658</td>\n",
       "      <td>0.095213</td>\n",
       "      <td>0.179524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161987</td>\n",
       "      <td>0.217850</td>\n",
       "      <td>0.376573</td>\n",
       "      <td>0.274281</td>\n",
       "      <td>0.297571</td>\n",
       "      <td>0.144669</td>\n",
       "      <td>0.227274</td>\n",
       "      <td>0.103086</td>\n",
       "      <td>0.054425</td>\n",
       "      <td>0.125273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>0.126925</td>\n",
       "      <td>0.019972</td>\n",
       "      <td>0.168341</td>\n",
       "      <td>0.322239</td>\n",
       "      <td>0.199460</td>\n",
       "      <td>0.019359</td>\n",
       "      <td>0.093859</td>\n",
       "      <td>0.077440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048934</td>\n",
       "      <td>0.129016</td>\n",
       "      <td>0.088736</td>\n",
       "      <td>0.137602</td>\n",
       "      <td>0.156629</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>0.031712</td>\n",
       "      <td>0.085532</td>\n",
       "      <td>0.177860</td>\n",
       "      <td>0.149036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.027167</td>\n",
       "      <td>0.175996</td>\n",
       "      <td>0.228953</td>\n",
       "      <td>0.134911</td>\n",
       "      <td>0.161669</td>\n",
       "      <td>0.203198</td>\n",
       "      <td>0.247635</td>\n",
       "      <td>0.330465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164551</td>\n",
       "      <td>0.190142</td>\n",
       "      <td>0.020170</td>\n",
       "      <td>0.172070</td>\n",
       "      <td>0.365101</td>\n",
       "      <td>0.257791</td>\n",
       "      <td>0.040872</td>\n",
       "      <td>0.166836</td>\n",
       "      <td>0.075724</td>\n",
       "      <td>0.089486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0      0   0.515625   0.057360   0.357953   0.438770   0.031918   0.087531   \n",
       "1      0   0.562500   0.047567   0.118912   0.168614   0.062635   0.087346   \n",
       "2      0   0.515625   0.265559   0.128987   0.042297   0.216937   0.261162   \n",
       "3      0   0.484375   0.126925   0.019972   0.168341   0.322239   0.199460   \n",
       "4      0   0.500000   0.027167   0.175996   0.228953   0.134911   0.161669   \n",
       "\n",
       "   feature_6  feature_7  feature_8  ...  feature_14  feature_15  feature_16  \\\n",
       "0   0.157434   0.207451   0.173957  ...    0.084361    0.162035    0.067411   \n",
       "1   0.146774   0.091407   0.184678  ...    0.206615    0.149359    0.163611   \n",
       "2   0.062658   0.095213   0.179524  ...    0.161987    0.217850    0.376573   \n",
       "3   0.019359   0.093859   0.077440  ...    0.048934    0.129016    0.088736   \n",
       "4   0.203198   0.247635   0.330465  ...    0.164551    0.190142    0.020170   \n",
       "\n",
       "   feature_17  feature_18  feature_19  feature_20  feature_21  feature_22  \\\n",
       "0    0.171821    0.034570    0.110094    0.026200    0.016835    0.102434   \n",
       "1    0.167420    0.250202    0.175125    0.006643    0.046324    0.104561   \n",
       "2    0.274281    0.297571    0.144669    0.227274    0.103086    0.054425   \n",
       "3    0.137602    0.156629    0.087048    0.031712    0.085532    0.177860   \n",
       "4    0.172070    0.365101    0.257791    0.040872    0.166836    0.075724   \n",
       "\n",
       "   feature_23  \n",
       "0    0.202999  \n",
       "1    0.039302  \n",
       "2    0.125273  \n",
       "3    0.149036  \n",
       "4    0.089486  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77638c94-04af-43fb-a1db-0548a9c497cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acc8851d-dc36-4634-8d2f-8a3016c14ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def custom_train_test_split(dataframe, target_column, test_size=0.2, random_state=None):\n",
    "    # Separate features and labels\n",
    "    X = dataframe.drop(target_column, axis=1)  # Features\n",
    "    y = dataframe[target_column]  # Labels\n",
    "\n",
    "    # Perform the train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53594f6d-e920-4b66-91d8-7ba1ad60e4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = custom_train_test_split(normalized_features, 'class', test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "165bd148-e43b-401d-94a1-24f08ca4ef46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features (all columns except the first)\n",
    "x_d = normalized_features.iloc[:, 1:].values\n",
    "\n",
    "# Extract target variable (first column)\n",
    "y_d = normalized_features.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b4890-0ac4-4c6d-b3c1-9acf0c770916",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "705fec05-d73e-470c-9a2b-d71b16d96f36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Josh\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "33/33 [==============================] - 2s 16ms/step - loss: 2.6754 - accuracy: 0.0878 - val_loss: 2.6065 - val_accuracy: 0.1071\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5982 - accuracy: 0.1088 - val_loss: 2.5750 - val_accuracy: 0.1250\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5616 - accuracy: 0.1240 - val_loss: 2.5609 - val_accuracy: 0.1250\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5325 - accuracy: 0.1393 - val_loss: 2.5519 - val_accuracy: 0.1429\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5080 - accuracy: 0.1517 - val_loss: 2.5487 - val_accuracy: 0.1786\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4872 - accuracy: 0.1527 - val_loss: 2.5415 - val_accuracy: 0.1964\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4680 - accuracy: 0.1613 - val_loss: 2.5388 - val_accuracy: 0.1607\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4492 - accuracy: 0.1679 - val_loss: 2.5364 - val_accuracy: 0.1429\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4329 - accuracy: 0.1689 - val_loss: 2.5311 - val_accuracy: 0.1429\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4143 - accuracy: 0.1899 - val_loss: 2.5259 - val_accuracy: 0.1429\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3950 - accuracy: 0.1956 - val_loss: 2.5276 - val_accuracy: 0.1429\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3776 - accuracy: 0.2080 - val_loss: 2.5326 - val_accuracy: 0.1964\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3629 - accuracy: 0.2137 - val_loss: 2.5274 - val_accuracy: 0.1607\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3467 - accuracy: 0.2118 - val_loss: 2.5347 - val_accuracy: 0.1607\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3301 - accuracy: 0.2176 - val_loss: 2.5276 - val_accuracy: 0.1786\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3149 - accuracy: 0.2233 - val_loss: 2.5185 - val_accuracy: 0.1607\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3016 - accuracy: 0.2290 - val_loss: 2.5323 - val_accuracy: 0.1786\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2870 - accuracy: 0.2357 - val_loss: 2.5246 - val_accuracy: 0.1964\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2717 - accuracy: 0.2462 - val_loss: 2.5240 - val_accuracy: 0.1786\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2567 - accuracy: 0.2519 - val_loss: 2.5372 - val_accuracy: 0.1786\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2443 - accuracy: 0.2615 - val_loss: 2.5344 - val_accuracy: 0.1607\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2301 - accuracy: 0.2662 - val_loss: 2.5299 - val_accuracy: 0.1964\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2161 - accuracy: 0.2710 - val_loss: 2.5336 - val_accuracy: 0.1607\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2058 - accuracy: 0.2605 - val_loss: 2.5439 - val_accuracy: 0.1786\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1918 - accuracy: 0.2739 - val_loss: 2.5397 - val_accuracy: 0.1607\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1768 - accuracy: 0.2910 - val_loss: 2.5399 - val_accuracy: 0.1607\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1639 - accuracy: 0.2968 - val_loss: 2.5454 - val_accuracy: 0.1429\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1504 - accuracy: 0.2996 - val_loss: 2.5535 - val_accuracy: 0.1607\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1359 - accuracy: 0.3073 - val_loss: 2.5543 - val_accuracy: 0.1607\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1255 - accuracy: 0.3034 - val_loss: 2.5687 - val_accuracy: 0.1429\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1095 - accuracy: 0.3053 - val_loss: 2.5600 - val_accuracy: 0.1429\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0975 - accuracy: 0.3139 - val_loss: 2.5765 - val_accuracy: 0.1429\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0851 - accuracy: 0.3197 - val_loss: 2.5770 - val_accuracy: 0.1250\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0710 - accuracy: 0.3225 - val_loss: 2.5881 - val_accuracy: 0.1250\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0588 - accuracy: 0.3282 - val_loss: 2.5876 - val_accuracy: 0.1250\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0469 - accuracy: 0.3235 - val_loss: 2.6024 - val_accuracy: 0.1250\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0361 - accuracy: 0.3244 - val_loss: 2.6119 - val_accuracy: 0.1429\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0240 - accuracy: 0.3368 - val_loss: 2.6138 - val_accuracy: 0.1250\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0153 - accuracy: 0.3387 - val_loss: 2.6359 - val_accuracy: 0.1250\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9976 - accuracy: 0.3426 - val_loss: 2.6271 - val_accuracy: 0.1071\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9891 - accuracy: 0.3435 - val_loss: 2.6449 - val_accuracy: 0.1071\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9774 - accuracy: 0.3473 - val_loss: 2.6514 - val_accuracy: 0.1429\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9636 - accuracy: 0.3473 - val_loss: 2.6650 - val_accuracy: 0.1250\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9524 - accuracy: 0.3511 - val_loss: 2.6834 - val_accuracy: 0.1429\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9401 - accuracy: 0.3597 - val_loss: 2.6850 - val_accuracy: 0.1429\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9310 - accuracy: 0.3588 - val_loss: 2.6866 - val_accuracy: 0.1607\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9163 - accuracy: 0.3693 - val_loss: 2.7048 - val_accuracy: 0.1429\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9060 - accuracy: 0.3731 - val_loss: 2.7256 - val_accuracy: 0.1429\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8961 - accuracy: 0.3664 - val_loss: 2.7285 - val_accuracy: 0.1429\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8861 - accuracy: 0.3798 - val_loss: 2.7341 - val_accuracy: 0.1429\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8768 - accuracy: 0.3721 - val_loss: 2.7474 - val_accuracy: 0.1429\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8667 - accuracy: 0.3702 - val_loss: 2.7446 - val_accuracy: 0.1607\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8557 - accuracy: 0.3874 - val_loss: 2.7713 - val_accuracy: 0.1607\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8450 - accuracy: 0.3874 - val_loss: 2.7666 - val_accuracy: 0.1429\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8347 - accuracy: 0.3893 - val_loss: 2.7935 - val_accuracy: 0.1607\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8292 - accuracy: 0.3903 - val_loss: 2.8034 - val_accuracy: 0.1429\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8149 - accuracy: 0.3903 - val_loss: 2.8097 - val_accuracy: 0.1429\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8065 - accuracy: 0.3969 - val_loss: 2.8206 - val_accuracy: 0.1429\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7971 - accuracy: 0.3969 - val_loss: 2.8376 - val_accuracy: 0.1429\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7895 - accuracy: 0.4046 - val_loss: 2.8631 - val_accuracy: 0.1429\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7793 - accuracy: 0.4055 - val_loss: 2.8584 - val_accuracy: 0.1607\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7705 - accuracy: 0.4084 - val_loss: 2.8805 - val_accuracy: 0.1429\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7615 - accuracy: 0.4160 - val_loss: 2.8922 - val_accuracy: 0.1429\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7544 - accuracy: 0.4094 - val_loss: 2.8941 - val_accuracy: 0.1250\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7419 - accuracy: 0.4208 - val_loss: 2.9088 - val_accuracy: 0.1429\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7338 - accuracy: 0.4227 - val_loss: 2.9271 - val_accuracy: 0.1429\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7237 - accuracy: 0.4284 - val_loss: 2.9276 - val_accuracy: 0.1250\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7182 - accuracy: 0.4323 - val_loss: 2.9549 - val_accuracy: 0.1071\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7053 - accuracy: 0.4294 - val_loss: 2.9547 - val_accuracy: 0.1250\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7021 - accuracy: 0.4437 - val_loss: 2.9643 - val_accuracy: 0.1071\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6933 - accuracy: 0.4447 - val_loss: 2.9860 - val_accuracy: 0.1250\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6865 - accuracy: 0.4437 - val_loss: 3.0014 - val_accuracy: 0.1071\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6764 - accuracy: 0.4561 - val_loss: 3.0182 - val_accuracy: 0.1071\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6670 - accuracy: 0.4494 - val_loss: 3.0068 - val_accuracy: 0.1071\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6579 - accuracy: 0.4609 - val_loss: 3.0525 - val_accuracy: 0.1071\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6524 - accuracy: 0.4618 - val_loss: 3.0327 - val_accuracy: 0.1071\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6433 - accuracy: 0.4618 - val_loss: 3.0587 - val_accuracy: 0.1250\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6373 - accuracy: 0.4647 - val_loss: 3.0850 - val_accuracy: 0.1250\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6304 - accuracy: 0.4695 - val_loss: 3.0888 - val_accuracy: 0.1071\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6245 - accuracy: 0.4647 - val_loss: 3.0975 - val_accuracy: 0.1071\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6150 - accuracy: 0.4733 - val_loss: 3.1226 - val_accuracy: 0.1250\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6060 - accuracy: 0.4733 - val_loss: 3.1434 - val_accuracy: 0.1071\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6012 - accuracy: 0.4742 - val_loss: 3.1422 - val_accuracy: 0.1071\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5965 - accuracy: 0.4752 - val_loss: 3.1559 - val_accuracy: 0.1071\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5871 - accuracy: 0.4847 - val_loss: 3.1711 - val_accuracy: 0.1250\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5818 - accuracy: 0.4885 - val_loss: 3.2000 - val_accuracy: 0.1071\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5725 - accuracy: 0.4885 - val_loss: 3.2094 - val_accuracy: 0.1071\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5665 - accuracy: 0.4914 - val_loss: 3.2237 - val_accuracy: 0.1071\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5589 - accuracy: 0.4990 - val_loss: 3.2173 - val_accuracy: 0.1250\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5529 - accuracy: 0.4933 - val_loss: 3.2254 - val_accuracy: 0.1071\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5489 - accuracy: 0.5029 - val_loss: 3.2481 - val_accuracy: 0.1250\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5393 - accuracy: 0.4914 - val_loss: 3.2594 - val_accuracy: 0.1250\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5339 - accuracy: 0.5086 - val_loss: 3.2626 - val_accuracy: 0.1250\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5256 - accuracy: 0.5010 - val_loss: 3.2881 - val_accuracy: 0.1250\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5188 - accuracy: 0.5086 - val_loss: 3.2857 - val_accuracy: 0.1429\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5134 - accuracy: 0.5191 - val_loss: 3.3095 - val_accuracy: 0.1071\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5024 - accuracy: 0.5181 - val_loss: 3.3199 - val_accuracy: 0.1250\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4961 - accuracy: 0.5143 - val_loss: 3.3290 - val_accuracy: 0.1250\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4895 - accuracy: 0.5248 - val_loss: 3.3408 - val_accuracy: 0.1250\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4832 - accuracy: 0.5210 - val_loss: 3.3557 - val_accuracy: 0.1429\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 3.4614 - accuracy: 0.1159\n",
      "\n",
      "Test accuracy: 0.11594203114509583\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Data in x_d and y_d\n",
    "X = x_d\n",
    "y = y_d\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the ANN model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(14, activation='softmax')  # 14 output classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.05)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c715fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions = model.predict(X_test_scaled)\n",
    "# predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "# np.savetxt('test_predictions.csv', np.hstack((predicted_labels.reshape(-1, 1), y_test.reshape(-1,1))), delimiter=',', fmt='%d')\n",
    "# # np.savetxt('actual labels.csv', y_test.reshape(-1,1), delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157b35a",
   "metadata": {},
   "source": [
    "## Feature Selection PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9f8a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var: 0.99\n",
      "(1380, 23)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.7234 - accuracy: 0.0792 - val_loss: 2.6587 - val_accuracy: 0.0536\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.6234 - accuracy: 0.0964 - val_loss: 2.6233 - val_accuracy: 0.0536\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5794 - accuracy: 0.1031 - val_loss: 2.6009 - val_accuracy: 0.0536\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5489 - accuracy: 0.1221 - val_loss: 2.5865 - val_accuracy: 0.0893\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5212 - accuracy: 0.1469 - val_loss: 2.5714 - val_accuracy: 0.0714\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4975 - accuracy: 0.1632 - val_loss: 2.5627 - val_accuracy: 0.0893\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4751 - accuracy: 0.1727 - val_loss: 2.5454 - val_accuracy: 0.1071\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4535 - accuracy: 0.1794 - val_loss: 2.5402 - val_accuracy: 0.1429\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4337 - accuracy: 0.1880 - val_loss: 2.5391 - val_accuracy: 0.1607\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4140 - accuracy: 0.2004 - val_loss: 2.5267 - val_accuracy: 0.1250\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3956 - accuracy: 0.2080 - val_loss: 2.5340 - val_accuracy: 0.1250\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3751 - accuracy: 0.2032 - val_loss: 2.5294 - val_accuracy: 0.1250\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3581 - accuracy: 0.2185 - val_loss: 2.5327 - val_accuracy: 0.1071\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3407 - accuracy: 0.2252 - val_loss: 2.5339 - val_accuracy: 0.1250\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3208 - accuracy: 0.2319 - val_loss: 2.5339 - val_accuracy: 0.1071\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3061 - accuracy: 0.2414 - val_loss: 2.5386 - val_accuracy: 0.1071\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2890 - accuracy: 0.2510 - val_loss: 2.5398 - val_accuracy: 0.1071\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2720 - accuracy: 0.2624 - val_loss: 2.5518 - val_accuracy: 0.1250\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2555 - accuracy: 0.2662 - val_loss: 2.5462 - val_accuracy: 0.1071\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2414 - accuracy: 0.2672 - val_loss: 2.5540 - val_accuracy: 0.1250\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2243 - accuracy: 0.2739 - val_loss: 2.5634 - val_accuracy: 0.1250\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2131 - accuracy: 0.2786 - val_loss: 2.5624 - val_accuracy: 0.1250\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1959 - accuracy: 0.2815 - val_loss: 2.5787 - val_accuracy: 0.1071\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1845 - accuracy: 0.2863 - val_loss: 2.5838 - val_accuracy: 0.0893\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1691 - accuracy: 0.2929 - val_loss: 2.5783 - val_accuracy: 0.1071\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1562 - accuracy: 0.2996 - val_loss: 2.5883 - val_accuracy: 0.1071\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1435 - accuracy: 0.2996 - val_loss: 2.5970 - val_accuracy: 0.1071\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1309 - accuracy: 0.3015 - val_loss: 2.5988 - val_accuracy: 0.1071\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1182 - accuracy: 0.3044 - val_loss: 2.6080 - val_accuracy: 0.0893\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1042 - accuracy: 0.3197 - val_loss: 2.6043 - val_accuracy: 0.1071\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0911 - accuracy: 0.3263 - val_loss: 2.6172 - val_accuracy: 0.0714\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0836 - accuracy: 0.3311 - val_loss: 2.6199 - val_accuracy: 0.0893\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0686 - accuracy: 0.3330 - val_loss: 2.6298 - val_accuracy: 0.1071\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0578 - accuracy: 0.3368 - val_loss: 2.6309 - val_accuracy: 0.1071\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0452 - accuracy: 0.3426 - val_loss: 2.6500 - val_accuracy: 0.1071\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0349 - accuracy: 0.3378 - val_loss: 2.6380 - val_accuracy: 0.1071\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0219 - accuracy: 0.3502 - val_loss: 2.6528 - val_accuracy: 0.1071\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0142 - accuracy: 0.3521 - val_loss: 2.6644 - val_accuracy: 0.0893\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0002 - accuracy: 0.3559 - val_loss: 2.6730 - val_accuracy: 0.0893\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9892 - accuracy: 0.3607 - val_loss: 2.6755 - val_accuracy: 0.1071\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9797 - accuracy: 0.3626 - val_loss: 2.6946 - val_accuracy: 0.0714\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9673 - accuracy: 0.3750 - val_loss: 2.6898 - val_accuracy: 0.0714\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9568 - accuracy: 0.3721 - val_loss: 2.6958 - val_accuracy: 0.0893\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9463 - accuracy: 0.3721 - val_loss: 2.7154 - val_accuracy: 0.0714\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9329 - accuracy: 0.3788 - val_loss: 2.7201 - val_accuracy: 0.1071\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9243 - accuracy: 0.3941 - val_loss: 2.7268 - val_accuracy: 0.0714\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9141 - accuracy: 0.3950 - val_loss: 2.7444 - val_accuracy: 0.0893\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9031 - accuracy: 0.4065 - val_loss: 2.7416 - val_accuracy: 0.1071\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8947 - accuracy: 0.3979 - val_loss: 2.7518 - val_accuracy: 0.0893\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8837 - accuracy: 0.4036 - val_loss: 2.7630 - val_accuracy: 0.1250\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8728 - accuracy: 0.4074 - val_loss: 2.7728 - val_accuracy: 0.0893\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8656 - accuracy: 0.4189 - val_loss: 2.7960 - val_accuracy: 0.1071\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8565 - accuracy: 0.4065 - val_loss: 2.8005 - val_accuracy: 0.0893\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8467 - accuracy: 0.4113 - val_loss: 2.8089 - val_accuracy: 0.1071\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8352 - accuracy: 0.4170 - val_loss: 2.8243 - val_accuracy: 0.1250\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8227 - accuracy: 0.4265 - val_loss: 2.8341 - val_accuracy: 0.0714\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8149 - accuracy: 0.4332 - val_loss: 2.8541 - val_accuracy: 0.1250\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8046 - accuracy: 0.4275 - val_loss: 2.8600 - val_accuracy: 0.0714\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7970 - accuracy: 0.4332 - val_loss: 2.8703 - val_accuracy: 0.0893\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7902 - accuracy: 0.4361 - val_loss: 2.8612 - val_accuracy: 0.0714\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7780 - accuracy: 0.4370 - val_loss: 2.8857 - val_accuracy: 0.0536\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7687 - accuracy: 0.4342 - val_loss: 2.9089 - val_accuracy: 0.0536\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7597 - accuracy: 0.4380 - val_loss: 2.9040 - val_accuracy: 0.0893\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7499 - accuracy: 0.4447 - val_loss: 2.9146 - val_accuracy: 0.0536\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7414 - accuracy: 0.4437 - val_loss: 2.9388 - val_accuracy: 0.0893\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7362 - accuracy: 0.4466 - val_loss: 2.9192 - val_accuracy: 0.0536\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7245 - accuracy: 0.4590 - val_loss: 2.9580 - val_accuracy: 0.0893\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7187 - accuracy: 0.4542 - val_loss: 2.9495 - val_accuracy: 0.0714\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7092 - accuracy: 0.4618 - val_loss: 2.9750 - val_accuracy: 0.0714\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7004 - accuracy: 0.4571 - val_loss: 2.9667 - val_accuracy: 0.0714\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6923 - accuracy: 0.4590 - val_loss: 2.9825 - val_accuracy: 0.0893\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6841 - accuracy: 0.4666 - val_loss: 2.9773 - val_accuracy: 0.0714\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6751 - accuracy: 0.4714 - val_loss: 3.0100 - val_accuracy: 0.0714\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6670 - accuracy: 0.4723 - val_loss: 3.0195 - val_accuracy: 0.0714\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6606 - accuracy: 0.4723 - val_loss: 3.0254 - val_accuracy: 0.0714\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6566 - accuracy: 0.4771 - val_loss: 3.0483 - val_accuracy: 0.0714\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6446 - accuracy: 0.4819 - val_loss: 3.0444 - val_accuracy: 0.0714\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6365 - accuracy: 0.4876 - val_loss: 3.0589 - val_accuracy: 0.0893\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6281 - accuracy: 0.4876 - val_loss: 3.0671 - val_accuracy: 0.0714\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6210 - accuracy: 0.4914 - val_loss: 3.0892 - val_accuracy: 0.0714\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6140 - accuracy: 0.4905 - val_loss: 3.1034 - val_accuracy: 0.0714\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6093 - accuracy: 0.4943 - val_loss: 3.1278 - val_accuracy: 0.0893\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5998 - accuracy: 0.4895 - val_loss: 3.1337 - val_accuracy: 0.0714\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5928 - accuracy: 0.4981 - val_loss: 3.1480 - val_accuracy: 0.0714\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5863 - accuracy: 0.4981 - val_loss: 3.1516 - val_accuracy: 0.0714\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5804 - accuracy: 0.4952 - val_loss: 3.1726 - val_accuracy: 0.0893\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5711 - accuracy: 0.5038 - val_loss: 3.1726 - val_accuracy: 0.0893\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5632 - accuracy: 0.5086 - val_loss: 3.2017 - val_accuracy: 0.1071\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5583 - accuracy: 0.5029 - val_loss: 3.2039 - val_accuracy: 0.0893\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5494 - accuracy: 0.5048 - val_loss: 3.2011 - val_accuracy: 0.0893\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5425 - accuracy: 0.5057 - val_loss: 3.2305 - val_accuracy: 0.0893\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5369 - accuracy: 0.5153 - val_loss: 3.2620 - val_accuracy: 0.0893\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5329 - accuracy: 0.5124 - val_loss: 3.2515 - val_accuracy: 0.0893\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5233 - accuracy: 0.5191 - val_loss: 3.2825 - val_accuracy: 0.0893\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5150 - accuracy: 0.5267 - val_loss: 3.3030 - val_accuracy: 0.0893\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5138 - accuracy: 0.5258 - val_loss: 3.3044 - val_accuracy: 0.0893\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5043 - accuracy: 0.5162 - val_loss: 3.3135 - val_accuracy: 0.0893\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4979 - accuracy: 0.5315 - val_loss: 3.3369 - val_accuracy: 0.1071\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4906 - accuracy: 0.5353 - val_loss: 3.3359 - val_accuracy: 0.1071\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4868 - accuracy: 0.5258 - val_loss: 3.3419 - val_accuracy: 0.1071\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 3.4294 - accuracy: 0.1558\n",
      "\n",
      "Test accuracy: 0.1557970941066742\n",
      "var: 0.991\n",
      "(1380, 23)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.7340 - accuracy: 0.0677 - val_loss: 2.6146 - val_accuracy: 0.0714\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.6313 - accuracy: 0.0906 - val_loss: 2.5934 - val_accuracy: 0.0536\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5800 - accuracy: 0.1221 - val_loss: 2.5869 - val_accuracy: 0.1071\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5422 - accuracy: 0.1355 - val_loss: 2.5797 - val_accuracy: 0.1250\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5141 - accuracy: 0.1584 - val_loss: 2.5739 - val_accuracy: 0.1429\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4899 - accuracy: 0.1574 - val_loss: 2.5683 - val_accuracy: 0.1250\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4683 - accuracy: 0.1632 - val_loss: 2.5738 - val_accuracy: 0.1250\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4490 - accuracy: 0.1784 - val_loss: 2.5726 - val_accuracy: 0.1250\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4308 - accuracy: 0.1880 - val_loss: 2.5730 - val_accuracy: 0.1429\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4138 - accuracy: 0.1956 - val_loss: 2.5760 - val_accuracy: 0.1429\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3980 - accuracy: 0.2004 - val_loss: 2.5761 - val_accuracy: 0.1607\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3795 - accuracy: 0.2080 - val_loss: 2.5776 - val_accuracy: 0.1429\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3640 - accuracy: 0.2214 - val_loss: 2.5738 - val_accuracy: 0.1607\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3484 - accuracy: 0.2252 - val_loss: 2.5779 - val_accuracy: 0.1607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3302 - accuracy: 0.2319 - val_loss: 2.5870 - val_accuracy: 0.1786\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3132 - accuracy: 0.2366 - val_loss: 2.5974 - val_accuracy: 0.1786\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2980 - accuracy: 0.2452 - val_loss: 2.5960 - val_accuracy: 0.1786\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2814 - accuracy: 0.2452 - val_loss: 2.6016 - val_accuracy: 0.1607\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2661 - accuracy: 0.2481 - val_loss: 2.6163 - val_accuracy: 0.1786\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2491 - accuracy: 0.2605 - val_loss: 2.6194 - val_accuracy: 0.1786\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2313 - accuracy: 0.2634 - val_loss: 2.6304 - val_accuracy: 0.1964\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2130 - accuracy: 0.2777 - val_loss: 2.6400 - val_accuracy: 0.1786\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1966 - accuracy: 0.2729 - val_loss: 2.6515 - val_accuracy: 0.1429\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1787 - accuracy: 0.2853 - val_loss: 2.6633 - val_accuracy: 0.1429\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1644 - accuracy: 0.2844 - val_loss: 2.6689 - val_accuracy: 0.1607\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1476 - accuracy: 0.2996 - val_loss: 2.6827 - val_accuracy: 0.1429\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1273 - accuracy: 0.3044 - val_loss: 2.6882 - val_accuracy: 0.1607\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1147 - accuracy: 0.3063 - val_loss: 2.6946 - val_accuracy: 0.1607\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1020 - accuracy: 0.3177 - val_loss: 2.7134 - val_accuracy: 0.1429\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0841 - accuracy: 0.3177 - val_loss: 2.7094 - val_accuracy: 0.1250\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0709 - accuracy: 0.3197 - val_loss: 2.7269 - val_accuracy: 0.1250\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0564 - accuracy: 0.3273 - val_loss: 2.7491 - val_accuracy: 0.1250\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0440 - accuracy: 0.3282 - val_loss: 2.7568 - val_accuracy: 0.1429\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0289 - accuracy: 0.3416 - val_loss: 2.7617 - val_accuracy: 0.1429\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0161 - accuracy: 0.3454 - val_loss: 2.7693 - val_accuracy: 0.1250\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0032 - accuracy: 0.3483 - val_loss: 2.7925 - val_accuracy: 0.1071\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9928 - accuracy: 0.3550 - val_loss: 2.7885 - val_accuracy: 0.1250\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9804 - accuracy: 0.3540 - val_loss: 2.8061 - val_accuracy: 0.1071\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9689 - accuracy: 0.3616 - val_loss: 2.8107 - val_accuracy: 0.1429\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9545 - accuracy: 0.3674 - val_loss: 2.8353 - val_accuracy: 0.1250\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9423 - accuracy: 0.3798 - val_loss: 2.8360 - val_accuracy: 0.1250\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9322 - accuracy: 0.3865 - val_loss: 2.8484 - val_accuracy: 0.1250\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9179 - accuracy: 0.3750 - val_loss: 2.8601 - val_accuracy: 0.1250\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9075 - accuracy: 0.3826 - val_loss: 2.8836 - val_accuracy: 0.1250\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9000 - accuracy: 0.3884 - val_loss: 2.8798 - val_accuracy: 0.1071\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8855 - accuracy: 0.3931 - val_loss: 2.8897 - val_accuracy: 0.1071\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8744 - accuracy: 0.4008 - val_loss: 2.8956 - val_accuracy: 0.1071\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8644 - accuracy: 0.3941 - val_loss: 2.9260 - val_accuracy: 0.1071\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8526 - accuracy: 0.4055 - val_loss: 2.9285 - val_accuracy: 0.1071\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8427 - accuracy: 0.4094 - val_loss: 2.9288 - val_accuracy: 0.1250\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8318 - accuracy: 0.4113 - val_loss: 2.9490 - val_accuracy: 0.1250\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8242 - accuracy: 0.4008 - val_loss: 2.9526 - val_accuracy: 0.1071\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8148 - accuracy: 0.4179 - val_loss: 2.9519 - val_accuracy: 0.0714\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8011 - accuracy: 0.4265 - val_loss: 2.9805 - val_accuracy: 0.1071\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7920 - accuracy: 0.4246 - val_loss: 2.9911 - val_accuracy: 0.1071\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7826 - accuracy: 0.4275 - val_loss: 2.9783 - val_accuracy: 0.0893\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7749 - accuracy: 0.4275 - val_loss: 3.0159 - val_accuracy: 0.0714\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7635 - accuracy: 0.4303 - val_loss: 3.0255 - val_accuracy: 0.0714\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7532 - accuracy: 0.4313 - val_loss: 3.0400 - val_accuracy: 0.0714\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7452 - accuracy: 0.4342 - val_loss: 3.0347 - val_accuracy: 0.0536\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7349 - accuracy: 0.4370 - val_loss: 3.0493 - val_accuracy: 0.0893\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7255 - accuracy: 0.4447 - val_loss: 3.0832 - val_accuracy: 0.0714\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7153 - accuracy: 0.4437 - val_loss: 3.0718 - val_accuracy: 0.0536\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7055 - accuracy: 0.4590 - val_loss: 3.1181 - val_accuracy: 0.0536\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7017 - accuracy: 0.4523 - val_loss: 3.1146 - val_accuracy: 0.0893\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6900 - accuracy: 0.4590 - val_loss: 3.1075 - val_accuracy: 0.0714\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6795 - accuracy: 0.4609 - val_loss: 3.1421 - val_accuracy: 0.0536\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6716 - accuracy: 0.4637 - val_loss: 3.1333 - val_accuracy: 0.0893\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6638 - accuracy: 0.4723 - val_loss: 3.1730 - val_accuracy: 0.1071\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6556 - accuracy: 0.4628 - val_loss: 3.1864 - val_accuracy: 0.0714\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6482 - accuracy: 0.4704 - val_loss: 3.1757 - val_accuracy: 0.0536\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6384 - accuracy: 0.4781 - val_loss: 3.2162 - val_accuracy: 0.0536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6309 - accuracy: 0.4790 - val_loss: 3.2190 - val_accuracy: 0.0536\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6203 - accuracy: 0.4761 - val_loss: 3.2197 - val_accuracy: 0.0536\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6133 - accuracy: 0.4885 - val_loss: 3.2441 - val_accuracy: 0.0714\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6066 - accuracy: 0.4781 - val_loss: 3.2610 - val_accuracy: 0.0536\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6008 - accuracy: 0.4943 - val_loss: 3.2518 - val_accuracy: 0.0536\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5914 - accuracy: 0.4885 - val_loss: 3.2879 - val_accuracy: 0.0714\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5860 - accuracy: 0.4971 - val_loss: 3.2909 - val_accuracy: 0.0714\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5765 - accuracy: 0.5000 - val_loss: 3.2999 - val_accuracy: 0.0893\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5689 - accuracy: 0.5000 - val_loss: 3.3186 - val_accuracy: 0.0714\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5592 - accuracy: 0.5076 - val_loss: 3.3181 - val_accuracy: 0.0536\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5548 - accuracy: 0.5057 - val_loss: 3.3426 - val_accuracy: 0.0714\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5477 - accuracy: 0.5115 - val_loss: 3.3472 - val_accuracy: 0.0714\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5372 - accuracy: 0.5134 - val_loss: 3.3562 - val_accuracy: 0.0714\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5330 - accuracy: 0.5143 - val_loss: 3.4100 - val_accuracy: 0.0536\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5281 - accuracy: 0.5172 - val_loss: 3.4055 - val_accuracy: 0.0714\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5193 - accuracy: 0.5200 - val_loss: 3.4165 - val_accuracy: 0.0714\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5103 - accuracy: 0.5296 - val_loss: 3.4118 - val_accuracy: 0.0714\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5049 - accuracy: 0.5353 - val_loss: 3.4459 - val_accuracy: 0.0714\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4993 - accuracy: 0.5286 - val_loss: 3.4445 - val_accuracy: 0.0714\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4909 - accuracy: 0.5391 - val_loss: 3.4630 - val_accuracy: 0.0714\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4866 - accuracy: 0.5344 - val_loss: 3.4912 - val_accuracy: 0.0536\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4791 - accuracy: 0.5382 - val_loss: 3.4711 - val_accuracy: 0.0536\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4719 - accuracy: 0.5372 - val_loss: 3.4951 - val_accuracy: 0.0714\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4652 - accuracy: 0.5448 - val_loss: 3.5328 - val_accuracy: 0.0714\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4576 - accuracy: 0.5553 - val_loss: 3.5258 - val_accuracy: 0.0536\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4564 - accuracy: 0.5506 - val_loss: 3.5329 - val_accuracy: 0.0714\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4485 - accuracy: 0.5515 - val_loss: 3.5751 - val_accuracy: 0.0714\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4437 - accuracy: 0.5573 - val_loss: 3.5569 - val_accuracy: 0.0714\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3.5697 - accuracy: 0.1304\n",
      "\n",
      "Test accuracy: 0.1304347813129425\n",
      "var: 0.992\n",
      "(1380, 23)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 14ms/step - loss: 2.7305 - accuracy: 0.0773 - val_loss: 2.6916 - val_accuracy: 0.0714\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.6307 - accuracy: 0.0964 - val_loss: 2.6405 - val_accuracy: 0.0893\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5855 - accuracy: 0.1193 - val_loss: 2.5951 - val_accuracy: 0.1071\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5504 - accuracy: 0.1365 - val_loss: 2.5632 - val_accuracy: 0.0893\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5209 - accuracy: 0.1460 - val_loss: 2.5437 - val_accuracy: 0.1071\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4946 - accuracy: 0.1594 - val_loss: 2.5273 - val_accuracy: 0.0893\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4721 - accuracy: 0.1746 - val_loss: 2.5166 - val_accuracy: 0.0893\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4500 - accuracy: 0.1851 - val_loss: 2.5026 - val_accuracy: 0.1250\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4292 - accuracy: 0.1880 - val_loss: 2.5036 - val_accuracy: 0.1250\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4104 - accuracy: 0.2090 - val_loss: 2.4887 - val_accuracy: 0.1429\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3917 - accuracy: 0.2042 - val_loss: 2.4924 - val_accuracy: 0.1071\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3745 - accuracy: 0.2176 - val_loss: 2.4891 - val_accuracy: 0.0893\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3571 - accuracy: 0.2185 - val_loss: 2.4842 - val_accuracy: 0.1071\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3389 - accuracy: 0.2376 - val_loss: 2.4791 - val_accuracy: 0.0893\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3218 - accuracy: 0.2433 - val_loss: 2.4871 - val_accuracy: 0.1071\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3083 - accuracy: 0.2538 - val_loss: 2.4840 - val_accuracy: 0.1250\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2910 - accuracy: 0.2557 - val_loss: 2.4857 - val_accuracy: 0.1250\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2763 - accuracy: 0.2519 - val_loss: 2.4836 - val_accuracy: 0.1607\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2611 - accuracy: 0.2595 - val_loss: 2.4909 - val_accuracy: 0.1429\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2484 - accuracy: 0.2576 - val_loss: 2.4891 - val_accuracy: 0.1429\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2324 - accuracy: 0.2634 - val_loss: 2.4926 - val_accuracy: 0.1607\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2199 - accuracy: 0.2691 - val_loss: 2.4951 - val_accuracy: 0.1607\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2050 - accuracy: 0.2719 - val_loss: 2.5038 - val_accuracy: 0.1786\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1918 - accuracy: 0.2729 - val_loss: 2.5071 - val_accuracy: 0.1964\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1776 - accuracy: 0.2824 - val_loss: 2.5013 - val_accuracy: 0.1964\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1649 - accuracy: 0.2767 - val_loss: 2.5139 - val_accuracy: 0.1786\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1503 - accuracy: 0.2872 - val_loss: 2.5114 - val_accuracy: 0.1786\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1393 - accuracy: 0.2824 - val_loss: 2.5167 - val_accuracy: 0.1786\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1267 - accuracy: 0.2920 - val_loss: 2.5232 - val_accuracy: 0.1786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1134 - accuracy: 0.2968 - val_loss: 2.5310 - val_accuracy: 0.1607\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1028 - accuracy: 0.2939 - val_loss: 2.5344 - val_accuracy: 0.1964\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0902 - accuracy: 0.3063 - val_loss: 2.5420 - val_accuracy: 0.1964\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0787 - accuracy: 0.2968 - val_loss: 2.5476 - val_accuracy: 0.1964\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0671 - accuracy: 0.3101 - val_loss: 2.5500 - val_accuracy: 0.1964\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0536 - accuracy: 0.3044 - val_loss: 2.5569 - val_accuracy: 0.1964\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0430 - accuracy: 0.3120 - val_loss: 2.5627 - val_accuracy: 0.1964\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0283 - accuracy: 0.3158 - val_loss: 2.5679 - val_accuracy: 0.1786\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0204 - accuracy: 0.3273 - val_loss: 2.5819 - val_accuracy: 0.1964\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0051 - accuracy: 0.3292 - val_loss: 2.5814 - val_accuracy: 0.2143\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9964 - accuracy: 0.3359 - val_loss: 2.5932 - val_accuracy: 0.1964\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9864 - accuracy: 0.3387 - val_loss: 2.5999 - val_accuracy: 0.2321\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9711 - accuracy: 0.3511 - val_loss: 2.6109 - val_accuracy: 0.2321\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9601 - accuracy: 0.3502 - val_loss: 2.6102 - val_accuracy: 0.1964\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9482 - accuracy: 0.3540 - val_loss: 2.6201 - val_accuracy: 0.2143\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9344 - accuracy: 0.3664 - val_loss: 2.6250 - val_accuracy: 0.2143\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9235 - accuracy: 0.3655 - val_loss: 2.6372 - val_accuracy: 0.2321\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9129 - accuracy: 0.3731 - val_loss: 2.6443 - val_accuracy: 0.1964\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9030 - accuracy: 0.3693 - val_loss: 2.6578 - val_accuracy: 0.2321\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8925 - accuracy: 0.3836 - val_loss: 2.6640 - val_accuracy: 0.2321\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8819 - accuracy: 0.3855 - val_loss: 2.6729 - val_accuracy: 0.2143\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8711 - accuracy: 0.3979 - val_loss: 2.6809 - val_accuracy: 0.2321\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8603 - accuracy: 0.3989 - val_loss: 2.6876 - val_accuracy: 0.2321\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8486 - accuracy: 0.3979 - val_loss: 2.7021 - val_accuracy: 0.2143\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8410 - accuracy: 0.4008 - val_loss: 2.7185 - val_accuracy: 0.1964\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8324 - accuracy: 0.4065 - val_loss: 2.7354 - val_accuracy: 0.2143\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8204 - accuracy: 0.4160 - val_loss: 2.7405 - val_accuracy: 0.1964\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8116 - accuracy: 0.4141 - val_loss: 2.7452 - val_accuracy: 0.2143\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8009 - accuracy: 0.4103 - val_loss: 2.7557 - val_accuracy: 0.2321\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.7925 - accuracy: 0.4227 - val_loss: 2.7698 - val_accuracy: 0.2143\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7870 - accuracy: 0.4198 - val_loss: 2.7903 - val_accuracy: 0.1964\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7737 - accuracy: 0.4227 - val_loss: 2.7999 - val_accuracy: 0.1964\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7671 - accuracy: 0.4294 - val_loss: 2.8136 - val_accuracy: 0.1964\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7544 - accuracy: 0.4303 - val_loss: 2.8309 - val_accuracy: 0.1964\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7460 - accuracy: 0.4361 - val_loss: 2.8265 - val_accuracy: 0.1964\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7364 - accuracy: 0.4323 - val_loss: 2.8427 - val_accuracy: 0.1786\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7264 - accuracy: 0.4284 - val_loss: 2.8681 - val_accuracy: 0.1786\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7201 - accuracy: 0.4408 - val_loss: 2.8822 - val_accuracy: 0.1964\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7115 - accuracy: 0.4408 - val_loss: 2.8887 - val_accuracy: 0.1786\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7029 - accuracy: 0.4427 - val_loss: 2.9056 - val_accuracy: 0.1786\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6924 - accuracy: 0.4504 - val_loss: 2.9048 - val_accuracy: 0.1964\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6871 - accuracy: 0.4475 - val_loss: 2.9190 - val_accuracy: 0.1964\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6780 - accuracy: 0.4561 - val_loss: 2.9439 - val_accuracy: 0.2143\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6721 - accuracy: 0.4552 - val_loss: 2.9538 - val_accuracy: 0.1786\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6649 - accuracy: 0.4590 - val_loss: 2.9794 - val_accuracy: 0.1964\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.6535 - accuracy: 0.4599 - val_loss: 2.9804 - val_accuracy: 0.1786\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6460 - accuracy: 0.4676 - val_loss: 3.0039 - val_accuracy: 0.1964\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6390 - accuracy: 0.4790 - val_loss: 3.0209 - val_accuracy: 0.1964\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6332 - accuracy: 0.4733 - val_loss: 3.0188 - val_accuracy: 0.1964\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6219 - accuracy: 0.4809 - val_loss: 3.0549 - val_accuracy: 0.1964\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6174 - accuracy: 0.4847 - val_loss: 3.0661 - val_accuracy: 0.2143\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6083 - accuracy: 0.4847 - val_loss: 3.0647 - val_accuracy: 0.1964\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6002 - accuracy: 0.4924 - val_loss: 3.0838 - val_accuracy: 0.2143\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5909 - accuracy: 0.4924 - val_loss: 3.1010 - val_accuracy: 0.2143\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5845 - accuracy: 0.4962 - val_loss: 3.1243 - val_accuracy: 0.1786\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5763 - accuracy: 0.4971 - val_loss: 3.1407 - val_accuracy: 0.2321\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5668 - accuracy: 0.5010 - val_loss: 3.1542 - val_accuracy: 0.2143\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5663 - accuracy: 0.5019 - val_loss: 3.1578 - val_accuracy: 0.2321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5552 - accuracy: 0.5048 - val_loss: 3.1608 - val_accuracy: 0.2321\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5460 - accuracy: 0.5057 - val_loss: 3.1845 - val_accuracy: 0.2143\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5420 - accuracy: 0.5029 - val_loss: 3.2052 - val_accuracy: 0.1964\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5340 - accuracy: 0.5086 - val_loss: 3.1982 - val_accuracy: 0.2321\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5272 - accuracy: 0.5143 - val_loss: 3.2251 - val_accuracy: 0.2143\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5189 - accuracy: 0.5181 - val_loss: 3.2287 - val_accuracy: 0.2143\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5136 - accuracy: 0.5124 - val_loss: 3.2489 - val_accuracy: 0.1964\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5050 - accuracy: 0.5219 - val_loss: 3.2539 - val_accuracy: 0.1964\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5014 - accuracy: 0.5162 - val_loss: 3.2659 - val_accuracy: 0.2143\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4901 - accuracy: 0.5191 - val_loss: 3.2767 - val_accuracy: 0.2143\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4851 - accuracy: 0.5305 - val_loss: 3.2957 - val_accuracy: 0.2143\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4776 - accuracy: 0.5258 - val_loss: 3.3130 - val_accuracy: 0.2143\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4724 - accuracy: 0.5334 - val_loss: 3.3194 - val_accuracy: 0.2143\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 3.5154 - accuracy: 0.1232\n",
      "\n",
      "Test accuracy: 0.12318840622901917\n",
      "var: 0.993\n",
      "(1380, 23)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 14ms/step - loss: 2.7225 - accuracy: 0.0763 - val_loss: 2.5743 - val_accuracy: 0.0893\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.6328 - accuracy: 0.0802 - val_loss: 2.5579 - val_accuracy: 0.1250\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.5885 - accuracy: 0.1059 - val_loss: 2.5423 - val_accuracy: 0.1429\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.5522 - accuracy: 0.1231 - val_loss: 2.5388 - val_accuracy: 0.1964\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5220 - accuracy: 0.1450 - val_loss: 2.5219 - val_accuracy: 0.1250\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4933 - accuracy: 0.1508 - val_loss: 2.5153 - val_accuracy: 0.1607\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4700 - accuracy: 0.1574 - val_loss: 2.5125 - val_accuracy: 0.1607\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4481 - accuracy: 0.1708 - val_loss: 2.5096 - val_accuracy: 0.1607\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4259 - accuracy: 0.1775 - val_loss: 2.5166 - val_accuracy: 0.1607\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.4057 - accuracy: 0.1851 - val_loss: 2.5152 - val_accuracy: 0.1607\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3903 - accuracy: 0.1937 - val_loss: 2.5135 - val_accuracy: 0.1429\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3698 - accuracy: 0.2099 - val_loss: 2.5190 - val_accuracy: 0.1429\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3513 - accuracy: 0.2118 - val_loss: 2.5240 - val_accuracy: 0.1071\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3350 - accuracy: 0.2156 - val_loss: 2.5280 - val_accuracy: 0.1250\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3173 - accuracy: 0.2281 - val_loss: 2.5300 - val_accuracy: 0.1429\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3013 - accuracy: 0.2281 - val_loss: 2.5366 - val_accuracy: 0.1429\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2850 - accuracy: 0.2462 - val_loss: 2.5356 - val_accuracy: 0.1250\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2681 - accuracy: 0.2462 - val_loss: 2.5464 - val_accuracy: 0.1429\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2513 - accuracy: 0.2538 - val_loss: 2.5510 - val_accuracy: 0.1429\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2366 - accuracy: 0.2557 - val_loss: 2.5469 - val_accuracy: 0.1429\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2202 - accuracy: 0.2615 - val_loss: 2.5677 - val_accuracy: 0.1429\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2058 - accuracy: 0.2681 - val_loss: 2.5730 - val_accuracy: 0.1429\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1908 - accuracy: 0.2710 - val_loss: 2.5873 - val_accuracy: 0.1429\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1753 - accuracy: 0.2824 - val_loss: 2.5921 - val_accuracy: 0.1250\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1617 - accuracy: 0.2834 - val_loss: 2.6041 - val_accuracy: 0.1250\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1490 - accuracy: 0.2901 - val_loss: 2.6147 - val_accuracy: 0.1250\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1332 - accuracy: 0.2948 - val_loss: 2.6369 - val_accuracy: 0.1250\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1228 - accuracy: 0.2987 - val_loss: 2.6438 - val_accuracy: 0.1250\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1103 - accuracy: 0.3063 - val_loss: 2.6608 - val_accuracy: 0.1250\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0961 - accuracy: 0.3130 - val_loss: 2.6623 - val_accuracy: 0.1429\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0817 - accuracy: 0.3158 - val_loss: 2.6679 - val_accuracy: 0.1429\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0674 - accuracy: 0.3282 - val_loss: 2.6904 - val_accuracy: 0.1429\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0563 - accuracy: 0.3302 - val_loss: 2.7005 - val_accuracy: 0.1429\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0461 - accuracy: 0.3330 - val_loss: 2.7016 - val_accuracy: 0.1429\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0319 - accuracy: 0.3311 - val_loss: 2.7265 - val_accuracy: 0.1429\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0208 - accuracy: 0.3368 - val_loss: 2.7252 - val_accuracy: 0.1429\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0112 - accuracy: 0.3416 - val_loss: 2.7407 - val_accuracy: 0.1607\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9996 - accuracy: 0.3416 - val_loss: 2.7623 - val_accuracy: 0.1429\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9874 - accuracy: 0.3531 - val_loss: 2.7730 - val_accuracy: 0.1250\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9748 - accuracy: 0.3502 - val_loss: 2.7835 - val_accuracy: 0.1429\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9659 - accuracy: 0.3531 - val_loss: 2.8047 - val_accuracy: 0.1429\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9537 - accuracy: 0.3578 - val_loss: 2.8003 - val_accuracy: 0.1429\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9443 - accuracy: 0.3626 - val_loss: 2.8263 - val_accuracy: 0.1250\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9333 - accuracy: 0.3721 - val_loss: 2.8396 - val_accuracy: 0.1429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9233 - accuracy: 0.3702 - val_loss: 2.8431 - val_accuracy: 0.1429\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9116 - accuracy: 0.3760 - val_loss: 2.8626 - val_accuracy: 0.1429\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8995 - accuracy: 0.3826 - val_loss: 2.8635 - val_accuracy: 0.1429\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8921 - accuracy: 0.3788 - val_loss: 2.8874 - val_accuracy: 0.1429\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8804 - accuracy: 0.3845 - val_loss: 2.8862 - val_accuracy: 0.1429\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8703 - accuracy: 0.3893 - val_loss: 2.8992 - val_accuracy: 0.1429\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8610 - accuracy: 0.3884 - val_loss: 2.8993 - val_accuracy: 0.1429\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8532 - accuracy: 0.3922 - val_loss: 2.9325 - val_accuracy: 0.1429\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8426 - accuracy: 0.3941 - val_loss: 2.9159 - val_accuracy: 0.1429\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8325 - accuracy: 0.3931 - val_loss: 2.9289 - val_accuracy: 0.1250\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8234 - accuracy: 0.4017 - val_loss: 2.9559 - val_accuracy: 0.1250\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8175 - accuracy: 0.4046 - val_loss: 2.9451 - val_accuracy: 0.1429\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8075 - accuracy: 0.4017 - val_loss: 2.9914 - val_accuracy: 0.1250\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7977 - accuracy: 0.4103 - val_loss: 2.9767 - val_accuracy: 0.1429\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7899 - accuracy: 0.4122 - val_loss: 2.9915 - val_accuracy: 0.1250\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7775 - accuracy: 0.4170 - val_loss: 2.9893 - val_accuracy: 0.1071\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7729 - accuracy: 0.4227 - val_loss: 3.0145 - val_accuracy: 0.1250\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7644 - accuracy: 0.4294 - val_loss: 3.0258 - val_accuracy: 0.1071\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7568 - accuracy: 0.4303 - val_loss: 3.0278 - val_accuracy: 0.1250\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7491 - accuracy: 0.4303 - val_loss: 3.0424 - val_accuracy: 0.0893\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7367 - accuracy: 0.4380 - val_loss: 3.0379 - val_accuracy: 0.1071\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7305 - accuracy: 0.4332 - val_loss: 3.0576 - val_accuracy: 0.0893\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7218 - accuracy: 0.4456 - val_loss: 3.0676 - val_accuracy: 0.0893\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7162 - accuracy: 0.4418 - val_loss: 3.0924 - val_accuracy: 0.0893\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7030 - accuracy: 0.4427 - val_loss: 3.1162 - val_accuracy: 0.0893\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6954 - accuracy: 0.4609 - val_loss: 3.0930 - val_accuracy: 0.1071\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6898 - accuracy: 0.4485 - val_loss: 3.1302 - val_accuracy: 0.0893\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6813 - accuracy: 0.4590 - val_loss: 3.1303 - val_accuracy: 0.0893\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6706 - accuracy: 0.4513 - val_loss: 3.1308 - val_accuracy: 0.0893\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6627 - accuracy: 0.4609 - val_loss: 3.1469 - val_accuracy: 0.1071\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6556 - accuracy: 0.4637 - val_loss: 3.1577 - val_accuracy: 0.0893\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6503 - accuracy: 0.4647 - val_loss: 3.1532 - val_accuracy: 0.0893\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6419 - accuracy: 0.4685 - val_loss: 3.1771 - val_accuracy: 0.0893\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6348 - accuracy: 0.4685 - val_loss: 3.1940 - val_accuracy: 0.0893\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6269 - accuracy: 0.4761 - val_loss: 3.1899 - val_accuracy: 0.1250\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6173 - accuracy: 0.4819 - val_loss: 3.2138 - val_accuracy: 0.0893\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6096 - accuracy: 0.4800 - val_loss: 3.2192 - val_accuracy: 0.0893\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6014 - accuracy: 0.4771 - val_loss: 3.2190 - val_accuracy: 0.0893\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5955 - accuracy: 0.4962 - val_loss: 3.2518 - val_accuracy: 0.0893\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5831 - accuracy: 0.4838 - val_loss: 3.2660 - val_accuracy: 0.0893\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5801 - accuracy: 0.4866 - val_loss: 3.2583 - val_accuracy: 0.0893\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5746 - accuracy: 0.4943 - val_loss: 3.2743 - val_accuracy: 0.0893\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5679 - accuracy: 0.4933 - val_loss: 3.2744 - val_accuracy: 0.0893\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5581 - accuracy: 0.4924 - val_loss: 3.3071 - val_accuracy: 0.0893\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5476 - accuracy: 0.5019 - val_loss: 3.3154 - val_accuracy: 0.0893\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5436 - accuracy: 0.5067 - val_loss: 3.3228 - val_accuracy: 0.0893\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5361 - accuracy: 0.5048 - val_loss: 3.3446 - val_accuracy: 0.0893\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5265 - accuracy: 0.5086 - val_loss: 3.3220 - val_accuracy: 0.0893\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5226 - accuracy: 0.5124 - val_loss: 3.3624 - val_accuracy: 0.0893\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5126 - accuracy: 0.5219 - val_loss: 3.3602 - val_accuracy: 0.0893\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5046 - accuracy: 0.5057 - val_loss: 3.3964 - val_accuracy: 0.0893\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4982 - accuracy: 0.5200 - val_loss: 3.4024 - val_accuracy: 0.0893\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4899 - accuracy: 0.5172 - val_loss: 3.4134 - val_accuracy: 0.1071\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4828 - accuracy: 0.5248 - val_loss: 3.4101 - val_accuracy: 0.0893\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4759 - accuracy: 0.5277 - val_loss: 3.4329 - val_accuracy: 0.0893\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4674 - accuracy: 0.5229 - val_loss: 3.4340 - val_accuracy: 0.0893\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 3.5397 - accuracy: 0.1232\n",
      "\n",
      "Test accuracy: 0.12318840622901917\n",
      "var: 0.994\n",
      "(1380, 23)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.7012 - accuracy: 0.0687 - val_loss: 2.6448 - val_accuracy: 0.0536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.6314 - accuracy: 0.0897 - val_loss: 2.6250 - val_accuracy: 0.0714\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5953 - accuracy: 0.1135 - val_loss: 2.6062 - val_accuracy: 0.0893\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5655 - accuracy: 0.1374 - val_loss: 2.5907 - val_accuracy: 0.1071\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5384 - accuracy: 0.1536 - val_loss: 2.5883 - val_accuracy: 0.0893\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5149 - accuracy: 0.1613 - val_loss: 2.5784 - val_accuracy: 0.0893\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4906 - accuracy: 0.1632 - val_loss: 2.5652 - val_accuracy: 0.0893\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4709 - accuracy: 0.1746 - val_loss: 2.5625 - val_accuracy: 0.0357\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4509 - accuracy: 0.1784 - val_loss: 2.5617 - val_accuracy: 0.1071\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4319 - accuracy: 0.1870 - val_loss: 2.5568 - val_accuracy: 0.1071\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4132 - accuracy: 0.1956 - val_loss: 2.5539 - val_accuracy: 0.1071\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3941 - accuracy: 0.2032 - val_loss: 2.5532 - val_accuracy: 0.1250\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3768 - accuracy: 0.2109 - val_loss: 2.5495 - val_accuracy: 0.0893\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3608 - accuracy: 0.2147 - val_loss: 2.5444 - val_accuracy: 0.1071\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3419 - accuracy: 0.2176 - val_loss: 2.5471 - val_accuracy: 0.1429\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3278 - accuracy: 0.2271 - val_loss: 2.5451 - val_accuracy: 0.1607\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3122 - accuracy: 0.2328 - val_loss: 2.5469 - val_accuracy: 0.1250\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2924 - accuracy: 0.2395 - val_loss: 2.5495 - val_accuracy: 0.1429\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.2777 - accuracy: 0.2462 - val_loss: 2.5502 - val_accuracy: 0.1250\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2650 - accuracy: 0.2490 - val_loss: 2.5606 - val_accuracy: 0.1429\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2467 - accuracy: 0.2529 - val_loss: 2.5597 - val_accuracy: 0.1429\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2343 - accuracy: 0.2538 - val_loss: 2.5694 - val_accuracy: 0.1250\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2197 - accuracy: 0.2586 - val_loss: 2.5543 - val_accuracy: 0.1250\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2046 - accuracy: 0.2691 - val_loss: 2.5603 - val_accuracy: 0.1250\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1898 - accuracy: 0.2700 - val_loss: 2.5814 - val_accuracy: 0.1429\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1767 - accuracy: 0.2739 - val_loss: 2.5650 - val_accuracy: 0.1250\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1617 - accuracy: 0.2777 - val_loss: 2.5778 - val_accuracy: 0.1429\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1496 - accuracy: 0.2767 - val_loss: 2.5799 - val_accuracy: 0.1250\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1362 - accuracy: 0.2948 - val_loss: 2.5952 - val_accuracy: 0.1071\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1226 - accuracy: 0.2910 - val_loss: 2.5922 - val_accuracy: 0.1071\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1108 - accuracy: 0.3025 - val_loss: 2.5884 - val_accuracy: 0.1250\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0972 - accuracy: 0.3177 - val_loss: 2.5943 - val_accuracy: 0.1071\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0836 - accuracy: 0.3130 - val_loss: 2.6037 - val_accuracy: 0.0893\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0704 - accuracy: 0.3168 - val_loss: 2.6033 - val_accuracy: 0.1071\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0566 - accuracy: 0.3292 - val_loss: 2.6128 - val_accuracy: 0.0893\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0484 - accuracy: 0.3349 - val_loss: 2.6234 - val_accuracy: 0.1071\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0336 - accuracy: 0.3349 - val_loss: 2.6115 - val_accuracy: 0.0893\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0218 - accuracy: 0.3445 - val_loss: 2.6299 - val_accuracy: 0.0893\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0109 - accuracy: 0.3511 - val_loss: 2.6410 - val_accuracy: 0.0893\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9968 - accuracy: 0.3607 - val_loss: 2.6549 - val_accuracy: 0.1071\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9847 - accuracy: 0.3607 - val_loss: 2.6532 - val_accuracy: 0.0893\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.9754 - accuracy: 0.3607 - val_loss: 2.6621 - val_accuracy: 0.0893\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9622 - accuracy: 0.3845 - val_loss: 2.6712 - val_accuracy: 0.0893\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9513 - accuracy: 0.3807 - val_loss: 2.6780 - val_accuracy: 0.0893\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9425 - accuracy: 0.3884 - val_loss: 2.6795 - val_accuracy: 0.1071\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9308 - accuracy: 0.3845 - val_loss: 2.7005 - val_accuracy: 0.0714\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9181 - accuracy: 0.3874 - val_loss: 2.7018 - val_accuracy: 0.0893\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9092 - accuracy: 0.3960 - val_loss: 2.7046 - val_accuracy: 0.0893\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8983 - accuracy: 0.3941 - val_loss: 2.7217 - val_accuracy: 0.0893\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8844 - accuracy: 0.3989 - val_loss: 2.7287 - val_accuracy: 0.0893\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8752 - accuracy: 0.3998 - val_loss: 2.7480 - val_accuracy: 0.0714\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8661 - accuracy: 0.4074 - val_loss: 2.7489 - val_accuracy: 0.0714\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8536 - accuracy: 0.4122 - val_loss: 2.7587 - val_accuracy: 0.0714\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8423 - accuracy: 0.4151 - val_loss: 2.7612 - val_accuracy: 0.0714\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.8336 - accuracy: 0.4218 - val_loss: 2.7857 - val_accuracy: 0.0714\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8212 - accuracy: 0.4198 - val_loss: 2.7699 - val_accuracy: 0.0714\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8154 - accuracy: 0.4246 - val_loss: 2.7907 - val_accuracy: 0.0714\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8031 - accuracy: 0.4208 - val_loss: 2.8109 - val_accuracy: 0.0714\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7947 - accuracy: 0.4332 - val_loss: 2.7880 - val_accuracy: 0.0714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7800 - accuracy: 0.4456 - val_loss: 2.8306 - val_accuracy: 0.0714\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7766 - accuracy: 0.4332 - val_loss: 2.8393 - val_accuracy: 0.0714\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7661 - accuracy: 0.4380 - val_loss: 2.8342 - val_accuracy: 0.0893\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7557 - accuracy: 0.4342 - val_loss: 2.8428 - val_accuracy: 0.0714\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7431 - accuracy: 0.4466 - val_loss: 2.8568 - val_accuracy: 0.0714\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7352 - accuracy: 0.4494 - val_loss: 2.8723 - val_accuracy: 0.0893\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7266 - accuracy: 0.4552 - val_loss: 2.8847 - val_accuracy: 0.0714\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7182 - accuracy: 0.4580 - val_loss: 2.8999 - val_accuracy: 0.0714\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7098 - accuracy: 0.4618 - val_loss: 2.9023 - val_accuracy: 0.0893\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6963 - accuracy: 0.4676 - val_loss: 2.8815 - val_accuracy: 0.1071\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6879 - accuracy: 0.4599 - val_loss: 2.9169 - val_accuracy: 0.0893\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6801 - accuracy: 0.4695 - val_loss: 2.9409 - val_accuracy: 0.0714\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6713 - accuracy: 0.4695 - val_loss: 2.9498 - val_accuracy: 0.0893\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6634 - accuracy: 0.4685 - val_loss: 2.9625 - val_accuracy: 0.0893\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6566 - accuracy: 0.4685 - val_loss: 2.9741 - val_accuracy: 0.0893\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6486 - accuracy: 0.4771 - val_loss: 2.9579 - val_accuracy: 0.0893\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6393 - accuracy: 0.4828 - val_loss: 2.9601 - val_accuracy: 0.0714\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6330 - accuracy: 0.4847 - val_loss: 2.9777 - val_accuracy: 0.0893\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6228 - accuracy: 0.4895 - val_loss: 3.0015 - val_accuracy: 0.1071\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6155 - accuracy: 0.4990 - val_loss: 2.9960 - val_accuracy: 0.0893\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6070 - accuracy: 0.4990 - val_loss: 3.0110 - val_accuracy: 0.1071\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6020 - accuracy: 0.4828 - val_loss: 3.0304 - val_accuracy: 0.1250\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5923 - accuracy: 0.5019 - val_loss: 3.0426 - val_accuracy: 0.0714\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5834 - accuracy: 0.5057 - val_loss: 3.0262 - val_accuracy: 0.1071\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5780 - accuracy: 0.5019 - val_loss: 3.0694 - val_accuracy: 0.0893\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5683 - accuracy: 0.5057 - val_loss: 3.0306 - val_accuracy: 0.1071\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5641 - accuracy: 0.5029 - val_loss: 3.0828 - val_accuracy: 0.1071\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5552 - accuracy: 0.5067 - val_loss: 3.0896 - val_accuracy: 0.0893\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5457 - accuracy: 0.5057 - val_loss: 3.0756 - val_accuracy: 0.1071\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5387 - accuracy: 0.5162 - val_loss: 3.1005 - val_accuracy: 0.1071\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5325 - accuracy: 0.5105 - val_loss: 3.0972 - val_accuracy: 0.0893\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5232 - accuracy: 0.5210 - val_loss: 3.1124 - val_accuracy: 0.1071\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5180 - accuracy: 0.5172 - val_loss: 3.1348 - val_accuracy: 0.0893\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5109 - accuracy: 0.5191 - val_loss: 3.1107 - val_accuracy: 0.1071\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5061 - accuracy: 0.5134 - val_loss: 3.1611 - val_accuracy: 0.1071\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4989 - accuracy: 0.5267 - val_loss: 3.1119 - val_accuracy: 0.1071\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4908 - accuracy: 0.5210 - val_loss: 3.1686 - val_accuracy: 0.1071\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4834 - accuracy: 0.5296 - val_loss: 3.1796 - val_accuracy: 0.1071\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4748 - accuracy: 0.5344 - val_loss: 3.1954 - val_accuracy: 0.1071\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4677 - accuracy: 0.5344 - val_loss: 3.2081 - val_accuracy: 0.1071\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.4643 - accuracy: 0.5334 - val_loss: 3.2290 - val_accuracy: 0.1071\n",
      "9/9 [==============================] - 0s 939us/step - loss: 3.3858 - accuracy: 0.1341\n",
      "\n",
      "Test accuracy: 0.13405796885490417\n",
      "var: 0.995\n",
      "(1380, 24)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 15ms/step - loss: 2.6676 - accuracy: 0.1040 - val_loss: 2.6059 - val_accuracy: 0.0714\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.5878 - accuracy: 0.1231 - val_loss: 2.5565 - val_accuracy: 0.1607\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5469 - accuracy: 0.1393 - val_loss: 2.5372 - val_accuracy: 0.1429\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5157 - accuracy: 0.1460 - val_loss: 2.5254 - val_accuracy: 0.1429\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.4865 - accuracy: 0.1641 - val_loss: 2.5141 - val_accuracy: 0.1429\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4638 - accuracy: 0.1803 - val_loss: 2.5081 - val_accuracy: 0.1607\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.4416 - accuracy: 0.1975 - val_loss: 2.5103 - val_accuracy: 0.1607\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4226 - accuracy: 0.1937 - val_loss: 2.5056 - val_accuracy: 0.1786\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4034 - accuracy: 0.2042 - val_loss: 2.5083 - val_accuracy: 0.1607\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3846 - accuracy: 0.2281 - val_loss: 2.5151 - val_accuracy: 0.1250\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3667 - accuracy: 0.2347 - val_loss: 2.5224 - val_accuracy: 0.1429\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3497 - accuracy: 0.2500 - val_loss: 2.5186 - val_accuracy: 0.1429\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3346 - accuracy: 0.2538 - val_loss: 2.5272 - val_accuracy: 0.1071\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3179 - accuracy: 0.2567 - val_loss: 2.5326 - val_accuracy: 0.1429\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3037 - accuracy: 0.2662 - val_loss: 2.5386 - val_accuracy: 0.1429\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2884 - accuracy: 0.2605 - val_loss: 2.5459 - val_accuracy: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2741 - accuracy: 0.2710 - val_loss: 2.5507 - val_accuracy: 0.1250\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2584 - accuracy: 0.2729 - val_loss: 2.5563 - val_accuracy: 0.1429\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2457 - accuracy: 0.2834 - val_loss: 2.5637 - val_accuracy: 0.1250\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2303 - accuracy: 0.2824 - val_loss: 2.5755 - val_accuracy: 0.1607\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2182 - accuracy: 0.2805 - val_loss: 2.5817 - val_accuracy: 0.1429\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2036 - accuracy: 0.2901 - val_loss: 2.5865 - val_accuracy: 0.1429\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1911 - accuracy: 0.2939 - val_loss: 2.5967 - val_accuracy: 0.1250\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1784 - accuracy: 0.2939 - val_loss: 2.6090 - val_accuracy: 0.1607\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1646 - accuracy: 0.3092 - val_loss: 2.6166 - val_accuracy: 0.1429\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1487 - accuracy: 0.3073 - val_loss: 2.6214 - val_accuracy: 0.1429\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1392 - accuracy: 0.3034 - val_loss: 2.6276 - val_accuracy: 0.1607\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.1278 - accuracy: 0.3101 - val_loss: 2.6360 - val_accuracy: 0.1607\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1128 - accuracy: 0.3235 - val_loss: 2.6356 - val_accuracy: 0.1429\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1013 - accuracy: 0.3235 - val_loss: 2.6438 - val_accuracy: 0.1607\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0885 - accuracy: 0.3244 - val_loss: 2.6498 - val_accuracy: 0.1250\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0784 - accuracy: 0.3282 - val_loss: 2.6649 - val_accuracy: 0.1429\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0634 - accuracy: 0.3368 - val_loss: 2.6646 - val_accuracy: 0.1250\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0515 - accuracy: 0.3368 - val_loss: 2.6837 - val_accuracy: 0.1250\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0386 - accuracy: 0.3426 - val_loss: 2.6821 - val_accuracy: 0.1250\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0271 - accuracy: 0.3511 - val_loss: 2.6954 - val_accuracy: 0.1250\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0178 - accuracy: 0.3483 - val_loss: 2.6978 - val_accuracy: 0.1250\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0058 - accuracy: 0.3531 - val_loss: 2.7062 - val_accuracy: 0.1429\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9953 - accuracy: 0.3559 - val_loss: 2.7194 - val_accuracy: 0.1429\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9834 - accuracy: 0.3645 - val_loss: 2.7416 - val_accuracy: 0.1250\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9703 - accuracy: 0.3712 - val_loss: 2.7356 - val_accuracy: 0.1250\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9608 - accuracy: 0.3645 - val_loss: 2.7447 - val_accuracy: 0.1429\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9508 - accuracy: 0.3769 - val_loss: 2.7499 - val_accuracy: 0.1429\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9384 - accuracy: 0.3740 - val_loss: 2.7600 - val_accuracy: 0.1429\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9294 - accuracy: 0.3731 - val_loss: 2.7875 - val_accuracy: 0.1250\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9176 - accuracy: 0.3826 - val_loss: 2.7799 - val_accuracy: 0.1250\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9069 - accuracy: 0.3865 - val_loss: 2.7988 - val_accuracy: 0.1250\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8948 - accuracy: 0.3969 - val_loss: 2.7903 - val_accuracy: 0.1429\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8858 - accuracy: 0.3931 - val_loss: 2.8005 - val_accuracy: 0.1250\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8770 - accuracy: 0.3960 - val_loss: 2.8197 - val_accuracy: 0.1250\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8664 - accuracy: 0.3969 - val_loss: 2.8315 - val_accuracy: 0.1250\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8548 - accuracy: 0.3998 - val_loss: 2.8346 - val_accuracy: 0.1250\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8477 - accuracy: 0.4008 - val_loss: 2.8434 - val_accuracy: 0.1250\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8358 - accuracy: 0.4122 - val_loss: 2.8693 - val_accuracy: 0.1250\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8264 - accuracy: 0.4132 - val_loss: 2.8642 - val_accuracy: 0.1250\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8155 - accuracy: 0.4189 - val_loss: 2.8832 - val_accuracy: 0.1250\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8103 - accuracy: 0.4160 - val_loss: 2.9002 - val_accuracy: 0.1429\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7962 - accuracy: 0.4170 - val_loss: 2.8919 - val_accuracy: 0.1250\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7911 - accuracy: 0.4170 - val_loss: 2.9271 - val_accuracy: 0.1250\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7805 - accuracy: 0.4208 - val_loss: 2.9308 - val_accuracy: 0.1250\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7689 - accuracy: 0.4265 - val_loss: 2.9284 - val_accuracy: 0.1250\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7626 - accuracy: 0.4380 - val_loss: 2.9274 - val_accuracy: 0.1071\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7517 - accuracy: 0.4265 - val_loss: 2.9379 - val_accuracy: 0.1250\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7456 - accuracy: 0.4332 - val_loss: 2.9505 - val_accuracy: 0.1071\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7354 - accuracy: 0.4370 - val_loss: 2.9663 - val_accuracy: 0.1250\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7257 - accuracy: 0.4332 - val_loss: 2.9578 - val_accuracy: 0.1071\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7164 - accuracy: 0.4351 - val_loss: 2.9915 - val_accuracy: 0.1250\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7101 - accuracy: 0.4447 - val_loss: 2.9888 - val_accuracy: 0.1071\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7013 - accuracy: 0.4361 - val_loss: 2.9972 - val_accuracy: 0.1071\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6934 - accuracy: 0.4427 - val_loss: 3.0215 - val_accuracy: 0.1071\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6866 - accuracy: 0.4437 - val_loss: 3.0218 - val_accuracy: 0.1250\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6796 - accuracy: 0.4466 - val_loss: 3.0359 - val_accuracy: 0.1250\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6734 - accuracy: 0.4561 - val_loss: 3.0510 - val_accuracy: 0.1071\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6619 - accuracy: 0.4561 - val_loss: 3.0617 - val_accuracy: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6530 - accuracy: 0.4666 - val_loss: 3.0550 - val_accuracy: 0.1071\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6476 - accuracy: 0.4685 - val_loss: 3.0979 - val_accuracy: 0.1429\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6378 - accuracy: 0.4666 - val_loss: 3.0856 - val_accuracy: 0.1250\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6331 - accuracy: 0.4647 - val_loss: 3.1012 - val_accuracy: 0.1429\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6254 - accuracy: 0.4676 - val_loss: 3.1157 - val_accuracy: 0.1607\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6178 - accuracy: 0.4695 - val_loss: 3.1290 - val_accuracy: 0.1250\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6111 - accuracy: 0.4666 - val_loss: 3.1482 - val_accuracy: 0.1071\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6030 - accuracy: 0.4685 - val_loss: 3.1549 - val_accuracy: 0.1071\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5955 - accuracy: 0.4790 - val_loss: 3.1569 - val_accuracy: 0.1250\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5916 - accuracy: 0.4752 - val_loss: 3.1868 - val_accuracy: 0.1607\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5840 - accuracy: 0.4847 - val_loss: 3.1871 - val_accuracy: 0.1250\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5746 - accuracy: 0.4809 - val_loss: 3.1972 - val_accuracy: 0.1250\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5685 - accuracy: 0.4809 - val_loss: 3.2208 - val_accuracy: 0.1250\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5646 - accuracy: 0.4828 - val_loss: 3.2219 - val_accuracy: 0.1250\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5562 - accuracy: 0.4857 - val_loss: 3.2463 - val_accuracy: 0.1250\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5488 - accuracy: 0.4952 - val_loss: 3.2558 - val_accuracy: 0.1250\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5444 - accuracy: 0.4924 - val_loss: 3.2713 - val_accuracy: 0.1250\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5343 - accuracy: 0.4990 - val_loss: 3.2851 - val_accuracy: 0.1250\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5275 - accuracy: 0.4933 - val_loss: 3.2857 - val_accuracy: 0.1250\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1.5279 - accuracy: 0.4990 - val_loss: 3.3225 - val_accuracy: 0.1250\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5172 - accuracy: 0.5010 - val_loss: 3.3252 - val_accuracy: 0.1429\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5074 - accuracy: 0.5029 - val_loss: 3.3513 - val_accuracy: 0.1250\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5061 - accuracy: 0.5029 - val_loss: 3.3567 - val_accuracy: 0.1429\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4994 - accuracy: 0.5076 - val_loss: 3.3674 - val_accuracy: 0.1250\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4938 - accuracy: 0.5067 - val_loss: 3.3805 - val_accuracy: 0.1250\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4863 - accuracy: 0.5048 - val_loss: 3.3932 - val_accuracy: 0.1250\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3.4643 - accuracy: 0.1268\n",
      "\n",
      "Test accuracy: 0.12681159377098083\n",
      "var: 0.996\n",
      "(1380, 24)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.7007 - accuracy: 0.0763 - val_loss: 2.5965 - val_accuracy: 0.1071\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.6198 - accuracy: 0.1097 - val_loss: 2.5729 - val_accuracy: 0.0893\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5764 - accuracy: 0.1174 - val_loss: 2.5608 - val_accuracy: 0.0893\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5440 - accuracy: 0.1298 - val_loss: 2.5457 - val_accuracy: 0.1250\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.5151 - accuracy: 0.1469 - val_loss: 2.5320 - val_accuracy: 0.0893\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.4916 - accuracy: 0.1622 - val_loss: 2.5254 - val_accuracy: 0.0714\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4674 - accuracy: 0.1727 - val_loss: 2.5083 - val_accuracy: 0.0893\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4473 - accuracy: 0.1765 - val_loss: 2.5042 - val_accuracy: 0.0714\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4267 - accuracy: 0.1899 - val_loss: 2.4976 - val_accuracy: 0.0714\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4108 - accuracy: 0.1975 - val_loss: 2.4904 - val_accuracy: 0.1071\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3925 - accuracy: 0.1975 - val_loss: 2.4856 - val_accuracy: 0.1250\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3765 - accuracy: 0.2061 - val_loss: 2.4833 - val_accuracy: 0.1250\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3600 - accuracy: 0.2233 - val_loss: 2.4772 - val_accuracy: 0.1071\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3456 - accuracy: 0.2290 - val_loss: 2.4634 - val_accuracy: 0.0893\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3281 - accuracy: 0.2204 - val_loss: 2.4778 - val_accuracy: 0.0893\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3114 - accuracy: 0.2414 - val_loss: 2.4536 - val_accuracy: 0.0893\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2968 - accuracy: 0.2443 - val_loss: 2.4664 - val_accuracy: 0.0893\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2817 - accuracy: 0.2481 - val_loss: 2.4570 - val_accuracy: 0.0893\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2684 - accuracy: 0.2481 - val_loss: 2.4559 - val_accuracy: 0.0893\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2538 - accuracy: 0.2548 - val_loss: 2.4589 - val_accuracy: 0.0893\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2395 - accuracy: 0.2729 - val_loss: 2.4549 - val_accuracy: 0.0893\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2251 - accuracy: 0.2758 - val_loss: 2.4482 - val_accuracy: 0.0893\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2115 - accuracy: 0.2748 - val_loss: 2.4532 - val_accuracy: 0.1250\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1979 - accuracy: 0.2767 - val_loss: 2.4551 - val_accuracy: 0.1071\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1884 - accuracy: 0.2815 - val_loss: 2.4629 - val_accuracy: 0.1071\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1712 - accuracy: 0.2920 - val_loss: 2.4587 - val_accuracy: 0.1250\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1599 - accuracy: 0.2948 - val_loss: 2.4675 - val_accuracy: 0.1071\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1495 - accuracy: 0.2958 - val_loss: 2.4671 - val_accuracy: 0.1071\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1364 - accuracy: 0.3044 - val_loss: 2.4734 - val_accuracy: 0.1071\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1237 - accuracy: 0.3044 - val_loss: 2.4835 - val_accuracy: 0.1250\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1131 - accuracy: 0.3149 - val_loss: 2.4870 - val_accuracy: 0.0893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1019 - accuracy: 0.3101 - val_loss: 2.4942 - val_accuracy: 0.1429\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0883 - accuracy: 0.3187 - val_loss: 2.5000 - val_accuracy: 0.1250\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0803 - accuracy: 0.3244 - val_loss: 2.5095 - val_accuracy: 0.0893\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0696 - accuracy: 0.3302 - val_loss: 2.5200 - val_accuracy: 0.1071\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0579 - accuracy: 0.3359 - val_loss: 2.5196 - val_accuracy: 0.1071\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0477 - accuracy: 0.3406 - val_loss: 2.5309 - val_accuracy: 0.0893\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0356 - accuracy: 0.3464 - val_loss: 2.5410 - val_accuracy: 0.1071\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0257 - accuracy: 0.3426 - val_loss: 2.5549 - val_accuracy: 0.0893\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0154 - accuracy: 0.3540 - val_loss: 2.5589 - val_accuracy: 0.1071\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0057 - accuracy: 0.3607 - val_loss: 2.5676 - val_accuracy: 0.0893\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9969 - accuracy: 0.3607 - val_loss: 2.5712 - val_accuracy: 0.0714\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9832 - accuracy: 0.3645 - val_loss: 2.5918 - val_accuracy: 0.0714\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9753 - accuracy: 0.3550 - val_loss: 2.5935 - val_accuracy: 0.1071\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9638 - accuracy: 0.3702 - val_loss: 2.6090 - val_accuracy: 0.0893\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9592 - accuracy: 0.3702 - val_loss: 2.6332 - val_accuracy: 0.1071\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9445 - accuracy: 0.3769 - val_loss: 2.6145 - val_accuracy: 0.0714\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9335 - accuracy: 0.3769 - val_loss: 2.6433 - val_accuracy: 0.1250\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9266 - accuracy: 0.3779 - val_loss: 2.6396 - val_accuracy: 0.0714\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9163 - accuracy: 0.3817 - val_loss: 2.6555 - val_accuracy: 0.0714\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9044 - accuracy: 0.3874 - val_loss: 2.6681 - val_accuracy: 0.0893\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8979 - accuracy: 0.3807 - val_loss: 2.6658 - val_accuracy: 0.0893\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8891 - accuracy: 0.3903 - val_loss: 2.6852 - val_accuracy: 0.0893\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8782 - accuracy: 0.3960 - val_loss: 2.6970 - val_accuracy: 0.0893\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8698 - accuracy: 0.3931 - val_loss: 2.6969 - val_accuracy: 0.0893\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8601 - accuracy: 0.4084 - val_loss: 2.7096 - val_accuracy: 0.1071\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8517 - accuracy: 0.3989 - val_loss: 2.7163 - val_accuracy: 0.1071\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8429 - accuracy: 0.4141 - val_loss: 2.7127 - val_accuracy: 0.1071\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8338 - accuracy: 0.4103 - val_loss: 2.7379 - val_accuracy: 0.0893\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8234 - accuracy: 0.4103 - val_loss: 2.7442 - val_accuracy: 0.1071\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8167 - accuracy: 0.4141 - val_loss: 2.7558 - val_accuracy: 0.0893\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8057 - accuracy: 0.4208 - val_loss: 2.7681 - val_accuracy: 0.0893\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7993 - accuracy: 0.4122 - val_loss: 2.7818 - val_accuracy: 0.0714\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7909 - accuracy: 0.4294 - val_loss: 2.7837 - val_accuracy: 0.0893\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7832 - accuracy: 0.4418 - val_loss: 2.8009 - val_accuracy: 0.0714\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7761 - accuracy: 0.4351 - val_loss: 2.7958 - val_accuracy: 0.0714\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7710 - accuracy: 0.4284 - val_loss: 2.8233 - val_accuracy: 0.0714\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7604 - accuracy: 0.4284 - val_loss: 2.8136 - val_accuracy: 0.1071\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7540 - accuracy: 0.4370 - val_loss: 2.8311 - val_accuracy: 0.0893\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7471 - accuracy: 0.4370 - val_loss: 2.8322 - val_accuracy: 0.0893\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7387 - accuracy: 0.4494 - val_loss: 2.8530 - val_accuracy: 0.0893\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7318 - accuracy: 0.4561 - val_loss: 2.8674 - val_accuracy: 0.1071\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7203 - accuracy: 0.4475 - val_loss: 2.8704 - val_accuracy: 0.0893\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7136 - accuracy: 0.4552 - val_loss: 2.8854 - val_accuracy: 0.1071\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7095 - accuracy: 0.4561 - val_loss: 2.9040 - val_accuracy: 0.0893\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7005 - accuracy: 0.4656 - val_loss: 2.8914 - val_accuracy: 0.1071\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6942 - accuracy: 0.4676 - val_loss: 2.9111 - val_accuracy: 0.1071\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6877 - accuracy: 0.4599 - val_loss: 2.9251 - val_accuracy: 0.1071\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6830 - accuracy: 0.4637 - val_loss: 2.9369 - val_accuracy: 0.0893\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6690 - accuracy: 0.4676 - val_loss: 2.9438 - val_accuracy: 0.1071\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6679 - accuracy: 0.4723 - val_loss: 2.9469 - val_accuracy: 0.1250\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6602 - accuracy: 0.4676 - val_loss: 2.9660 - val_accuracy: 0.1071\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6518 - accuracy: 0.4714 - val_loss: 2.9637 - val_accuracy: 0.1071\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6440 - accuracy: 0.4819 - val_loss: 2.9703 - val_accuracy: 0.1071\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6416 - accuracy: 0.4723 - val_loss: 2.9718 - val_accuracy: 0.1429\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6317 - accuracy: 0.4838 - val_loss: 2.9847 - val_accuracy: 0.1250\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6241 - accuracy: 0.4790 - val_loss: 2.9969 - val_accuracy: 0.1250\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6194 - accuracy: 0.4876 - val_loss: 3.0147 - val_accuracy: 0.1250\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6133 - accuracy: 0.4809 - val_loss: 3.0162 - val_accuracy: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6023 - accuracy: 0.4914 - val_loss: 3.0238 - val_accuracy: 0.1250\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5961 - accuracy: 0.5010 - val_loss: 3.0334 - val_accuracy: 0.1429\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5901 - accuracy: 0.4924 - val_loss: 3.0434 - val_accuracy: 0.1250\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5837 - accuracy: 0.5000 - val_loss: 3.0752 - val_accuracy: 0.1429\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5817 - accuracy: 0.5010 - val_loss: 3.0626 - val_accuracy: 0.1429\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5736 - accuracy: 0.4971 - val_loss: 3.0869 - val_accuracy: 0.1250\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5675 - accuracy: 0.5019 - val_loss: 3.1070 - val_accuracy: 0.1429\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5578 - accuracy: 0.5095 - val_loss: 3.0894 - val_accuracy: 0.1250\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5544 - accuracy: 0.5000 - val_loss: 3.1208 - val_accuracy: 0.1250\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5480 - accuracy: 0.5086 - val_loss: 3.1134 - val_accuracy: 0.1429\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5401 - accuracy: 0.5105 - val_loss: 3.1399 - val_accuracy: 0.1429\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3.3515 - accuracy: 0.1522\n",
      "\n",
      "Test accuracy: 0.15217390656471252\n",
      "var: 0.997\n",
      "(1380, 24)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.7297 - accuracy: 0.0821 - val_loss: 2.6467 - val_accuracy: 0.1250\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.6281 - accuracy: 0.0983 - val_loss: 2.6089 - val_accuracy: 0.1250\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5785 - accuracy: 0.1021 - val_loss: 2.5760 - val_accuracy: 0.1429\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5444 - accuracy: 0.1260 - val_loss: 2.5640 - val_accuracy: 0.1071\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5160 - accuracy: 0.1393 - val_loss: 2.5414 - val_accuracy: 0.1429\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4900 - accuracy: 0.1603 - val_loss: 2.5259 - val_accuracy: 0.1250\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4714 - accuracy: 0.1727 - val_loss: 2.5112 - val_accuracy: 0.1071\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4487 - accuracy: 0.1794 - val_loss: 2.5040 - val_accuracy: 0.0714\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4303 - accuracy: 0.1813 - val_loss: 2.4961 - val_accuracy: 0.0714\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4124 - accuracy: 0.1823 - val_loss: 2.5004 - val_accuracy: 0.0893\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3954 - accuracy: 0.1994 - val_loss: 2.4965 - val_accuracy: 0.0893\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3798 - accuracy: 0.1985 - val_loss: 2.4945 - val_accuracy: 0.0893\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3604 - accuracy: 0.2090 - val_loss: 2.4860 - val_accuracy: 0.0893\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3444 - accuracy: 0.2233 - val_loss: 2.4883 - val_accuracy: 0.0893\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3308 - accuracy: 0.2281 - val_loss: 2.4825 - val_accuracy: 0.0893\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3156 - accuracy: 0.2290 - val_loss: 2.4811 - val_accuracy: 0.1071\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3011 - accuracy: 0.2433 - val_loss: 2.4902 - val_accuracy: 0.0714\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2850 - accuracy: 0.2481 - val_loss: 2.4897 - val_accuracy: 0.1250\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2729 - accuracy: 0.2462 - val_loss: 2.4939 - val_accuracy: 0.0893\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2595 - accuracy: 0.2500 - val_loss: 2.5021 - val_accuracy: 0.0893\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2455 - accuracy: 0.2567 - val_loss: 2.5022 - val_accuracy: 0.1250\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2332 - accuracy: 0.2634 - val_loss: 2.5108 - val_accuracy: 0.1250\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2185 - accuracy: 0.2653 - val_loss: 2.5129 - val_accuracy: 0.1429\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2095 - accuracy: 0.2691 - val_loss: 2.5287 - val_accuracy: 0.0893\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1944 - accuracy: 0.2710 - val_loss: 2.5270 - val_accuracy: 0.1250\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1836 - accuracy: 0.2767 - val_loss: 2.5388 - val_accuracy: 0.1250\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1713 - accuracy: 0.2796 - val_loss: 2.5391 - val_accuracy: 0.1429\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1604 - accuracy: 0.2805 - val_loss: 2.5448 - val_accuracy: 0.1250\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1467 - accuracy: 0.2920 - val_loss: 2.5503 - val_accuracy: 0.1250\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1368 - accuracy: 0.2996 - val_loss: 2.5610 - val_accuracy: 0.1250\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1258 - accuracy: 0.2939 - val_loss: 2.5672 - val_accuracy: 0.1250\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1121 - accuracy: 0.3015 - val_loss: 2.5692 - val_accuracy: 0.1429\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1021 - accuracy: 0.3120 - val_loss: 2.5790 - val_accuracy: 0.1429\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0911 - accuracy: 0.3120 - val_loss: 2.5829 - val_accuracy: 0.1429\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0800 - accuracy: 0.3139 - val_loss: 2.5993 - val_accuracy: 0.1607\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0703 - accuracy: 0.3235 - val_loss: 2.6011 - val_accuracy: 0.1607\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0602 - accuracy: 0.3263 - val_loss: 2.6095 - val_accuracy: 0.1429\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0502 - accuracy: 0.3311 - val_loss: 2.6146 - val_accuracy: 0.1607\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0383 - accuracy: 0.3311 - val_loss: 2.6309 - val_accuracy: 0.1429\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0279 - accuracy: 0.3387 - val_loss: 2.6307 - val_accuracy: 0.1607\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0189 - accuracy: 0.3473 - val_loss: 2.6351 - val_accuracy: 0.1429\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0096 - accuracy: 0.3406 - val_loss: 2.6377 - val_accuracy: 0.1429\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9996 - accuracy: 0.3511 - val_loss: 2.6467 - val_accuracy: 0.1429\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9900 - accuracy: 0.3540 - val_loss: 2.6658 - val_accuracy: 0.1429\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9783 - accuracy: 0.3588 - val_loss: 2.6700 - val_accuracy: 0.1429\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9691 - accuracy: 0.3616 - val_loss: 2.6847 - val_accuracy: 0.1429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9589 - accuracy: 0.3664 - val_loss: 2.6884 - val_accuracy: 0.1429\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9498 - accuracy: 0.3693 - val_loss: 2.6941 - val_accuracy: 0.1250\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9415 - accuracy: 0.3740 - val_loss: 2.7147 - val_accuracy: 0.1429\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9340 - accuracy: 0.3731 - val_loss: 2.7181 - val_accuracy: 0.1429\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9230 - accuracy: 0.3731 - val_loss: 2.7307 - val_accuracy: 0.1250\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9113 - accuracy: 0.3884 - val_loss: 2.7298 - val_accuracy: 0.1429\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9028 - accuracy: 0.3855 - val_loss: 2.7367 - val_accuracy: 0.1429\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8941 - accuracy: 0.3884 - val_loss: 2.7567 - val_accuracy: 0.1250\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8842 - accuracy: 0.3893 - val_loss: 2.7551 - val_accuracy: 0.1429\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8752 - accuracy: 0.3931 - val_loss: 2.7735 - val_accuracy: 0.1429\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8653 - accuracy: 0.3989 - val_loss: 2.7856 - val_accuracy: 0.1429\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8547 - accuracy: 0.3969 - val_loss: 2.7746 - val_accuracy: 0.1607\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8449 - accuracy: 0.4084 - val_loss: 2.7968 - val_accuracy: 0.1250\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8368 - accuracy: 0.4151 - val_loss: 2.8075 - val_accuracy: 0.1429\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8282 - accuracy: 0.4103 - val_loss: 2.8105 - val_accuracy: 0.1250\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8174 - accuracy: 0.4208 - val_loss: 2.8327 - val_accuracy: 0.1429\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8109 - accuracy: 0.4208 - val_loss: 2.8365 - val_accuracy: 0.1429\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8024 - accuracy: 0.4275 - val_loss: 2.8437 - val_accuracy: 0.1429\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7943 - accuracy: 0.4284 - val_loss: 2.8656 - val_accuracy: 0.1250\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7828 - accuracy: 0.4323 - val_loss: 2.8669 - val_accuracy: 0.1250\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7766 - accuracy: 0.4246 - val_loss: 2.8757 - val_accuracy: 0.1250\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7667 - accuracy: 0.4313 - val_loss: 2.8934 - val_accuracy: 0.1071\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7580 - accuracy: 0.4323 - val_loss: 2.9040 - val_accuracy: 0.1071\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7498 - accuracy: 0.4399 - val_loss: 2.9129 - val_accuracy: 0.1071\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7411 - accuracy: 0.4485 - val_loss: 2.9357 - val_accuracy: 0.1250\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7296 - accuracy: 0.4571 - val_loss: 2.9300 - val_accuracy: 0.1250\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7229 - accuracy: 0.4552 - val_loss: 2.9596 - val_accuracy: 0.1071\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7185 - accuracy: 0.4437 - val_loss: 2.9601 - val_accuracy: 0.1250\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7062 - accuracy: 0.4513 - val_loss: 2.9650 - val_accuracy: 0.1071\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6990 - accuracy: 0.4599 - val_loss: 2.9826 - val_accuracy: 0.1071\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6930 - accuracy: 0.4571 - val_loss: 3.0010 - val_accuracy: 0.1250\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6792 - accuracy: 0.4637 - val_loss: 3.0106 - val_accuracy: 0.1250\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6734 - accuracy: 0.4761 - val_loss: 3.0271 - val_accuracy: 0.1250\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6677 - accuracy: 0.4695 - val_loss: 3.0410 - val_accuracy: 0.1071\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6590 - accuracy: 0.4666 - val_loss: 3.0590 - val_accuracy: 0.1250\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6496 - accuracy: 0.4723 - val_loss: 3.0590 - val_accuracy: 0.1250\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6412 - accuracy: 0.4752 - val_loss: 3.0880 - val_accuracy: 0.1071\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6349 - accuracy: 0.4742 - val_loss: 3.0802 - val_accuracy: 0.0893\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6255 - accuracy: 0.4781 - val_loss: 3.1007 - val_accuracy: 0.1250\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6178 - accuracy: 0.4828 - val_loss: 3.1085 - val_accuracy: 0.1250\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6132 - accuracy: 0.4781 - val_loss: 3.1322 - val_accuracy: 0.1250\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6033 - accuracy: 0.4809 - val_loss: 3.1386 - val_accuracy: 0.0893\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5950 - accuracy: 0.4857 - val_loss: 3.1540 - val_accuracy: 0.1071\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5868 - accuracy: 0.4933 - val_loss: 3.1638 - val_accuracy: 0.0893\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5829 - accuracy: 0.5000 - val_loss: 3.1918 - val_accuracy: 0.0893\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5731 - accuracy: 0.4990 - val_loss: 3.1955 - val_accuracy: 0.0893\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5697 - accuracy: 0.5057 - val_loss: 3.1937 - val_accuracy: 0.1071\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5604 - accuracy: 0.4971 - val_loss: 3.2292 - val_accuracy: 0.1071\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5535 - accuracy: 0.4990 - val_loss: 3.2395 - val_accuracy: 0.0893\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5489 - accuracy: 0.5010 - val_loss: 3.2440 - val_accuracy: 0.0893\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5400 - accuracy: 0.5057 - val_loss: 3.2644 - val_accuracy: 0.0893\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5335 - accuracy: 0.5115 - val_loss: 3.2733 - val_accuracy: 0.0893\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5263 - accuracy: 0.5143 - val_loss: 3.2867 - val_accuracy: 0.0893\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5186 - accuracy: 0.5229 - val_loss: 3.2788 - val_accuracy: 0.0893\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3.4027 - accuracy: 0.1413\n",
      "\n",
      "Test accuracy: 0.14130434393882751\n",
      "var: 0.998\n",
      "(1380, 24)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 2.7776 - accuracy: 0.0830 - val_loss: 2.7694 - val_accuracy: 0.0536\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.6528 - accuracy: 0.1069 - val_loss: 2.7037 - val_accuracy: 0.0893\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.6025 - accuracy: 0.1078 - val_loss: 2.6740 - val_accuracy: 0.0893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.5694 - accuracy: 0.1269 - val_loss: 2.6561 - val_accuracy: 0.0893\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5427 - accuracy: 0.1460 - val_loss: 2.6437 - val_accuracy: 0.1071\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5147 - accuracy: 0.1613 - val_loss: 2.6245 - val_accuracy: 0.1071\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4890 - accuracy: 0.1718 - val_loss: 2.6149 - val_accuracy: 0.1250\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.4669 - accuracy: 0.1775 - val_loss: 2.6085 - val_accuracy: 0.1429\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4461 - accuracy: 0.1889 - val_loss: 2.6064 - val_accuracy: 0.1250\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4274 - accuracy: 0.1937 - val_loss: 2.6062 - val_accuracy: 0.1250\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.4088 - accuracy: 0.2023 - val_loss: 2.6111 - val_accuracy: 0.1250\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3910 - accuracy: 0.2109 - val_loss: 2.6108 - val_accuracy: 0.1250\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3737 - accuracy: 0.2214 - val_loss: 2.6133 - val_accuracy: 0.1250\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3599 - accuracy: 0.2233 - val_loss: 2.6219 - val_accuracy: 0.1429\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3424 - accuracy: 0.2433 - val_loss: 2.6238 - val_accuracy: 0.1429\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3255 - accuracy: 0.2433 - val_loss: 2.6292 - val_accuracy: 0.1429\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3091 - accuracy: 0.2452 - val_loss: 2.6248 - val_accuracy: 0.1429\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2932 - accuracy: 0.2576 - val_loss: 2.6318 - val_accuracy: 0.1429\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2773 - accuracy: 0.2691 - val_loss: 2.6258 - val_accuracy: 0.1250\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2640 - accuracy: 0.2681 - val_loss: 2.6365 - val_accuracy: 0.1071\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2471 - accuracy: 0.2767 - val_loss: 2.6406 - val_accuracy: 0.1071\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2319 - accuracy: 0.2824 - val_loss: 2.6423 - val_accuracy: 0.1071\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2198 - accuracy: 0.2853 - val_loss: 2.6434 - val_accuracy: 0.1071\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2012 - accuracy: 0.2920 - val_loss: 2.6588 - val_accuracy: 0.0893\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1877 - accuracy: 0.3006 - val_loss: 2.6601 - val_accuracy: 0.1071\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1730 - accuracy: 0.3063 - val_loss: 2.6608 - val_accuracy: 0.1071\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1594 - accuracy: 0.2977 - val_loss: 2.6627 - val_accuracy: 0.1250\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1484 - accuracy: 0.3082 - val_loss: 2.6670 - val_accuracy: 0.1250\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1339 - accuracy: 0.3092 - val_loss: 2.6822 - val_accuracy: 0.1429\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1197 - accuracy: 0.3082 - val_loss: 2.6834 - val_accuracy: 0.1250\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1068 - accuracy: 0.3139 - val_loss: 2.6894 - val_accuracy: 0.1250\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0917 - accuracy: 0.3206 - val_loss: 2.7082 - val_accuracy: 0.1071\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0798 - accuracy: 0.3168 - val_loss: 2.7020 - val_accuracy: 0.1429\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0671 - accuracy: 0.3158 - val_loss: 2.7179 - val_accuracy: 0.1250\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0558 - accuracy: 0.3197 - val_loss: 2.7257 - val_accuracy: 0.1250\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0449 - accuracy: 0.3206 - val_loss: 2.7411 - val_accuracy: 0.1429\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0299 - accuracy: 0.3225 - val_loss: 2.7353 - val_accuracy: 0.1250\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0214 - accuracy: 0.3368 - val_loss: 2.7375 - val_accuracy: 0.1071\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0084 - accuracy: 0.3368 - val_loss: 2.7436 - val_accuracy: 0.1250\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9955 - accuracy: 0.3387 - val_loss: 2.7609 - val_accuracy: 0.1250\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9837 - accuracy: 0.3473 - val_loss: 2.7751 - val_accuracy: 0.1071\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9725 - accuracy: 0.3416 - val_loss: 2.7641 - val_accuracy: 0.1250\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9616 - accuracy: 0.3492 - val_loss: 2.7787 - val_accuracy: 0.1250\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9488 - accuracy: 0.3531 - val_loss: 2.7807 - val_accuracy: 0.1250\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9377 - accuracy: 0.3559 - val_loss: 2.8006 - val_accuracy: 0.1071\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9299 - accuracy: 0.3655 - val_loss: 2.8099 - val_accuracy: 0.1071\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9144 - accuracy: 0.3635 - val_loss: 2.8210 - val_accuracy: 0.1071\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9077 - accuracy: 0.3674 - val_loss: 2.8302 - val_accuracy: 0.1071\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8964 - accuracy: 0.3769 - val_loss: 2.8458 - val_accuracy: 0.1071\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8859 - accuracy: 0.3845 - val_loss: 2.8430 - val_accuracy: 0.1071\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8755 - accuracy: 0.3826 - val_loss: 2.8638 - val_accuracy: 0.1250\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8635 - accuracy: 0.3874 - val_loss: 2.8856 - val_accuracy: 0.1071\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8510 - accuracy: 0.3912 - val_loss: 2.8641 - val_accuracy: 0.1250\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8427 - accuracy: 0.3960 - val_loss: 2.8948 - val_accuracy: 0.1250\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8324 - accuracy: 0.4198 - val_loss: 2.9154 - val_accuracy: 0.0893\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8204 - accuracy: 0.4122 - val_loss: 2.9069 - val_accuracy: 0.1071\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8153 - accuracy: 0.4160 - val_loss: 2.9167 - val_accuracy: 0.1071\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8033 - accuracy: 0.4179 - val_loss: 2.9381 - val_accuracy: 0.1071\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7933 - accuracy: 0.4141 - val_loss: 2.9287 - val_accuracy: 0.1250\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7864 - accuracy: 0.4208 - val_loss: 2.9727 - val_accuracy: 0.1071\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7747 - accuracy: 0.4370 - val_loss: 2.9610 - val_accuracy: 0.0893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7650 - accuracy: 0.4380 - val_loss: 2.9941 - val_accuracy: 0.1250\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7543 - accuracy: 0.4303 - val_loss: 2.9998 - val_accuracy: 0.0893\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7451 - accuracy: 0.4447 - val_loss: 3.0050 - val_accuracy: 0.1071\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7361 - accuracy: 0.4447 - val_loss: 3.0138 - val_accuracy: 0.1071\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7270 - accuracy: 0.4456 - val_loss: 3.0329 - val_accuracy: 0.1071\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7182 - accuracy: 0.4513 - val_loss: 3.0451 - val_accuracy: 0.1071\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7116 - accuracy: 0.4494 - val_loss: 3.0630 - val_accuracy: 0.1250\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7002 - accuracy: 0.4571 - val_loss: 3.0721 - val_accuracy: 0.0893\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6926 - accuracy: 0.4590 - val_loss: 3.0935 - val_accuracy: 0.1429\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6853 - accuracy: 0.4590 - val_loss: 3.1068 - val_accuracy: 0.0893\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6766 - accuracy: 0.4532 - val_loss: 3.1139 - val_accuracy: 0.0893\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6699 - accuracy: 0.4685 - val_loss: 3.1300 - val_accuracy: 0.0893\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6585 - accuracy: 0.4637 - val_loss: 3.1476 - val_accuracy: 0.1071\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6503 - accuracy: 0.4742 - val_loss: 3.1605 - val_accuracy: 0.1250\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6441 - accuracy: 0.4742 - val_loss: 3.1644 - val_accuracy: 0.0893\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6393 - accuracy: 0.4723 - val_loss: 3.1864 - val_accuracy: 0.1071\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6278 - accuracy: 0.4781 - val_loss: 3.1948 - val_accuracy: 0.1250\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6237 - accuracy: 0.4819 - val_loss: 3.2096 - val_accuracy: 0.1071\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6151 - accuracy: 0.4771 - val_loss: 3.2240 - val_accuracy: 0.1071\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6042 - accuracy: 0.4828 - val_loss: 3.2338 - val_accuracy: 0.0893\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5992 - accuracy: 0.4809 - val_loss: 3.2502 - val_accuracy: 0.0893\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5885 - accuracy: 0.4905 - val_loss: 3.2669 - val_accuracy: 0.1071\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5823 - accuracy: 0.4885 - val_loss: 3.2614 - val_accuracy: 0.0893\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5759 - accuracy: 0.4933 - val_loss: 3.2769 - val_accuracy: 0.0893\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5692 - accuracy: 0.4905 - val_loss: 3.2956 - val_accuracy: 0.0893\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5580 - accuracy: 0.5019 - val_loss: 3.3058 - val_accuracy: 0.1071\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5553 - accuracy: 0.4914 - val_loss: 3.3033 - val_accuracy: 0.0893\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5495 - accuracy: 0.4952 - val_loss: 3.3261 - val_accuracy: 0.1071\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5428 - accuracy: 0.5000 - val_loss: 3.3533 - val_accuracy: 0.1071\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5360 - accuracy: 0.5143 - val_loss: 3.3642 - val_accuracy: 0.0893\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5291 - accuracy: 0.4943 - val_loss: 3.3570 - val_accuracy: 0.1071\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5217 - accuracy: 0.5076 - val_loss: 3.3759 - val_accuracy: 0.0893\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5161 - accuracy: 0.5067 - val_loss: 3.3808 - val_accuracy: 0.0893\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5076 - accuracy: 0.5115 - val_loss: 3.3914 - val_accuracy: 0.0893\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5012 - accuracy: 0.5086 - val_loss: 3.4018 - val_accuracy: 0.0893\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4992 - accuracy: 0.5057 - val_loss: 3.4166 - val_accuracy: 0.1071\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4931 - accuracy: 0.5162 - val_loss: 3.4230 - val_accuracy: 0.0893\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4868 - accuracy: 0.5134 - val_loss: 3.4342 - val_accuracy: 0.1071\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4786 - accuracy: 0.5229 - val_loss: 3.4430 - val_accuracy: 0.0893\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3.3642 - accuracy: 0.1087\n",
      "\n",
      "Test accuracy: 0.10869564861059189\n",
      "var: 0.999\n",
      "(1380, 24)\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 16ms/step - loss: 2.7490 - accuracy: 0.0525 - val_loss: 2.7193 - val_accuracy: 0.0714\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.6456 - accuracy: 0.0954 - val_loss: 2.6839 - val_accuracy: 0.0714\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.6046 - accuracy: 0.1250 - val_loss: 2.6698 - val_accuracy: 0.1071\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5758 - accuracy: 0.1365 - val_loss: 2.6567 - val_accuracy: 0.0893\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5505 - accuracy: 0.1479 - val_loss: 2.6500 - val_accuracy: 0.0536\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5293 - accuracy: 0.1679 - val_loss: 2.6334 - val_accuracy: 0.0536\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.5075 - accuracy: 0.1756 - val_loss: 2.6238 - val_accuracy: 0.0714\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4883 - accuracy: 0.1803 - val_loss: 2.6085 - val_accuracy: 0.0714\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4665 - accuracy: 0.1899 - val_loss: 2.6003 - val_accuracy: 0.0714\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.4503 - accuracy: 0.1966 - val_loss: 2.5871 - val_accuracy: 0.1071\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.4321 - accuracy: 0.2013 - val_loss: 2.5835 - val_accuracy: 0.1250\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.4138 - accuracy: 0.2099 - val_loss: 2.5680 - val_accuracy: 0.1071\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 2.3980 - accuracy: 0.2042 - val_loss: 2.5591 - val_accuracy: 0.1429\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3806 - accuracy: 0.2166 - val_loss: 2.5547 - val_accuracy: 0.1607\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3657 - accuracy: 0.2128 - val_loss: 2.5474 - val_accuracy: 0.1607\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3504 - accuracy: 0.2300 - val_loss: 2.5516 - val_accuracy: 0.1250\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3344 - accuracy: 0.2433 - val_loss: 2.5479 - val_accuracy: 0.1607\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3186 - accuracy: 0.2366 - val_loss: 2.5421 - val_accuracy: 0.1429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3034 - accuracy: 0.2433 - val_loss: 2.5435 - val_accuracy: 0.1429\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2882 - accuracy: 0.2443 - val_loss: 2.5387 - val_accuracy: 0.1786\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2731 - accuracy: 0.2538 - val_loss: 2.5412 - val_accuracy: 0.1429\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2590 - accuracy: 0.2567 - val_loss: 2.5348 - val_accuracy: 0.1964\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2472 - accuracy: 0.2624 - val_loss: 2.5369 - val_accuracy: 0.1607\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2327 - accuracy: 0.2681 - val_loss: 2.5495 - val_accuracy: 0.1964\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2195 - accuracy: 0.2700 - val_loss: 2.5480 - val_accuracy: 0.1607\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2039 - accuracy: 0.2719 - val_loss: 2.5539 - val_accuracy: 0.1607\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1888 - accuracy: 0.2777 - val_loss: 2.5642 - val_accuracy: 0.1607\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1780 - accuracy: 0.2777 - val_loss: 2.5693 - val_accuracy: 0.1964\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1627 - accuracy: 0.2958 - val_loss: 2.5691 - val_accuracy: 0.1786\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1501 - accuracy: 0.2891 - val_loss: 2.5676 - val_accuracy: 0.1964\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1406 - accuracy: 0.2901 - val_loss: 2.5784 - val_accuracy: 0.1786\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1268 - accuracy: 0.2948 - val_loss: 2.5866 - val_accuracy: 0.1786\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.1139 - accuracy: 0.3006 - val_loss: 2.6002 - val_accuracy: 0.1964\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0985 - accuracy: 0.3082 - val_loss: 2.6018 - val_accuracy: 0.1429\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0863 - accuracy: 0.3158 - val_loss: 2.6050 - val_accuracy: 0.1786\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0732 - accuracy: 0.3216 - val_loss: 2.6119 - val_accuracy: 0.1607\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0625 - accuracy: 0.3168 - val_loss: 2.6266 - val_accuracy: 0.1607\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0525 - accuracy: 0.3216 - val_loss: 2.6261 - val_accuracy: 0.1786\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0382 - accuracy: 0.3273 - val_loss: 2.6388 - val_accuracy: 0.1429\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0279 - accuracy: 0.3216 - val_loss: 2.6360 - val_accuracy: 0.1786\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0151 - accuracy: 0.3387 - val_loss: 2.6508 - val_accuracy: 0.1429\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.0086 - accuracy: 0.3387 - val_loss: 2.6583 - val_accuracy: 0.1607\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9954 - accuracy: 0.3483 - val_loss: 2.6681 - val_accuracy: 0.1429\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9833 - accuracy: 0.3473 - val_loss: 2.6681 - val_accuracy: 0.1429\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9741 - accuracy: 0.3635 - val_loss: 2.6964 - val_accuracy: 0.1429\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9609 - accuracy: 0.3531 - val_loss: 2.6940 - val_accuracy: 0.1786\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9472 - accuracy: 0.3655 - val_loss: 2.6951 - val_accuracy: 0.1250\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9451 - accuracy: 0.3645 - val_loss: 2.7060 - val_accuracy: 0.1250\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9304 - accuracy: 0.3674 - val_loss: 2.7272 - val_accuracy: 0.1250\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9178 - accuracy: 0.3712 - val_loss: 2.7422 - val_accuracy: 0.1250\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9059 - accuracy: 0.3817 - val_loss: 2.7263 - val_accuracy: 0.1250\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8971 - accuracy: 0.3845 - val_loss: 2.7534 - val_accuracy: 0.1250\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8869 - accuracy: 0.3836 - val_loss: 2.7514 - val_accuracy: 0.1250\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8775 - accuracy: 0.3931 - val_loss: 2.7689 - val_accuracy: 0.1250\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8723 - accuracy: 0.3893 - val_loss: 2.7767 - val_accuracy: 0.1250\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8611 - accuracy: 0.3950 - val_loss: 2.7730 - val_accuracy: 0.1250\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8459 - accuracy: 0.3941 - val_loss: 2.7900 - val_accuracy: 0.1250\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8389 - accuracy: 0.4055 - val_loss: 2.8016 - val_accuracy: 0.1250\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8293 - accuracy: 0.4027 - val_loss: 2.8145 - val_accuracy: 0.1250\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8183 - accuracy: 0.4094 - val_loss: 2.8210 - val_accuracy: 0.1250\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8061 - accuracy: 0.4084 - val_loss: 2.8194 - val_accuracy: 0.1250\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.8019 - accuracy: 0.4189 - val_loss: 2.8382 - val_accuracy: 0.1250\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7907 - accuracy: 0.4170 - val_loss: 2.8564 - val_accuracy: 0.1250\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7822 - accuracy: 0.4256 - val_loss: 2.8487 - val_accuracy: 0.1250\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7723 - accuracy: 0.4160 - val_loss: 2.8655 - val_accuracy: 0.1250\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7606 - accuracy: 0.4246 - val_loss: 2.8574 - val_accuracy: 0.1250\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7548 - accuracy: 0.4303 - val_loss: 2.8896 - val_accuracy: 0.1250\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7477 - accuracy: 0.4265 - val_loss: 2.8846 - val_accuracy: 0.1250\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7363 - accuracy: 0.4313 - val_loss: 2.8998 - val_accuracy: 0.1250\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7305 - accuracy: 0.4323 - val_loss: 2.8985 - val_accuracy: 0.1250\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7220 - accuracy: 0.4399 - val_loss: 2.9289 - val_accuracy: 0.1250\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7128 - accuracy: 0.4351 - val_loss: 2.9193 - val_accuracy: 0.1250\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7027 - accuracy: 0.4380 - val_loss: 2.9385 - val_accuracy: 0.1250\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.7000 - accuracy: 0.4323 - val_loss: 2.9382 - val_accuracy: 0.1250\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6869 - accuracy: 0.4427 - val_loss: 2.9623 - val_accuracy: 0.1429\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6792 - accuracy: 0.4466 - val_loss: 2.9648 - val_accuracy: 0.1429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6708 - accuracy: 0.4475 - val_loss: 2.9580 - val_accuracy: 0.1429\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6631 - accuracy: 0.4437 - val_loss: 2.9768 - val_accuracy: 0.1607\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6564 - accuracy: 0.4485 - val_loss: 2.9863 - val_accuracy: 0.1429\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6505 - accuracy: 0.4590 - val_loss: 3.0025 - val_accuracy: 0.1429\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6433 - accuracy: 0.4618 - val_loss: 3.0066 - val_accuracy: 0.1607\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6355 - accuracy: 0.4542 - val_loss: 3.0293 - val_accuracy: 0.1429\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6263 - accuracy: 0.4552 - val_loss: 3.0188 - val_accuracy: 0.1607\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6174 - accuracy: 0.4676 - val_loss: 3.0344 - val_accuracy: 0.1429\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6128 - accuracy: 0.4628 - val_loss: 3.0259 - val_accuracy: 0.1429\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6057 - accuracy: 0.4676 - val_loss: 3.0586 - val_accuracy: 0.1607\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6010 - accuracy: 0.4714 - val_loss: 3.0780 - val_accuracy: 0.1607\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5891 - accuracy: 0.4714 - val_loss: 3.0897 - val_accuracy: 0.1429\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5822 - accuracy: 0.4685 - val_loss: 3.0873 - val_accuracy: 0.1429\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5763 - accuracy: 0.4771 - val_loss: 3.1116 - val_accuracy: 0.1429\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5726 - accuracy: 0.4723 - val_loss: 3.1240 - val_accuracy: 0.1429\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5641 - accuracy: 0.4752 - val_loss: 3.1125 - val_accuracy: 0.1607\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5570 - accuracy: 0.4800 - val_loss: 3.1410 - val_accuracy: 0.1429\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5503 - accuracy: 0.4809 - val_loss: 3.1510 - val_accuracy: 0.1429\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5428 - accuracy: 0.4838 - val_loss: 3.1614 - val_accuracy: 0.1250\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5375 - accuracy: 0.4933 - val_loss: 3.1838 - val_accuracy: 0.1250\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5315 - accuracy: 0.4895 - val_loss: 3.1991 - val_accuracy: 0.1250\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5237 - accuracy: 0.4962 - val_loss: 3.2067 - val_accuracy: 0.1429\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5187 - accuracy: 0.4990 - val_loss: 3.2229 - val_accuracy: 0.1250\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.5155 - accuracy: 0.5019 - val_loss: 3.2296 - val_accuracy: 0.1429\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 3.4072 - accuracy: 0.1268\n",
      "\n",
      "Test accuracy: 0.12681159377098083\n",
      "Best Accuracy: 0.1557970941066742\n",
      "Trimmed x_d with Best Selected Features:\n",
      "(1380, 23)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_pca_for_feature_selection(X, desired_variance_ratio):\n",
    "    pca = PCA(n_components=desired_variance_ratio)\n",
    "    reduced_features = pca.fit_transform(X)\n",
    "    return reduced_features, pca\n",
    "\n",
    "def pca(X, y, test_size=0.2, random_state=42):\n",
    "\n",
    "    variance_ratios = [i / 1000 for i in range(990, 1000)]\n",
    "    max_accuracy = 0.0\n",
    "    best_selected_features = None\n",
    "    best_pca_model = None\n",
    "\n",
    "    for desired_variance_ratio in variance_ratios:\n",
    "        reduced_features, pca_model = apply_pca_for_feature_selection(X, desired_variance_ratio)\n",
    "        print(f\"var: {desired_variance_ratio}\")\n",
    "        print(np.shape(reduced_features))\n",
    "\n",
    "        # Split the dataset into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Build the ANN model\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            keras.layers.Dense(32, activation='relu'),\n",
    "            keras.layers.Dense(14, activation='softmax')  # 14 output classes\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.05)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
    "        print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "        # Update max accuracy and store corresponding features\n",
    "        if test_acc > max_accuracy:\n",
    "            max_accuracy = test_acc\n",
    "            best_selected_features = reduced_features.copy()\n",
    "            best_pca_model = pca_model\n",
    "\n",
    "    # Apply inverse_transform to the full dataset with the best selected features\n",
    "    trimmed_x_d = best_pca_model.inverse_transform(best_selected_features)\n",
    "\n",
    "    return best_selected_features, max_accuracy\n",
    "\n",
    "# Example usage:\n",
    "reduced_x_d, max_accuracy = pca(x_d, y_d)\n",
    "print(f\"Best Accuracy: {max_accuracy}\")\n",
    "print(\"Trimmed x_d with Best Selected Features:\")\n",
    "print(np.shape(reduced_x_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bcf09",
   "metadata": {},
   "source": [
    "## ANN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca19135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "activation_0 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.01, 0.1, 0.3, 0.5], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "X = reduced_x_d\n",
    "y = y_d\n",
    "\n",
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Flatten())\n",
    "\n",
    "        # Tune the number of layers.\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "            model.add(\n",
    "                layers.Dense(\n",
    "                    # Tune number of units separately.\n",
    "                    units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                    activation=hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\", \"sigmoid\"]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        model.add(layers.Dense(14, activation=\"softmax\"))\n",
    "\n",
    "        learning_rate = hp.Choice(\"learning_rate\", [0.001, 0.01, 0.1, 0.3, 0.5])\n",
    "#         alpha = hp.Choice(\"alpha\", [0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate), #, momentum=alpha\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32, 64, 128]),\n",
    "            **kwargs,\n",
    "        )\n",
    "    \n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Create an instance of HyperParameters\n",
    "hp = kt.HyperParameters()\n",
    "\n",
    "# Build the model with default hyperparameters\n",
    "my_hypermodel = MyHyperModel()\n",
    "my_model = my_hypermodel.build(hp)\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel=my_hypermodel,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=50,\n",
    "    factor=2,\n",
    "    directory=\"Parameter Tuning\",\n",
    "    project_name=\"14 Classes - Zernike\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96f1d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 186 Complete [00h 00m 07s]\n",
      "val_accuracy: 0.25\n",
      "\n",
      "Best val_accuracy So Far: 0.25\n",
      "Total elapsed time: 00h 10m 40s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.05, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02db1ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in Parameter Tuning\\14 Classes - Zernike\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0185 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 128\n",
      "activation_0: tanh\n",
      "learning_rate: 0.001\n",
      "units_1: 224\n",
      "activation_1: relu\n",
      "batch_size: 16\n",
      "units_2: 320\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.25\n",
      "\n",
      "Trial 0114 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 416\n",
      "activation_0: tanh\n",
      "learning_rate: 0.01\n",
      "units_1: 96\n",
      "activation_1: sigmoid\n",
      "batch_size: 32\n",
      "units_2: 128\n",
      "activation_2: relu\n",
      "tuner/epochs: 13\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0102\n",
      "Score: 0.2321428507566452\n",
      "\n",
      "Trial 0067 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 352\n",
      "activation_0: tanh\n",
      "learning_rate: 0.01\n",
      "units_1: 352\n",
      "activation_1: sigmoid\n",
      "batch_size: 32\n",
      "units_2: 512\n",
      "activation_2: tanh\n",
      "tuner/epochs: 13\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 5\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0061\n",
      "Score: 0.2142857164144516\n",
      "\n",
      "Trial 0073 summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 32\n",
      "activation_0: relu\n",
      "learning_rate: 0.1\n",
      "units_1: 480\n",
      "activation_1: relu\n",
      "batch_size: 32\n",
      "units_2: 224\n",
      "activation_2: relu\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 5\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 0069\n",
      "Score: 0.2142857164144516\n",
      "\n",
      "Trial 0082 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 384\n",
      "activation_0: relu\n",
      "learning_rate: 0.01\n",
      "units_1: 192\n",
      "activation_1: sigmoid\n",
      "batch_size: 32\n",
      "units_2: 352\n",
      "activation_2: relu\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.2142857164144516\n",
      "\n",
      "Trial 0130 summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 384\n",
      "activation_0: tanh\n",
      "learning_rate: 0.01\n",
      "units_1: 512\n",
      "activation_1: tanh\n",
      "batch_size: 32\n",
      "units_2: 256\n",
      "activation_2: tanh\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 0.2142857164144516\n",
      "\n",
      "Trial 0144 summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 448\n",
      "activation_0: tanh\n",
      "learning_rate: 0.01\n",
      "units_1: 64\n",
      "activation_1: tanh\n",
      "batch_size: 128\n",
      "units_2: 32\n",
      "activation_2: tanh\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0138\n",
      "Score: 0.2142857164144516\n",
      "\n",
      "Trial 0159 summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 288\n",
      "activation_0: tanh\n",
      "learning_rate: 0.1\n",
      "units_1: 416\n",
      "activation_1: sigmoid\n",
      "batch_size: 64\n",
      "units_2: 32\n",
      "activation_2: sigmoid\n",
      "tuner/epochs: 13\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.2142857164144516\n",
      "\n",
      "Trial 0161 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 160\n",
      "activation_0: tanh\n",
      "learning_rate: 0.001\n",
      "units_1: 128\n",
      "activation_1: relu\n",
      "batch_size: 128\n",
      "units_2: 512\n",
      "activation_2: tanh\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0157\n",
      "Score: 0.2142857164144516\n",
      "\n",
      "Trial 0171 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 480\n",
      "activation_0: relu\n",
      "learning_rate: 0.01\n",
      "units_1: 160\n",
      "activation_1: relu\n",
      "batch_size: 64\n",
      "units_2: 320\n",
      "activation_2: relu\n",
      "tuner/epochs: 25\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.2142857164144516\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e38f980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "33/33 [==============================] - 2s 15ms/step - loss: 2.6877 - accuracy: 0.0802 - val_loss: 2.6705 - val_accuracy: 0.0714\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.6028 - accuracy: 0.1097 - val_loss: 2.5633 - val_accuracy: 0.0714\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.5383 - accuracy: 0.1174 - val_loss: 2.5213 - val_accuracy: 0.1250\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.5089 - accuracy: 0.1584 - val_loss: 2.4915 - val_accuracy: 0.1607\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4744 - accuracy: 0.1622 - val_loss: 2.4847 - val_accuracy: 0.1786\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.4447 - accuracy: 0.1756 - val_loss: 2.5011 - val_accuracy: 0.1250\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.4215 - accuracy: 0.1851 - val_loss: 2.5137 - val_accuracy: 0.1071\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3978 - accuracy: 0.1966 - val_loss: 2.5132 - val_accuracy: 0.1250\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3770 - accuracy: 0.2032 - val_loss: 2.4793 - val_accuracy: 0.1607\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3499 - accuracy: 0.2118 - val_loss: 2.5044 - val_accuracy: 0.1607\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.3440 - accuracy: 0.2080 - val_loss: 2.5191 - val_accuracy: 0.1250\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.3163 - accuracy: 0.2071 - val_loss: 2.5214 - val_accuracy: 0.1607\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2914 - accuracy: 0.2357 - val_loss: 2.5739 - val_accuracy: 0.1429\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2841 - accuracy: 0.2414 - val_loss: 2.5682 - val_accuracy: 0.1429\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2558 - accuracy: 0.2357 - val_loss: 2.5560 - val_accuracy: 0.1607\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2387 - accuracy: 0.2653 - val_loss: 2.5509 - val_accuracy: 0.1607\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.2192 - accuracy: 0.2576 - val_loss: 2.5877 - val_accuracy: 0.1607\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.2061 - accuracy: 0.2615 - val_loss: 2.6087 - val_accuracy: 0.1429\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1930 - accuracy: 0.2719 - val_loss: 2.6126 - val_accuracy: 0.1964\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1699 - accuracy: 0.2634 - val_loss: 2.6201 - val_accuracy: 0.1607\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1538 - accuracy: 0.2815 - val_loss: 2.6125 - val_accuracy: 0.1964\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1392 - accuracy: 0.2939 - val_loss: 2.6509 - val_accuracy: 0.1607\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.1175 - accuracy: 0.3006 - val_loss: 2.6537 - val_accuracy: 0.1607\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0889 - accuracy: 0.3101 - val_loss: 2.6878 - val_accuracy: 0.1429\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0635 - accuracy: 0.3168 - val_loss: 2.6915 - val_accuracy: 0.1607\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0372 - accuracy: 0.3340 - val_loss: 2.7195 - val_accuracy: 0.1964\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0273 - accuracy: 0.3263 - val_loss: 2.6861 - val_accuracy: 0.1429\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.0023 - accuracy: 0.3531 - val_loss: 2.7144 - val_accuracy: 0.1250\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.9774 - accuracy: 0.3616 - val_loss: 2.7178 - val_accuracy: 0.1071\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9533 - accuracy: 0.3683 - val_loss: 2.7501 - val_accuracy: 0.1607\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.9258 - accuracy: 0.3788 - val_loss: 2.7622 - val_accuracy: 0.1607\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8920 - accuracy: 0.3989 - val_loss: 2.7685 - val_accuracy: 0.1429\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8837 - accuracy: 0.3950 - val_loss: 2.8059 - val_accuracy: 0.1429\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8641 - accuracy: 0.4027 - val_loss: 2.8039 - val_accuracy: 0.1429\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.8253 - accuracy: 0.4160 - val_loss: 2.8048 - val_accuracy: 0.1607\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7929 - accuracy: 0.4246 - val_loss: 2.8277 - val_accuracy: 0.1786\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7906 - accuracy: 0.4380 - val_loss: 2.8541 - val_accuracy: 0.2143\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7556 - accuracy: 0.4389 - val_loss: 2.8536 - val_accuracy: 0.1786\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7227 - accuracy: 0.4580 - val_loss: 2.8969 - val_accuracy: 0.1786\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.7064 - accuracy: 0.4571 - val_loss: 2.8968 - val_accuracy: 0.1607\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.6657 - accuracy: 0.4752 - val_loss: 2.9107 - val_accuracy: 0.2143\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6431 - accuracy: 0.4761 - val_loss: 2.9396 - val_accuracy: 0.1964\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.6233 - accuracy: 0.4809 - val_loss: 2.9543 - val_accuracy: 0.1786\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5863 - accuracy: 0.4857 - val_loss: 2.9841 - val_accuracy: 0.1607\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5522 - accuracy: 0.5067 - val_loss: 3.0158 - val_accuracy: 0.1607\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.5319 - accuracy: 0.5219 - val_loss: 3.0121 - val_accuracy: 0.1786\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4988 - accuracy: 0.5267 - val_loss: 3.0623 - val_accuracy: 0.1786\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4761 - accuracy: 0.5391 - val_loss: 3.0635 - val_accuracy: 0.1786\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.4542 - accuracy: 0.5487 - val_loss: 3.0978 - val_accuracy: 0.1607\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.4149 - accuracy: 0.5573 - val_loss: 3.0999 - val_accuracy: 0.1964\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3922 - accuracy: 0.5754 - val_loss: 3.1589 - val_accuracy: 0.1786\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.3588 - accuracy: 0.5763 - val_loss: 3.1527 - val_accuracy: 0.2143\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3395 - accuracy: 0.5935 - val_loss: 3.2039 - val_accuracy: 0.1964\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.3018 - accuracy: 0.6050 - val_loss: 3.1930 - val_accuracy: 0.2143\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1.2819 - accuracy: 0.6135 - val_loss: 3.2458 - val_accuracy: 0.1964\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2585 - accuracy: 0.6260 - val_loss: 3.3012 - val_accuracy: 0.1786\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.2301 - accuracy: 0.6374 - val_loss: 3.3084 - val_accuracy: 0.1250\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1963 - accuracy: 0.6431 - val_loss: 3.3563 - val_accuracy: 0.1429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1723 - accuracy: 0.6508 - val_loss: 3.3643 - val_accuracy: 0.1786\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1566 - accuracy: 0.6546 - val_loss: 3.3322 - val_accuracy: 0.1786\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.1155 - accuracy: 0.6746 - val_loss: 3.4306 - val_accuracy: 0.1607\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0860 - accuracy: 0.6899 - val_loss: 3.4077 - val_accuracy: 0.2321\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0605 - accuracy: 0.6966 - val_loss: 3.4618 - val_accuracy: 0.1786\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0315 - accuracy: 0.7099 - val_loss: 3.4677 - val_accuracy: 0.1786\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 1.0069 - accuracy: 0.7223 - val_loss: 3.4883 - val_accuracy: 0.2143\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9890 - accuracy: 0.7290 - val_loss: 3.5416 - val_accuracy: 0.1964\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9640 - accuracy: 0.7309 - val_loss: 3.5652 - val_accuracy: 0.1607\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9326 - accuracy: 0.7376 - val_loss: 3.5924 - val_accuracy: 0.1786\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.9113 - accuracy: 0.7481 - val_loss: 3.6087 - val_accuracy: 0.1786\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8875 - accuracy: 0.7691 - val_loss: 3.6394 - val_accuracy: 0.1786\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8715 - accuracy: 0.7576 - val_loss: 3.7383 - val_accuracy: 0.2500\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8465 - accuracy: 0.7815 - val_loss: 3.7150 - val_accuracy: 0.1607\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8239 - accuracy: 0.7796 - val_loss: 3.7682 - val_accuracy: 0.1786\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.8007 - accuracy: 0.7968 - val_loss: 3.8279 - val_accuracy: 0.1607\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7677 - accuracy: 0.8111 - val_loss: 3.8283 - val_accuracy: 0.1964\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7559 - accuracy: 0.8015 - val_loss: 3.8394 - val_accuracy: 0.1607\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7329 - accuracy: 0.8073 - val_loss: 3.8896 - val_accuracy: 0.1786\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.7140 - accuracy: 0.8225 - val_loss: 3.9630 - val_accuracy: 0.1607\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.6847 - accuracy: 0.8321 - val_loss: 3.9958 - val_accuracy: 0.1786\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.6741 - accuracy: 0.8416 - val_loss: 4.0660 - val_accuracy: 0.1964\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.6482 - accuracy: 0.8492 - val_loss: 4.0388 - val_accuracy: 0.1964\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.6291 - accuracy: 0.8368 - val_loss: 4.1345 - val_accuracy: 0.1607\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.6151 - accuracy: 0.8569 - val_loss: 4.1643 - val_accuracy: 0.1964\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.5909 - accuracy: 0.8683 - val_loss: 4.1751 - val_accuracy: 0.1607\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.5767 - accuracy: 0.8721 - val_loss: 4.2778 - val_accuracy: 0.1607\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.5494 - accuracy: 0.8788 - val_loss: 4.2625 - val_accuracy: 0.1786\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.5339 - accuracy: 0.8874 - val_loss: 4.3084 - val_accuracy: 0.1429\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.5271 - accuracy: 0.8874 - val_loss: 4.3748 - val_accuracy: 0.1429\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.5076 - accuracy: 0.8950 - val_loss: 4.3691 - val_accuracy: 0.1429\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4893 - accuracy: 0.8912 - val_loss: 4.4400 - val_accuracy: 0.1786\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4680 - accuracy: 0.8998 - val_loss: 4.4735 - val_accuracy: 0.1607\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.4577 - accuracy: 0.8998 - val_loss: 4.5516 - val_accuracy: 0.1429\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.4490 - accuracy: 0.9065 - val_loss: 4.5496 - val_accuracy: 0.1429\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.4329 - accuracy: 0.9074 - val_loss: 4.6049 - val_accuracy: 0.1250\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.4127 - accuracy: 0.9151 - val_loss: 4.6651 - val_accuracy: 0.1250\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.4004 - accuracy: 0.9294 - val_loss: 4.7220 - val_accuracy: 0.1429\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.3923 - accuracy: 0.9170 - val_loss: 4.8005 - val_accuracy: 0.1786\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.3821 - accuracy: 0.9198 - val_loss: 4.8921 - val_accuracy: 0.1429\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.3577 - accuracy: 0.9294 - val_loss: 4.9089 - val_accuracy: 0.1250\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.3512 - accuracy: 0.9294 - val_loss: 4.8736 - val_accuracy: 0.1786\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 5.0064 - accuracy: 0.1304\n",
      "[test loss, test accuracy]: [5.006433010101318, 0.1304347813129425]\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.05)\n",
    "\n",
    "# Check Result\n",
    "y_pred_a = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_a, axis=1)\n",
    "\n",
    "eval_result = model.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ded1ce49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.16      0.14        19\n",
      "           1       0.19      0.36      0.25        14\n",
      "           2       0.08      0.05      0.06        22\n",
      "           3       0.13      0.12      0.13        24\n",
      "           4       0.13      0.10      0.11        20\n",
      "           5       0.14      0.24      0.18        21\n",
      "           6       0.17      0.14      0.15        22\n",
      "           7       0.00      0.00      0.00        21\n",
      "           8       0.12      0.10      0.11        21\n",
      "           9       0.13      0.10      0.11        21\n",
      "          10       0.21      0.20      0.21        20\n",
      "          11       0.12      0.20      0.15        15\n",
      "          12       0.06      0.07      0.06        15\n",
      "          13       0.10      0.10      0.10        21\n",
      "\n",
      "    accuracy                           0.13       276\n",
      "   macro avg       0.12      0.14      0.12       276\n",
      "weighted avg       0.12      0.13      0.12       276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class_list = set(y_test)\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "# print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=class_list, yticklabels=class_list)\n",
    "\n",
    "for i in range(len(conf_matrix)):\n",
    "    for j in range(len(conf_matrix[0])):\n",
    "        if i == j:\n",
    "            plt.text(j + 0.5, i + 0.5, str(conf_matrix[i, j]), ha='center', va='center', color='white')\n",
    "        else:\n",
    "            plt.text(j + 0.5, i + 0.5, str(conf_matrix[i, j]), ha='center', va='center', color='black')\n",
    "            \n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d440e015",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt('test_predictions.csv', np.hstack((y_test.reshape(-1, 1), y_pred.reshape(-1,1))), delimiter=',', fmt='%d')\n",
    "np.savetxt('actual labels.csv', y_test.reshape(-1,1), delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ada0d7e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Mapping:\n",
      "{'bacterial_leaf_blight': 0, 'bacterial_leaf_streak': 1, 'bakanae': 2, 'brown_spot': 3, 'grassy_stunt_virus': 4, 'healthy_rice_plant': 5, 'narrow_brown_spot': 6, 'ragged_stunt_virus': 7, 'rice_blast': 8, 'rice_false_smut': 9, 'sheath_blight': 10, 'sheath_rot': 11, 'stem_rot': 12, 'tungro_virus': 13}\n"
     ]
    }
   ],
   "source": [
    "# Display the mapping between class names and encoded numbers\n",
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"\\nClass Mapping:\")\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3359008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
